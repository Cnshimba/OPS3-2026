<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cluster and High Availability - Presentation Slides</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            color: #ffffff;
            line-height: 1.6;
        }
        
        .slide {
            min-height: 100vh;
            padding: 60px 80px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            page-break-after: always;
            border-bottom: 3px solid #0f3460;
        }
        
        .slide:nth-child(even) {
            background: linear-gradient(135deg, #16213e 0%, #1a1a2e 100%);
        }
        
        .title-slide {
            justify-content: center;
            align-items: center;
            text-align: center;
            background: linear-gradient(135deg, #0f3460 0%, #16213e 100%);
        }
        
        .title-slide h1 {
            font-size: 3.5em;
            margin-bottom: 20px;
            color: #e94560;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .title-slide .week-number {
            font-size: 1.5em;
            color: #ffd700;
            margin-bottom: 40px;
            letter-spacing: 2px;
        }
        
        .title-slide .course-code {
            font-size: 1.2em;
            color: #a8dadc;
            margin-top: 30px;
        }
        
        h2 {
            font-size: 2.5em;
            color: #e94560;
            margin-bottom: 30px;
            border-bottom: 3px solid #ffd700;
            padding-bottom: 15px;
        }
        
        h3 {
            font-size: 2em;
            color: #ffd700;
            margin-bottom: 25px;
        }
        
        ul {
            font-size: 1.4em;
            margin-left: 40px;
            margin-bottom: 20px;
        }
        
        ul li {
            margin-bottom: 15px;
            line-height: 1.8;
        }
        
        p {
            font-size: 1.3em;
            margin-bottom: 20px;
            line-height: 1.8;
        }
        
        .code-block {
            background: #0f0f0f;
            border-left: 4px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        pre {
            background: #0f0f0f;
            border-left: 4px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 1.1em;
            line-height: 1.5;
        }
        
        code {
            font-family: 'Courier New', monospace;
            color: #a8dadc;
        }
        
        .quote {
            background: rgba(233, 69, 96, 0.1);
            border-left: 5px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            font-style: italic;
            font-size: 1.2em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 1.2em;
        }
        
        th {
            background: #0f3460;
            color: #ffd700;
            padding: 15px;
            text-align: left;
            border: 1px solid #16213e;
        }
        
        td {
            padding: 12px;
            border: 1px solid #16213e;
            background: rgba(255, 255, 255, 0.05);
        }
        
        tr:hover {
            background: rgba(233, 69, 96, 0.1);
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 20px 0;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
        
        .key-points {
            background: rgba(255, 215, 0, 0.1);
            border: 2px solid #ffd700;
            padding: 30px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .slide-number {
            position: fixed;
            bottom: 20px;
            right: 30px;
            font-size: 1em;
            color: #a8dadc;
            opacity: 0.7;
        }
        
        @media print {
            .slide {
                page-break-after: always;
            }
        }
    </style>
</head>
<body>

    <div class="slide title-slide">
        <div class="week-number">Week 6</div>
        <h1>Cluster and High Availability</h1>
        <div class="course-code">OPS3 - Virtualization and Cloud Infrastructure</div>
        <div class="slide-number">Slide 1</div>
    </div>

    <div class="slide">
        <h2>Welcome to Week 6!</h2>
        <div class="slide-number">Slide 2</div>
    </div>

    <div class="slide">
        <h2>What You'll Learn This Week</h2>
        <div class="slide-number">Slide 3</div>
    </div>

    <div class="slide">
        <h2>1. Creating a Cluster</h2>
        <div class="slide-number">Slide 4</div>
    </div>

    <div class="slide">
        <h3>1.1 Initialization</h3>
        <ul>
            <li>When you initialize a cluster, Proxmox performs several critical actions.</li>
            <li>It generates a cryptographic key (/etc/corosync/authkey) appearing to secure communication and creates the central configuration database (/etc/pve/corosync.conf).</li>
            <li>This database is essentially the "source of truth" for the entire cluster.</li>
        </ul>
        <div class="slide-number">Slide 5</div>
    </div>

    <div class="slide">
        <h3>1.2 Joining a Node</h3>
        <ul>
            <li>Adding a second server is a "join" operation, not a creation operation.</li>
            <li>You instruct the new node to connect to the existing ring.</li>
            <li>The new node authenticates using the root password or an explicit join token, downloads the cluster keys and configuration files, and restarts its local services to synchronize with the quorum.</li>
        </ul>
        <ul>
            <li>Crucial Requirement: For a cluster to function correctly, every node must have a unique Hostname and a persistent Static IP address.</li>
            <li>If an IP address changes after the cluster is formed, Corosync communication will break, causing the node to lose quorum and effectively disconnecting it from the datacenter.</li>
        </ul>
        <div class="slide-number">Slide 6</div>
    </div>

    <div class="slide">
        <h3>Section 1 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>pvecm is the primary command-line tool for managing the cluster lifecycle, wrapping the underlying Corosync engine.</li>
            <li>Cluster Requirements are strict: nodes must have unique hostnames, static network configurations, and a reliable low-latency network connection.</li>
            <li>Joining involves a node authenticating to an existing cluster leader to download shared keys and configuration state.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why does Proxmox use SSH keys for cluster communication alongside Corosync keys?</li>
            <li>What is the impact on the cluster configuration file /etc/pve/corosync.conf if you change a node's IP address without updating it?</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>Proxmox VE Cluster Manager</li>
        </ul>
        <div class="slide-number">Slide 7</div>
    </div>

    <div class="slide">
        <h2>2. Quorum: The Rule of Majority Algorithm</h2>
        <div class="slide-number">Slide 8</div>
    </div>

    <div class="slide">
        <h3>2.1 The Split Brain Condition</h3>
        <p>"Split Brain" is a catastrophic failure state in a clustered environment where network communication is severed between nodes, yet the nodes themselves remain operational.</p>
        <p>Consider a two-node cluster (Node A and Node B) where the heartbeat connection fails:</p>
        <ul>
            <li>The Divergence: Node A cannot see Node B and assumes Node B has failed. Simultaneously, Node B assumes Node A has failed.</li>
            <li>The Conflict: Both nodes promote themselves to "Master" status and attempt to take ownership of the same resources (e.g., VM ID 100).</li>
            <li>The Consequence: Both nodes mount the same shared storage volume and attempt to write data concurrently.</li>
            <li>The Result: Since they are unaware of each other's write operations, they overwrite each other's filesystem journals, leading to irreversible data corruption within milliseconds.</li>
        </ul>
        <p>Figure 3: Split Brain Scenario - A network cut leads to dual active masters ensuring data corruption without quorum logic</p>
        <div class="slide-number">Slide 9</div>
    </div>

    <div class="slide">
        <h3>2.2 Quorum Logic</h3>
        <p>To prevent Split Brain, the Proxmox Cluster Manager (pvecm) enforces a strictly democratic requirement: operations can only proceed if a strict majority of nodes are present. The formula for this is (Total Votes / 2) + 1.</p>
        <ul>
            <li>In a 2-Node Cluster, there are 2 total votes.</li>
            <li>The majority needed is (2/2) + 1 = 2.</li>
            <li>This implies that if a single node fails, the survivor has only 1 vote.</li>
            <li>Since 1 is less than 2, Quorum is lost.</li>
            <li>The surviving node essentially "locks down," forcing the filesystem into Read-Only mode to prevent any possibility of corruption.</li>
        </ul>
        <ul>
            <li>In a 3-Node Cluster, there are 3 total votes.</li>
            <li>The majority needed is (3/2) + 1 = 2.5 (which rounds down to integer 2).</li>
            <li>If one node fails, the remaining two nodes have 2 votes.</li>
            <li>Since 2 equals 2, Quorum is maintained, and the cluster remains fully operational.</li>
            <li>This highlights the architectural best practice of always designing clusters with an ODD number of nodes (3, 5, 7) to allow for reliable tie-breaking.</li>
        </ul>
        <div class="slide-number">Slide 10</div>
    </div>

    <div class="slide">
        <h3>Section 2 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Quorum enforces the "Rule of Majority" using the formula (Total/2)+1 to ensure only one part of a partitioned cluster remains active.</li>
            <li>Split Brain occurs when disconnected nodes both attempt to become Master, leading to guaranteed data corruption.</li>
            <li>Safety Mechanism: If Quorum is lost, the cluster automatically locks down to Read-Only mode to preserve data integrity.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why is a 2-node cluster considered "dangerous" without an external vote (QDevice)?</li>
            <li>Does a "Majority" mean 51% (more than half) or exactly half?</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>Corosync Project</li>
        </ul>
        <div class="slide-number">Slide 11</div>
    </div>

    <div class="slide">
        <h2>3. High Availability (HA) Manager</h2>
        <div class="slide-number">Slide 12</div>
    </div>

    <div class="slide">
        <h3>3.1 Architecture Components</h3>
        <p>The HA system is composed of two primary agents that work in tandem to maintain service availability.</p>
        <ul>
            <li>The Cluster Resource Manager (pve-ha-crm) acts as the "Cluster Manager" or the "Boss." It runs as a single active instance on the current master node.</li>
            <li>Its job is to maintain the state of the cluster and make high-level decisions about where services should live.</li>
            <li>If the node running the active CRM fails, the cluster automatically elects a new master to take over this role.</li>
        </ul>
        <ul>
            <li>The Local Resource Manager (pve-ha-lrm) acts as the "Worker." An instance runs on every single node in the cluster.</li>
            <li>It receives orders from the CRM to start or stop services and reports the status of local resources back to the master.</li>
            <li>It is responsible for the actual execution of service management commands on the local hypervisor.</li>
        </ul>
        <div class="slide-number">Slide 13</div>
    </div>

    <div class="slide">
        <h3>3.2 Fencing Mechanism</h3>
        <p>The HA mechanism relies on absolute certainty. Before the cluster can steal VMs from a non-responsive node, it must be 100% sure that the node is truly dead.</p>
        <ul>
            <li>If Node A stops responding to heartbeats, Node B cannot know if Node A has crashed or if just the network cable was unplugged.</li>
            <li>If Node B starts Node A's VMs while Node A is still running them, both nodes would attempt to write to the same virtual disks simultaneously, guaranteeing severe data corruption.</li>
        </ul>
        <ul>
            <li>To solve this, we use Fencing, often referred to by the acronym STONITH (Shoot The Other Node In The Head).</li>
            <li>Upon detecting a failure, the cluster issues a command to a physical hardware device (like an IPMI controller or a Smart PDU) to physically cut power to the faulty node.</li>
            <li>This guarantees the node is dead.</li>
            <li>Only after this confirmation does the cluster restart the VMs on healthy nodes.</li>
        </ul>
        <p>Figure 5: The Fencing Process - How the cluster physically isolates a failed node before recovering its workloads</p>
        <div class="slide-number">Slide 14</div>
    </div>

    <div class="slide">
        <h3>Section 3 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>High Availability (HA) automates the recovery of services by restarting VMs on healthy nodes after a hardware failure.</li>
            <li>CRM and LRM act as the "Manager" and "Worker" services, respectively, to orchestrate the monitoring and recovery process.</li>
            <li>Fencing (STONITH) is the essential safety mechanism that physically powers off a non-responsive node to prevent Split Brain before recovery begins.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why is Fencing (STONITH) safer than just assuming a silent node is down?</li>
            <li>Can you have High Availability without Shared Storage? (Consider the implications of ZFS Replication).</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>Proxmox HA Simulator</li>
        </ul>
        <div class="slide-number">Slide 15</div>
    </div>

    <div class="slide">
        <h2>4. Troubleshooting the Cluster</h2>
        <div class="slide-number">Slide 16</div>
    </div>

    <div class="slide">
        <h3>4.1 Check Quorum</h3>
        <ul>
            <li>The first step in any cluster diagnosis is to verify the voting state.</li>
            <li>Run pvecm status to see the cluster's health from the perspective of the local node.</li>
            <li>Key fields to observe are Votes (number of nodes currently active) and Quorate.</li>
            <li>If Quorate is No, the cluster has lost its majority and will block any changes to the configuration database (pmxcfs) to prevent split-brain, effectively locking the cluster into a read-only mode.</li>
        </ul>
        <div class="slide-number">Slide 17</div>
    </div>

    <div class="slide">
        <h3>4.2 Check Corosync</h3>
        <ul>
            <li>If nodes are not syncing but the network appears up, the issue often lies with Corosync latency.</li>
            <li>Use systemctl status corosync to check the service health.</li>
            <li>The logs will reveal if the "token retransmit time" is being exceeded.</li>
            <li>Corosync requires extremely low latency (typically < 2ms) to function correctly.</li>
            <li>High latency links, such as Wi-Fi or saturated 1Gbps uplinks during backups, often cause Corosync to drop packets and declare nodes dead falsely.</li>
        </ul>
        <div class="slide-number">Slide 18</div>
    </div>

    <div class="slide">
        <h3>4.3 Force Quorum (Emergency Only)</h3>
        <p>In a catastrophic scenario where you have a 2-node cluster and one node permanently fails, the survivor will lose quorum (1 vote < 2 required). To recover management capability on the survivor, you can artificially lower the expected vote count.</p>
        <p>Warning: This command tells the survivor, "Pretend we only expected 1 vote." This allows it to become quorate alone. You must only do this if you are absolutely certain the other node is dead. If the other node comes back online while this is active, you will cause a Split Brain scenario.</p>
        <div class="slide-number">Slide 19</div>
    </div>

    <div class="slide">
        <h3>Section 4 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>pvecm status is the primary diagnostic tool for assessing voting health and determining if the cluster is Quorate.</li>
            <li>Corosync Latency is the most common cause of instability; high latency triggers false failure detection.</li>
            <li>Forcing Quorum (expected 1) is a destructive emergency measure to recover a surviving node in a broken cluster.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why is latency (Ping time) so critical for Corosync compared to bandwidth?</li>
            <li>What does "Quorate: No" actually mean for your ability to start, stop, or migrate VMs?</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>Clusterlabs Troubleshooting</li>
        </ul>
        <div class="slide-number">Slide 20</div>
    </div>

    <div class="slide">
        <h2>5. Live Migration CLI</h2>
        <div class="slide-number">Slide 21</div>
    </div>

    <div class="slide">
        <h3>Section 5 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Live Migration moves active RAM state between nodes, allowing hardware maintenance without service interruption.</li>
            <li>--online ensures the VM remains responsive during the transfer; without it, the VM would hibernate and resume (offline migration).</li>
            <li>--with-local-disks enables migrations even without shared storage by copying the disk image alongside the RAM, though this takes significantly longer.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>What happens to the VM if the network cable is unplugged during the RAM copy phase of a migration?</li>
            <li>Why must the CPU Type often be set to kvm64 or host model carefully in heterogeneous clusters?</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>QEMU Migration Documentation</li>
        </ul>
        <div class="slide-number">Slide 22</div>
    </div>

    <div class="slide">
        <h2>6. Enterprise Shared Storage Architectures</h2>
        <div class="slide-number">Slide 23</div>
    </div>

    <div class="slide">
        <h3>6.1 Distributed Storage: Ceph (Advanced)</h3>
        <p>While ZFS is the gold standard for local storage, modern data centers often span multiple servers. Ceph is a massively scalable, distributed, self-healing file system that runs across a cluster of Proxmox nodes.</p>
        <p>6.1.1 Architecture Components
Ceph is not just software; it is a living ecosystem made of daemons:</p>
        <ul>
            <li>OSD (Object Storage Daemon): The workhorse. One OSD runs per physical disk. It handles reading, writing, and replicating data.</li>
            <li>MON (Monitor): The brain. It maintains the "Cluster Map"â€”the master list of which nodes are alive and where data lives. You usually need at least 3 MONs for quorum.</li>
            <li>MGR (Manager): Collects metrics and state for the GUI dashboard.</li>
        </ul>
        <p>6.1.2 Implementation in Proxmox (HCI)
Proxmox VE is unique because it integrates Ceph directly into the hypervisor (Hyper-Converged Infrastructure). You do not need external storage servers. The architecture diagram below shows how OSDs, MONs, and MGRs work together across a Ceph cluster:</p>
        <p>Figure 8: Ceph Distributed Storage - OSDs manage disks, MONs maintain cluster maps, and MGRs collect metrics across multiple nodes</p>
        <div class="slide-number">Slide 24</div>
    </div>

    <div class="slide">
        <h3>6.2 External Shared Storage (SAN & NAS)</h3>
        <p>While Ceph is great for internal storage, many enterprises already have massive external storage arrays (SANs). Proxmox connects to these using standard protocols.</p>
        <ul>
            <li>Network Attached Storage (NAS): Uses NFS or SMB. The storage array manages the filesystem. Proxmox simply mounts a folder. It's easy, but effectively "Serial" (files are locked individually).</li>
            <li>Storage Area Network (SAN): Uses iSCSI or Fibre Channel. Proxmox sees a raw block device over the network.</li>
            <li>Parallel / Cluster File Systems: To allow multiple Proxmox nodes to mount the same SAN LUN simultaneously and write to it without corrupting data, we use a Clustered File System like GFS2 (Global File System 2) or OCFS2.
Locking: These systems use a specialized Distributed Lock Manager (DLM) to ensure that if Node A is writing to a file, Node B knows about it instantly.</li>
            <li>LVM-Shared: Alternatively, Proxmox often uses LVM on top of iSCSI in "Shared Mode" to manage raw disk volumes for VMs without a full filesystem layer.</li>
        </ul>
        <div class="slide-number">Slide 25</div>
    </div>

    <div class="slide">
        <h3>Section 6 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Ceph (HCI): Distributed, self-healing storage on compute nodes (3+ nodes req).</li>
            <li>SAN/NAS: External storage arrays. Block (iSCSI/FC) vs File (NFS).</li>
            <li>Cluster FS: GFS2/OCFS2 needed for simultaneous shared writes.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why is a 10GbE network mandatory for Ceph?</li>
            <li>What happens if two servers write to a standard ext4 non-clustered disk at the same time?</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>Ceph Intro</li>
        </ul>
        <div class="slide-number">Slide 26</div>
    </div>

    <div class="slide">
        <h2>7. Proxmox Backups (VZDump)</h2>
        <div class="slide-number">Slide 27</div>
    </div>

    <div class="slide">
        <h3>7.1 Backups (VZDump) vs Snapshots</h3>
        <ul>
            <li>Snapshot: A point-in-time "difference file" linked to the original disk. Dependent.</li>
            <li>Backup (VZDump): A comprehensive, independent archive (config + compressed disk data, e.g., .vma.zst). It can be moved offsite for disaster recovery.</li>
        </ul>
        <p>Figure 9: Snapshot vs. Backup - Snapshots are dependent save points for testing; Backups are independent archives for disaster recovery</p>
        <div class="slide-number">Slide 28</div>
    </div>

    <div class="slide">
        <h3>7.2 Proxmox Backup Modes</h3>
        <p>When performing a backup, the state of the VM determines the consistency of the data.</p>
        <p>Figure 10: Proxmox Backup Modes - Live (Snapshot), Suspend (Frozen), and Stop (Consistent) modes balance uptime vs. data consistency</p>
        <div class="slide-number">Slide 29</div>
    </div>

    <div class="slide">
        <h3>Section 7 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Backup: Independent Archive, essential for DR.</li>
            <li>Modes: Snapshot (Live), Suspend (Frozen), Stop (Consistent).</li>
        </ul>
        <div class="slide-number">Slide 30</div>
    </div>

    <div class="slide">
        <h2>8. Managing Storage via CLI (pvesm)</h2>
        <div class="slide-number">Slide 31</div>
    </div>

    <div class="slide">
        <h3>Section 8 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>pvesm helps when the GUI is unavailable.</li>
            <li>storage.cfg is the cluster-wide storage definition file.</li>
        </ul>
        <div class="slide-number">Slide 32</div>
    </div>

    <div class="slide">
        <h2>9. Additional Resources</h2>
        <div class="slide-number">Slide 33</div>
    </div>

    <div class="slide">
        <h2>10. Lab Exercises</h2>
        <div class="slide-number">Slide 34</div>
    </div>

    <div class="slide title-slide">
        <h2>Summary</h2>
        <p style="font-size: 1.5em; margin-top: 30px;">Review the key concepts covered in this week's material</p>
        <p style="font-size: 1.2em; margin-top: 20px; color: #ffd700;">Questions?</p>
        <div class="slide-number">Slide 35</div>
    </div>

</body>
</html>