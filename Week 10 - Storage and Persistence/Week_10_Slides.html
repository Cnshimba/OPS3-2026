<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Storage and Persistence (Cinder) - Presentation Slides</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            color: #ffffff;
            line-height: 1.6;
        }
        
        .slide {
            min-height: 100vh;
            padding: 60px 80px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            page-break-after: always;
            border-bottom: 3px solid #0f3460;
        }
        
        .slide:nth-child(even) {
            background: linear-gradient(135deg, #16213e 0%, #1a1a2e 100%);
        }
        
        .title-slide {
            justify-content: center;
            align-items: center;
            text-align: center;
            background: linear-gradient(135deg, #0f3460 0%, #16213e 100%);
        }
        
        .title-slide h1 {
            font-size: 3.5em;
            margin-bottom: 20px;
            color: #e94560;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .title-slide .week-number {
            font-size: 1.5em;
            color: #ffd700;
            margin-bottom: 40px;
            letter-spacing: 2px;
        }
        
        .title-slide .course-code {
            font-size: 1.2em;
            color: #a8dadc;
            margin-top: 30px;
        }
        
        h2 {
            font-size: 2.5em;
            color: #e94560;
            margin-bottom: 30px;
            border-bottom: 3px solid #ffd700;
            padding-bottom: 15px;
        }
        
        h3 {
            font-size: 2em;
            color: #ffd700;
            margin-bottom: 25px;
        }
        
        ul {
            font-size: 1.4em;
            margin-left: 40px;
            margin-bottom: 20px;
        }
        
        ul li {
            margin-bottom: 15px;
            line-height: 1.8;
        }
        
        p {
            font-size: 1.3em;
            margin-bottom: 20px;
            line-height: 1.8;
        }
        
        .code-block {
            background: #0f0f0f;
            border-left: 4px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        pre {
            background: #0f0f0f;
            border-left: 4px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 1.1em;
            line-height: 1.5;
        }
        
        code {
            font-family: 'Courier New', monospace;
            color: #a8dadc;
        }
        
        .quote {
            background: rgba(233, 69, 96, 0.1);
            border-left: 5px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            font-style: italic;
            font-size: 1.2em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 1.2em;
        }
        
        th {
            background: #0f3460;
            color: #ffd700;
            padding: 15px;
            text-align: left;
            border: 1px solid #16213e;
        }
        
        td {
            padding: 12px;
            border: 1px solid #16213e;
            background: rgba(255, 255, 255, 0.05);
        }
        
        tr:hover {
            background: rgba(233, 69, 96, 0.1);
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 20px 0;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
        
        .key-points {
            background: rgba(255, 215, 0, 0.1);
            border: 2px solid #ffd700;
            padding: 30px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .vut-logo {
            position: fixed;
            bottom: 20px;
            right: 30px;
            height: 60px;
           width: auto;
            opacity: 0.8;
        }
        
        @media print {
            .slide {
                page-break-after: always;
            }
        }
    </style>
</head>
<body>

    <div class="slide title-slide">
        <div class="week-number">Week 10</div>
        <h1>Storage and Persistence (Cinder)</h1>
        <div class="course-code">OPS3 - Virtualization and Cloud Infrastructure</div>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>1. The Hierarchy of Cloud Storage</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>1.1 Ephemeral vs. Persistent Storage</h3>
        <ul>
            <li>It is crucial to distinguish between the two primary ephemeral and persistent storage models in cloud architectures.</li>
            <li>Ephemeral Storage (Nova) is strictly tied to the lifecycle of the Compute Instance.</li>
            <li>It functions effectively as a local scratch disk, optimized for operating system caches and temporary file processing.</li>
            <li>However, if the instance is terminated or the underlying hypervisor fails, this data is irrevocably lost.</li>
            <li>In contrast, Persistent Storage (Cinder) operates as an independent capability.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Section 1 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Ephemeral Storage (Nova): Temporary, fast, dies with the VM. Use for OS / Cache.</li>
            <li>Persistent Storage (Cinder): Durable, independent, survives VM deletion. Use for Databases / Critical Data.</li>
            <li>Analogy: Ephemeral is RAM/Swap; Persistent is the Hard Drive.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why shouldn't you store your customer MySQL database on the Nova Ephemeral disk?</li>
            <li>If you delete a Cinder Volume, is the data recoverable? (Hint: Only if you have a Backup).</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>2. Block Storage Architecture (Cinder)</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.1 The Driver Model</h3>
        <p>Just as Nova utilizes virtualization drivers to interact with various CPU architectures, Cinder employs a Volume Driver architecture to communicate with diverse storage backends.</p>
        <p>Figure 1: Cinder Architecture - The Cinder Scheduler selects the backend, and the Volume Driver translates API calls into storage commands</p>
        <ul>
            <li>Laboratory: The LVM Driver manages local logical volumes on a standard Linux server.</li>
            <li>Enterprise: Customized drivers for Dell EMC, NetApp, or HPE arrays translate API calls into proprietary storage commands.</li>
            <li>Scale-Out: The Ceph Driver allows Cinder to provision resources from a distributed, software-defined storage cluster. When a user executes a creation command, Cinder identifies the correct driver and signals the backend hardware to provision the requested Logical Unit Number (LUN).</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.2 The Attachment Process (iSCSI/RBD)</h3>
        <p>The mechanism for attaching a volume to an instance involves a coordinated handshake between services.</p>
        <p>Figure 2: The Attachment Handshake - How Nova and Cinder coordinate to plug a remote disk into a running VM</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.3 Deep Dive: Storage Backends</h3>
        <p>Cinder operates as an abstraction layer, capable of interfacing with a diverse array of storage backends. In private cloud environments, the two most prevalent technologies serve as excellent examples of this flexibility: the Network File System (NFS) and the Ceph distributed storage cluster.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.3.1 NFS (Network File System)</h3>
        <ul>
            <li>The Network File System (NFS) represents the simpler deployment model, often utilized in smaller environments or laboratories.</li>
            <li>In this architecture, Cinder acts as a client that mounts a remote directory from an existing NAS appliance or Linux server (e.g., 192.168.1.5:/toptier).</li>
            <li>When a user requests a new volume, the Cinder Volume service generates a large file, typically in the QCOW2 format, within this mounted directory.</li>
            <li>While this approach is notably easy to implement—requiring only a standard Linux server or a commercial NAS like Synology—it suffers from scalability limitations.</li>
            <li>The performance of the entire cloud storage pool is often constrained by the throughput of the single network link connecting the Controller to the NAS, creating a significant bottleneck and a single point of failure.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.3.2 Ceph (The Gold Standard)</h3>
        <ul>
            <li>In contrast, Ceph represents the industry standard for production-grade OpenStack deployments.</li>
            <li>As a software-defined storage solution, Ceph eliminates the need for a central storage controller.</li>
            <li>Instead, it aggregates storage capacity from hundreds of individual hard drives distributed across many physical servers, unifying them into a massive, scalable "Pool." The integration between Cinder and Ceph is facilitated by the librbd library, which allows Cinder to manage reliable RADOS Block Devices (RBD).</li>
        </ul>
        <ul>
            <li>Ceph distinguishes itself through its self-healing capabilities and advanced snapshotting mechanism.</li>
            <li>When data is written to a Ceph-backed volume, it is split into 4MB objects and scattered deterministically across the cluster.</li>
            <li>If a physical drive fails, the cluster automatically detects the missing objects and replicates them from surviving redundant copies, effectively healing the system without human intervention.</li>
            <li>Furthermore, because Ceph manages data as discrete objects, it can create instantaneous snapshots using a Copy-on-Write mechanism.</li>
            <li>This allows administrators to generate thousands of recovery points without incurring the performance penalties associated with traditional storage arrays.</li>
        </ul>
        <p>Implementation Note: In production, Ceph is the preferred backend because it decouples storage from compute hardware entirely, allowing indefinite scaling.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.3.3 Configuring Cinder with Ceph</h3>
        <div class="quote">Note: This assumes a Ceph cluster is already running. To learn how to build one from scratch, see the Optional Ceph Setup Guide.</div>
        <p>To configure OpenStack Cinder to use a Ceph cluster as its backend, the administrator must edit the cinder.conf file on the Controller node. The process involves three key steps: installing the client libraries, authenticating, and defining the driver.</p>
        <p>1. Install Ceph Client:
The Cinder service requires the python libraries to communicate with the Ceph public network.</p>
        <p>2. Authentication (Keyring):
OpenStack acts as a client "user" to the Ceph cluster. You must copy the authentication keyring from the Ceph Monitor node to the Cinder node.</p>
        <p>3. Driver Configuration (/etc/cinder/cinder.conf):
Define a new backend section (e.g., [ceph]) and reference it in the enabled_backends list.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.3.4 Configuring Cinder with NFS</h3>
        <p>For smaller deployments or lab environments, NFS is a common backend. It requires a dedicated text file to list the shares and a specific driver configuration.</p>
        <p>1. Create Shares File:
Create a text file (e.g., /etc/cinder/nfs_shares) and list your NFS exports, one per line.</p>
        <p>2. Set Permissions:
Ensure the Cinder user can read this file.</p>
        <p>3. Driver Configuration (/etc/cinder/cinder.conf):</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Section 2 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Cinder: Manages block storage (Creating/Attaching volumes).</li>
            <li>Backends: Connects to LVM (Local), NFS (File), or Ceph (Distributed).</li>
            <li>Ceph: The Gold Standard for OpenStack. Self-healing, scalable, compliant.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why is Ceph preferred over NFS for large clouds? (Hint: Single Point of Failure).</li>
            <li>What happens to a generic "File" on an NFS share when Cinder creates a volume? (It becomes a .qcow2 or .raw disk image).</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>3. Data Safety Strategies</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>3.1 Snapshots (The Time Machine)</h3>
        <ul>
            <li>A Snapshot represents a point-in-time copy of a specific volume using a "Copy-on-Write" (Redirect on Write) mechanism.</li>
            <li>This technique ensures that the snapshot is created nearly instantly, as it relies on the existing data blocks rather than duplicating the entire drive volume.</li>
            <li>Snapshots are invaluable for functional recovery scenarios, such as capturing the state of a database before a major upgrade; if the upgrade fails, the administrator can rollback instantly.</li>
            <li>However, it is critical to note that snapshots typically reside on the same physical hardware as the source volume.</li>
            <li>Therefore, if the underlying storage array experiences a catastrophic failure, both the active volume and its snapshots will be lost.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>3.2 Backups (The Disaster Plan)</h3>
        <p>To mitigate the risk of physical hardware failure, Backups provide a complete disaster recovery solution.</p>
        <p>Figure 3: Block vs Object Storage - Cinder Backups move data from expensive, fast Block Storage to cheap, durable Object Storage (Swift/S3)</p>
        <ul>
            <li>A backup involves reading the full content of a block volume and transferring it to a separate, physically isolated system—typically an Object Storage service like Swift or Amazon S3.</li>
            <li>Although this process is slower due to network transfer requirements, it ensures data survivability.</li>
            <li>If the primary SAN or Ceph cluster were to be destroyed by fire or malfunction, the data could still be restored from the backup repository located in a different rack or data center.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>3.3 Architecting Redundancy: Public vs. Private</h3>
        <ul>
            <li>In a Public Cloud (AWS/Azure), achieving higher redundancy is often as simple as selecting a premium tier in a dropdown menu.</li>
            <li>However, in a Private Cloud environment using OpenStack, you are the architect responsible for building these layers yourself.</li>
            <li>Understanding how standard cloud redundancy levels map to OpenStack implementation is crucial for designing robust infrastructure.</li>
        </ul>
        <ul>
            <li>Level 1: Local Redundancy (LRS)
The foundational level of data safety is Local Redundancy, known as LRS in Azure or EBS in AWS.</li>
            <li>This concept ensures that data survives disk failures within a single rack or datacenter.</li>
            <li>In an OpenStack private cloud, this is achieved natively through Ceph.</li>
            <li>By default, Ceph creates a "Replica=3" pool, which automatically stores three copies of every object on different physical Object Storage Daemons (OSDs).</li>
            <li>If a physical drive fails, the system self-heals by replicating the data from the surviving copies to a new drive, mirroring the durability guarantees of public cloud LRS.</li>
        </ul>
        <ul>
            <li>Level 2: Zonal Redundancy (ZRS)
The next tier is Zonal Redundancy (ZRS), designed to ensure data survives the total destruction of a building due to fire or power loss.</li>
            <li>Public clouds implement this by replicating data across distinct Availability Zones—separate facilities with independent power and cooling.</li>
            <li>In OpenStack, you replicate this architecture using Cinder Availability Zones.</li>
            <li>By modifying cinder.conf, an administrator can define logical zones (e.g., zone-A, zone-B) and map them to specific storage racks or entirely separate Ceph clusters.</li>
            <li>This requires users to consciously select a zone when provisioning a volume, ensuring their application architecture can withstand a facility-level failure.</li>
        </ul>
        <ul>
            <li>Level 3: Geo Redundancy (GRS)
The highest level of protection is Geo Redundancy (GRS), which ensures survival against regional catastrophes, such as major natural disasters.</li>
            <li>While public clouds handle this via asynchronous replication between regions (e.g., North Europe to West Europe), a private cloud architect typically implements this using Cinder Backup.</li>
            <li>By configuring Cinder to send volume backups to a remote Swift Object Storage cluster located in a different city, you guarantee that critical data can be restored even if the primary datacenter is lost.</li>
            <li>More advanced (and expensive) setups can also utilize driver-level volume replication for real-time Active/Passive disaster recovery.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Section 3 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Snapshot: Quick, local, dependent on source. Use for "Undo" before changes.</li>
            <li>Backup: Slow, remote, independent. Use for Disaster Recovery (Fire/Flood).</li>
            <li>Redundancy: LRS (Disk Fail), ZRS (Rack/Building Fail), GRS (City Fail).</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why isn't a Snapshot considered a true Backup?</li>
            <li>Which Cinder feature would you use to protect against a data center power outage? (Cinder Backup / Replication).</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>4. Operations Cookbook (Nebula Inc.)</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>4.1 Creating a Volume</h3>
        <p>We begin by provisioning a specific persistent volume for our database. This is analogous to purchasing a physical hard drive; initially, it exists as an unattached operational resource within the storage inventory.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>4.2 Attaching the Volume</h3>
        <p>Once created, the volume must be physically connected to the compute instance. This command mimics the act of plugging a USB drive or SAS cable into a running server.</p>
        <ul>
            <li>Verification:
    bash
    openstack volume list
    # Status should be "in-use"</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>4.3 Formatting and Mounting (Guest OS)</h3>
        <p>It is important to remember that Cinder delivers a raw block device (e.g., /dev/vdb) without any file system. The administrator must log into the Guest OS to format the disk and mount it for use effectively transferring responsibility from the "Cloud Provider" to the "OS Administrator".</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>4.4 Snapshotting</h3>
        <p>Before executing any destructive changes or updates to the database, standard procedure dictates creating a snapshot to preserve the current state.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Section 4 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Process: Create Volume -> Attach to VM -> Format (mkfs) -> Mount.</li>
            <li>Guest Responsibility: The Cloud Provider connects the wire; YOU must format the disk.</li>
            <li>Persistence: Data survives detach/reattach and VM deletion.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why doesn't the volume show up automatically in /mnt when you attach it?</li>
            <li>What Linux command lists all block devices? (lsblk).</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>5. Industry Comparison: Storage</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>7. Summary and Next Steps</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Preparing for Week 11</h3>
        <p>Next week, we stop clicking buttons manually. We will introduce Automation and APIs. We will learn how to deploy this entire infrastructure using code (Bash/Python) and configuration scripts (Cloud-Init), moving from "Pets" to "Cattle" effectively.</p>
        <p>Checklist:</p>
        <ul>
            <li>Can you differentiate between nova-compute storage and cinder-volume storage?</li>
            <li>Do you understand why we need to format a volume after attaching it?</li>
            <li>Review your Linux command line skills (loops and variables) for next week.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>8. Additional Resources</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>9. Lab Exercises</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide title-slide">
        <h2>Summary</h2>
        <p style="font-size: 1.5em; margin-top: 30px;">Review the key concepts covered in this week's material</p>
        <p style="font-size: 1.2em; margin-top: 20px; color: #ffd700;">Questions?</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

</body>
</html>