// AUTO-GENERATED BY scripts/generate_course_context.py
// DO NOT EDIT MANUALLY
// Generated on: Now

const COURSE_CONTEXT = "--- WEEK 1 NOTES ---\nIntroduction to Virtualization Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Introduction to Virtualization The creation of virtual versions of physical computing resources Estimated Reading Time : 25 Minutes [!TIP] How to succeed in this week : Focus on understanding the WHY before the HOW. Virtualization The creation of virtual versions of physical computing resources solves real problems\u2014hardware consolidation, isolation, and flexibility. As you read, ask yourself: \"What problem does this solve?\" The lab will make the theory concrete. Welcome to Week 1! This document serves as your comprehensive guide to understanding virtualization The creation of virtual versions of physical computing resources technology and its foundational role in modern computing infrastructure. Virtualization The creation of virtual versions of physical computing resources has fundamentally transformed how organizations deploy, manage, and scale their IT resources. Important Note : This week is dedicated to Conceptual Mastery . We will explore the architecture of the \"Software Defined Data Center\" (SDDC)\u2014including Compute, Network, and Storage virtualization The creation of virtual versions of physical computing resources . Unlike future weeks which are heavy on implementation, this week focuses on understanding the why and how of the technology before we touch the what . This foundational knowledge is critical; you cannot build a skyscraper without a blueprint. Next week, we will begin our implementation journey by provisioning our Hypervisor Software that creates and manages virtual machines ( Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization VE) and creating our first Virtual Machines. For now, we focus on the architecture and verifying our hardware readiness. What You'll Learn This Week The Virtualization The creation of virtual versions of physical computing resources Abstraction : How software lies to hardware to enable efficiency. Hypervisor Software that creates and manages virtual machines Architectures : The critical difference between Type-1 (Bare Metal) and Type-2 (Hosted) systems. The Software Defined Data Center (SDDC) : Moving beyond just VMs to visualize Virtual Networking ( SDN Software-Defined Networking - Software-based network control ) and Software Defined Storage (SDS). The Modern Roadmap : How virtualization The creation of virtual versions of physical computing resources serves as the bedrock for Cloud Computing Computing services delivered over the internet and Containerization. 1. Understanding Virtualization Virtualization is a technology that enables the creation of virtual instances of computing resources, allowing multiple operating systems and applications to run concurrently on a single physical hardware platform. At its core, virtualization abstracts the physical hardware layer, presenting virtualized resources to guest operating systems as if they were dedicated physical components. This abstraction is what enables a single physical server with sufficient resources to host multiple virtual machines, each running its own operating system and applications in isolation from one another. 1.1 The Concept of Abstraction The fundamental principle underlying virtualization is abstraction. In traditional computing, an operating system has direct control over the physical hardware. The OS kernel communicates with the CPU, manages physical memory, controls disk I/O operations, and handles network communication through direct interaction with hardware devices. This tight coupling between the operating system and hardware creates limitations, particularly the one-to-one relationship between a physical machine and the operating system it runs. Virtualization breaks this constraint by introducing an intermediary software layer called a hypervisor. The hypervisor sits between the physical hardware and the operating systems, presenting virtualized hardware resources to each guest operating system. To the guest OS , these virtual resources appear identical to physical hardware. The guest OS remains unaware that it is running in a virtual environment rather than on dedicated physical hardware. This deception is the essence of virtualization and enables multiple operating systems to coexist on the same physical platform. Figure 1: The Abstraction Layer introduced by Virtualization The abstraction applies to all major hardware components. Physical CPU cores are divided into virtual CPUs (vCPUs) that can be allocated to different virtual machines. Physical memory is partitioned, with each VM - receiving a dedicated allocation that appears to the guest OS as physical RAM. Note: While the Hypervisor handles these \"Compute\" abstractions directly, modern virtualization also abstracts the connectivity (Network) and persistence (Storage) layers. We will explore those broader concepts in Section 3: Beyond Compute . 1.2 Historical Context and Evolution Virtualization is not a new concept. IBM developed the first virtualization systems in the 1960s for their mainframe computers, enabling multiple users to share expensive hardware resources efficiently. However, virtualization in the x86 architecture, which dominates modern computing, faced significant technical challenges. The x86 architecture was not originally designed with virtualization in mind, making it difficult to virtualize efficiently. This changed dramatically in 2005-2006 when Intel and AMD introduced hardware virtualization extensions to their processors. Intel's VT-x (Virtualization Technology) and AMD's AMD-V provided the necessary hardware support to efficiently virtualize x86 systems. These extensions allow the processor to directly support virtualization operations, eliminating the need for complex and performance-degrading software workarounds. The introduction of these hardware features coincided with the emergence of modern open-source virtualization solutions like KVM - Type 1 hypervisor , which was merged into the Linux kernel in 2007. 1.3 Practical Benefits of Virtualization The adoption of virtualization technology delivers several concrete benefits that have made it nearly ubiquitous in modern data centers. Server consolidation represents one of the most immediate and measurable advantages. Organizations that previously required dozens or hundreds of physical servers can consolidate these workloads onto a smaller number of more powerful physical hosts running multiple virtual machines. This consolidation reduces capital expenditure on hardware, lowers power consumption, decreases cooling requirements, and reduces the physical space needed for infrastructure. Consider a concrete example: A medium-sized organization running 20 physical servers, each consuming 200 watts of power, uses 4,000 watts continuously. With appropriate consolidation onto 4 physical hosts running virtual machines, power consumption might drop to 1,200 watts while maintaining the same computational capacity. Over a year, this represents significant savings in electricity costs and reduced cooling requirements, which often consume as much power as the servers themselves. Isolation and security benefits emerge from the separation between virtual machines. Each VM - operates in its own isolated environment. If one VM - experiences a kernel panic, becomes compromised by malware, or suffers a critical software failure, other VMs on the same physical host continue operating normally. This isolation extends beyond mere process separation within a single OS. VMs have separate virtual hardware, separate kernel instances, and separate network stacks, providing strong isolation boundaries that enhance security and stability. Operational flexibility represents another significant advantage. Deploying a new physical server traditionally required procurement, delivery, racking, cabling, OS installation, and configuration\u2014a process that could take days or weeks. Creating a new virtual machine can be accomplished in minutes. Need a test environment identical to production? Clone the production VM - and have a perfect replica ready immediately. Need to test a software upgrade? Take a snapshot - of the current VM - state, perform the upgrade, and if something goes wrong, restore the snapshot - to return to the previous state in seconds. Section 1 Checkpoint Summary : Virtualization abstracts physical hardware to run multiple OSs concurrently. The Hypervisor is the intermediary layer managing this abstraction. Benefits include server consolidation, isolation, and rapid provisioning. Reflection : How does the concept of \"abstraction\" apply to other areas of computing (e.g., programming languages)? Why were hardware virtualization extensions (VT-x/AMD-V) necessary for x86 architecture? Resources : Red Hat: What is Virtualization? 2. Types of Hypervisors A hypervisor, also called a Virtual Machine Monitor (VMM), is the software component responsible for creating and managing virtual machines. The hypervisor provides the virtualization layer, mediates access to physical hardware, schedules virtual CPU execution, manages memory allocation, and handles I/O operations between VMs and physical devices. There are two fundamentally different architectural approaches to implementing hypervisors, classified as Type-1 and Type-2 , each with distinct characteristics, performance profiles, and use cases. Figure 2: Architectural differences between Type-1 and Type-2 Hypervisors 2.1 Type-1: Bare-Metal Hypervisors Type-1 hypervisors, commonly referred to as bare-metal hypervisors, run directly on the physical hardware without any intervening operating system. The hypervisor itself serves as the operating system, providing a minimal, purpose-built environment optimized solely for running virtual machines. This architecture eliminates an entire software layer compared to Type-2 hypervisors, which has significant implications for performance, security, and manageability. When a physical server boots with a Type-1 hypervisor installed, the hypervisor loads directly from the boot device and initializes the hardware. The hypervisor takes complete control of physical resources including CPU cores, memory, storage devices, and network interfaces. It then presents virtualized versions of these resources to the virtual machines it hosts. Because there is no intermediary operating system consuming resources or adding processing overhead, Type-1 hypervisors can achieve near-native performance. The architecture of a Type-1 hypervisor consists of several key components. The hypervisor kernel manages basic hardware interaction, CPU virtualization, and memory management. Device drivers enable the hypervisor to communicate with physical hardware such as network cards, storage controllers, and management interfaces. A management layer provides interfaces for administrators to create, configure, and control virtual machines. In modern hypervisors like Proxmox platform combining KVM - Type 1 hypervisor and LXC VE , this management layer includes a web-based interface that simplifies VM - lifecycle management. Major examples of Type-1 hypervisors include Proxmox platform combining KVM - Type 1 hypervisor and LXC VE , which we will use throughout this course, VMware ESXi , which dominates enterprise virtualization deployments, Microsoft Hyper-V , which powers Azure's cloud infrastructure, and Xen , which is used by Amazon Web Services for EC2 instances. These hypervisors share common characteristics: they install directly on server hardware, provide high performance through direct hardware access, and are designed for production workloads requiring reliability and efficiency. The performance advantage of Type-1 hypervisors stems from their direct hardware access. When a virtual CPU needs to execute instructions, the hypervisor can schedule that vCPU - to run directly on a physical CPU core with minimal overhead. Hardware virtualization extensions like Intel VT-x and AMD-V enable the processor to switch between different virtual machines efficiently, with the hypervisor maintaining control when necessary but allowing VMs to execute at near-native speed most of the time. Security benefits also emerge from the Type-1 architecture. With no underlying operating system, the attack surface is dramatically reduced compared to Type-2 hypervisors. There is no general-purpose OS with system services, user accounts, or application software that could contain vulnerabilities. The hypervisor is purpose-built for virtualization, with a minimal codebase focused solely on managing virtual machines, which reduces the potential for security flaws. Type-1 hypervisors are the standard choice for production server environments, data centers, and cloud infrastructure. They excel in scenarios where performance, reliability, and resource efficiency are paramount. Organizations running business-critical applications, operating 24/7 services, or managing large-scale virtualization deployments invariably choose Type-1 hypervisors for their infrastructure. 2.2 Type-2: Hosted Hypervisors Type-2 hypervisors take a fundamentally different architectural approach. Rather than running directly on hardware, a Type-2 hypervisor runs as an application on top of a conventional operating system. The host operating system retains control of the physical hardware, and the hypervisor software requests resources from this host OS rather than managing hardware directly. This creates an additional layer in the software stack: physical hardware, host operating system, hypervisor application, and finally the virtual machines. The layered architecture of Type-2 hypervisors has significant implications. When a VM - running on a Type-2 hypervisor needs to perform an operation, the request passes through multiple layers. For example, a disk write operation initiated by a guest OS goes to the virtual disk - controller, which the hypervisor application handles by making system calls to the host operating system, which then interacts with the physical storage hardware. Each layer adds processing overhead and context switching, resulting in reduced performance compared to Type-1 hypervisors. Common examples of Type-2 hypervisors include Oracle VirtualBox , which is widely used for desktop virtualization and testing, VMware Workstation , which provides advanced features for developers and testers on Windows and Linux hosts, and Parallels Desktop , which enables macOS users to run Windows and Linux virtual machines. These products are designed primarily for desktop and development use cases rather than production server deployments. The major advantage of Type-2 hypervisors is their accessibility and ease of use. Installation involves downloading the hypervisor software and running a standard installation wizard on your existing operating system, whether Windows, macOS, or Linux. There is no need to dedicate the entire physical machine to virtualization; you can continue using your computer normally while running virtual machines when needed. This makes Type-2 hypervisors excellent choices for developers who need to test software on different operating systems, IT professionals learning virtualization concepts, or users who occasionally need to run applications from different operating systems. However, Type-2 hypervisors have clear limitations. The performance overhead from the additional OS layer makes them unsuitable for performance-sensitive production workloads. The host operating system consumes significant resources that are therefore unavailable to virtual machines. If the host OS experiences problems or requires maintenance and reboots, all running VMs are affected. These factors make Type-2 hypervisors inappropriate for production server deployments where reliability and performance are critical. The distinction between Type-1 and Type-2 hypervisors is fundamental to understanding virtualization architecture. Type-1 hypervisors represent the professional, production-grade approach optimized for performance and reliability at the cost of dedicating hardware to virtualization. Type-2 hypervisors represent a more accessible, flexible approach suitable for development, testing, and learning, accepting performance trade-offs in exchange for the convenience of running on an existing desktop operating system. For this course, we use Proxmox platform combining KVM - Type 1 hypervisor and LXC VE, a Type-1 hypervisor, because it provides professional-grade capabilities while remaining accessible for learning purposes. Section 2 Checkpoint Summary : Type-1 (Bare Metal) : Runs directly on hardware (Proxmox platform combining KVM - Type 1 hypervisor and LXC , ESXi). Best for performance/security. Type-2 (Hosted) : Runs as an app on an OS (VirtualBox). Best for testing/desktops. Major trade-off is Performance vs. Convenience. Reflection : In what specific scenario would a Type-2 hypervisor be preferred over Type-1? Why is the \"attack surface\" of a Type-1 hypervisor considered smaller? Resources : VMware: Type 1 vs Type 2 Hypervisors 3. Virtual Machine Anatomy (Concepts & Components) Now that we understand the software that manages virtualization (The Hypervisor), we must examine the entity it manages: the Virtual Machine itself. Regardless of whether you use a Type-1 or Type-2 hypervisor, every VM - is constructed from the same set of standardized software components that mimic physical hardware. Understanding these components\u2014and the rules for allocating them\u2014is critical for building stable systems. 3.1 The Virtual CPU (vCPU - ) The most critical resource for any computer is its processor. The vCPU - is the processing unit of your virtual machine, allowing the guest operating system to execute instructions as if it had exclusive access to a core. In reality, the hypervisor schedules the vCPU - on a physical CPU core for short durations, known as time-slicing. A critical rule in virtualization is to never allocate more vCPUs than you have physical cores . Violating this leads to \"CPU Contention,\" where multiple VMs fight for the scheduler's attention, causing significant performance degradation across the entire system. When configuring local resources, you must also choose a CPU Type. The Host Mode passes your exact physical CPU model and instruction set to the VM - , offering maximum performance and transparency. However, this restricts portability; a VM - created in Host Mode on an Intel server may crash if migrated to an AMD server. Alternatively, the Generic (kvm64) type presents a standard, simplified processor to the VM - , guaranteeing it can run on any hardware at the cost of some performance optimizations. 3.2 Virtual RAM (vRAM) While CPU time can be shared, memory is a finite resource. vRAM differs significantly from vCPU - allocation because memory cannot be easily time-sliced. When you assign vRAM, the hypervisor allocates a dedicated block of physical RAM to that VM - . Therefore, the golden rule of memory allocation is to never over-commit RAM . Unlike CPU cycles, which can be queued, RAM is a hard limit. If you assign 32GB of RAM to VMs on a server with only 16GB of physical capacity, the system will inevitably crash or freeze as it attempts to swap memory to the much slower hard drive. As a best practice, always reserve at least 2GB of unallocated physical RAM for the Proxmox platform combining KVM - Type 1 hypervisor and LXC host itself to ensure stability. 3.3 Virtual Disk - (Storage) Once processing and memory are handled, a VM - needs a place to persist data. To a virtual machine, the vDisk appears as a standard SATA or SCSI hard drive, but to the hypervisor, it is simply a file (encapsulation). This makes backing up a VM - as simple as copying a file. Administrators must choose between two provisioning methods. Thin Provisioning (QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format ) creates a file that starts small and grows only as data is written, making it space-efficient but requiring careful monitoring to prevent filling the physical storage. Thick Provisioning (RAW) allocates the entire disk space immediately; while this consumes more storage upfront, it offers slightly better performance by eliminating the overhead of dynamic growth. Comparison of Virtual Disk - Formats : Format Features Use Case RAW Zero overhead, Pre-allocated. Best performance; Database servers. QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format Thin provisioning, Snapshots. General Cloud/Home Lab usage. VMDK VMware compatible. Corporate data centers using vSphere. VHDX Microsoft compatible. Hyper-V & Azure. 3.4 Virtual Network - communication Interface (vNIC) Finally, for a VM - to communicate with the outside world, it requires a vNIC . This component connects the VM - to a virtual switch (Bridge) on the host, acting as the bridge between the virtual and physical networks. You will typically choose between two types of interfaces. The Emulated (E1000) interface mimics a real Intel network card, ensuring compatibility with almost any operating system out of the box, though it incurs higher CPU overhead. For performance-critical workloads, the Paravirtualized (VirtIO) interface is preferred; it is a software-defined card designed specifically for virtualization that works directly with the hypervisor to achieve network speeds often exceeding 10Gbps, though it requires specific drivers in the guest OS . Section 3 Checkpoint Summary : vCPU - : Do not over-allocate cores. Use 'Host' type for speed. vRAM : RAM is a hard limit. Do not over-commit. vDisk : RAW is fast (Thick), QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format is flexible (Thin). vNIC : Use VirtIO for performance, E1000 for compatibility. Reflection : Why is \"Thin Provisioning\" dangerous if you are not monitoring your storage? If you have an 8-Core CPU, why is it bad to give a single VM - 8 vCPUs? 4. Beyond Compute: The Software Defined Data Center So far, we have focused on Compute Virtualization \u2014abstracting the CPU and RAM. But a modern data center is more than just processors; it is a complex web of Cables (Networking) and Hard Drives (Storage). In a traditional data center, these are rigid physical appliances. in a Virtualized Data Center, they become software. This holistic approach is called the Software Defined Data Center (SDDC) . Figure 3: The Pillars of the Software Defined Data Center 4.1 Network Virtualization (SDN) In the physical world, connecting servers requires physical switches and cabling. Software Defined Networking (SDN) eliminates this physical dependency by introducing the virtual switch\u2014a software component residing within the hypervisor that functions exactly like its physical counterpart. This architecture decouples the network's control plane (intelligence) from the data plane (packet forwarding), allowing administrators to create complex network topologies, VLANs, and firewalls programmatically without ever touching a physical cable. We will explore these concepts in depth, including Linux Bridges, in Week 4: Virtual Networking . 4.2 Storage Virtualization (SDS) Software Defined Storage (SDS) fundamentally changes how we manage data persistence. Rather than relying on isolated physical disks attached to specific servers, SDS aggregates storage devices from multiple servers into a unified, reliable storage pool. This abstraction allows for enterprise-level features such as self-healing resilience; if a physical drive fails, the software automatically rebuilds the data on remaining drives without interrupting the virtual machine. By treating storage as a flexible software resource rather than a rigid hardware appliance, we gain immense scalability. These technologies, including Ceph and NFS, are the focus of Week 5: Storage & Backup . Section 4 Checkpoint Summary : SDDC : Visualizing Compute, Network, and Storage together. SDN : Moving network intelligence into software (Virtual Switches). SDS : Abstracting physical disks into flexible Storage Pools. Reflection : If the network is \"software,\" does it still need physical cables at all? How does SDS differ from a traditional RAID card? 5. The Future of Virtualization Virtualization is no longer just about optimizing server hardware; it is the fundamental building block of Cloud Computing. 5.1 From Virtualization to Cloud (IaaS) When you provision a virtual machine instance in AWS, Azure, or Google Cloud, you are essentially consuming a virtualized resource managed by a massive hypervisor farm. The Infrastructure-as-a-Service (IaaS) model exposes this virtualization as a service via APIs, relying on two core capabilities. First, Multi-tenancy allows multiple distinct customers (tenants) to run their workloads on the exact same physical hardware, securely isolated from one another by the hypervisor. Second, Elasticity leverages the fact that VMs are merely files and processes, allowing them to be provisioned, scaled, or destroyed in seconds to match demand\u2014something impossible with physical hardware. 5.2 Containers and Serverless By mastering the abstraction layer (the Hypervisor), you unlock the ability to understand even lighter forms of abstraction, such as Containers (Docker) , which share the OS kernel, and Serverless Functions , which abstract the OS entirely. We will explore these evolutions specifically in Week 3: Containers & Resources . Section 5 Checkpoint Summary : Virtualization is the bedrock of Cloud Computing (AWS, Azure). Abstraction enables flexibility, security, and efficiency. Type-1 vs Type-2 is the first major architectural decision. Reflection : How does virtualization enable \"Elasticity\" in the cloud? Can a single physical server run both Type-1 and Type-2 hypervisors simultaneously? Resources : IBM: What is Cloud Computing? 6. Summary and Next Steps This week laid the theoretical groundwork. You now understand that \"Virtualization\" is simply the art of deception\u2014tricking operating systems into believing they have dedicated hardware. You know the key difference between running a hypervisor directly on iron (Type-1) versus running it as an app (Type-2). Preparing for Week 2 Next week, we move from theory to heavy implementation. We will: Deconstruct the KVM - Type 1 hypervisor architecture to see how it uses the Linux Kernel. Deploy Proxmox platform combining KVM - Type 1 hypervisor and LXC VE on bare metal in our \"Mastery\" lab. Master the art of creating, managing, and optimizing Virtual Machines . Checklist: Can you clearly explain the role of a Hypervisor? Do you know which Hypervisor type is best for a Production Database vs a Developer Laptop? Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Course Index Next: Week 2: Virtual Machines\n\n--- WEEK 2 NOTES ---\nVirtual Machines (VMs) Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Virtual Machines (VMs) Estimated Reading Time : 30 Minutes [!TIP] How to succeed in this week : VMs are the building blocks of infrastructure. Pay close attention to the lifecycle operations (create, configure, snapshot Point-in-time copy of VM state for rollback - , clone An exact copy of a virtual machine ). These patterns repeat in every cloud platform you'll encounter. Practice the CLI Command Line Interface commands\u2014they're faster than the GUI. Welcome to Week 2! Having established the theoretical foundations of virtualization The creation of virtual versions of physical computing resources , we now shift our focus to heavy technical implementation. This week is a deep dive into the specific technologies that power modern open-source virtualization The creation of virtual versions of physical computing resources : KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware (the engine), QEMU Quick Emulator - Works with KVM for virtualization Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware for virtualization The creation of virtual versions of physical computing resources (the emulator), and Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization VE (the platform). We will deconstruct their compositions, explore their specific functions, and master their configurations. By the end of this week, you will move beyond theory to practice, demonstrating the ability to deploy virtual machines manually using QEMU Quick Emulator - Works with KVM for virtualization Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware for virtualization The creation of virtual versions of physical computing resources commands and efficiently managing them via the Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization interface. What You'll Learn This Week Create and configure virtual machines Understand CPU, RAM, and disk allocation strategies Install operating systems inside VMs Utilize the QEMU Quick Emulator - Works with KVM for virtualization Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware for virtualization The creation of virtual versions of physical computing resources Guest Agent Perform cloning, snapshots, and other VM Virtual Machine - A software-based emulation of a physical computer - management tasks 1. Deep Dive: Linux Virtualization (KVM - Type 1 hypervisor & QEMU - Type 1 hypervisor for virtualization ) Modern open-source virtualization relies on two pillars: KVM - Type 1 hypervisor (The Engine) and QEMU - Type 1 hypervisor for virtualization (The Hardware). Understanding how they interact is the key to mastering Linux virtualization, as these form the foundation upon which many enterprise platforms are built. 1.1 The Engine: KVM - Type 1 hypervisor (Kernel-based Virtual Machine) The Kernel-based Virtual Machine (KVM - Type 1 hypervisor ) is implemented as a loadable kernel module ( kvm.ko ) that transforms the Linux kernel into a Type-1 hypervisor. Unlike traditional hypervisors that run as a separate software layer, KVM - Type 1 hypervisor leverages the kernel's existing process scheduling, memory management, and I/O stack. It exposes a character device, /dev/kvm , which user-space processes (like QEMU - Type 1 hypervisor for virtualization ) interact with via ioctl() system calls to create and run virtual machines. The VM - Execution Loop (ioctl Interface) : The interaction between User Mode (the QEMU - Type 1 hypervisor for virtualization process) and Guest Mode (the VM - code) is handled via a blocking system call known as KVM_RUN . The process begins with Setup , where QEMU - Type 1 hypervisor for virtualization opens the /dev/kvm device node, issues the KVM_CREATE_VM call to initialize the virtual environment, and maps the necessary memory for the guest. Once initialized, the Execution phase begins: QEMU - Type 1 hypervisor for virtualization invokes the KVM_RUN ioctl, signaling the kernel to context-switch the CPU into Guest Mode (Ring -1 or VMX Root Operation). In this mode, the vCPU - executes instructions directly on the silicon at native speed (\"Direct Execution\"). This continues until the VM - attempts a privileged operation, such as writing to a hardware register or accessing restricted memory, which triggers a VM - Exit . The hardware forces the CPU back into Host Kernel Mode, where KVM - Type 1 hypervisor analyzes the exit reason. If the exit is \"Lightweight\" (e.g., a simple timer interrupt or paging request), KVM - Type 1 hypervisor handles it internally and immediately re-enters the VM - . However, if the exit is \"Heavyweight\" (requiring complex I/O like disk writes), KVM - Type 1 hypervisor returns control to the QEMU - Type 1 hypervisor for virtualization user-space process. QEMU - Type 1 hypervisor for virtualization then performs the necessary Emulation for the I/O operation and calls KVM_RUN again to resume execution, completing the loop. Figure 1.2: The Cycle of Direct Execution and Trapped Emulation. Code Snippet: Creating a VM - via KVM - Type 1 hypervisor API (C) // simplified example of creating a VM - in C using the KVM - Type 1 hypervisor API int kvm - Type 1 hypervisor = open ( \"/dev/kvm - Type 1 hypervisor \" , O_RDWR | O_CLOEXEC ); int vmfd = ioctl ( kvm - Type 1 hypervisor , KVM_CREATE_VM , 0 ); // Map 1GB of memory for the guest void * mem = mmap ( NULL , 1 << 30 , PROT_READ | PROT_WRITE , MAP_PRIVATE | MAP_ANONYMOUS , -1 , 0 ); struct kvm_userspace_memory_region region = { . slot = 0 , . guest_phys_addr = 0 , . memory_size = 1 << 30 , . userspace_addr = ( uint64_t ) mem , }; ioctl ( vmfd , KVM_SET_USER_MEMORY_REGION , & region ); Figure 1.1: Simplified C code showing how a user-space program instructs the kernel to create a VM - . 1.2 Advanced KVM - Type 1 hypervisor Technologies Modern KVM - Type 1 hypervisor deployments leverage several advanced kernel features to maximize efficiency: 1.2.1 KSM (Kernel Samepage Merging) Kernel Samepage Merging (KSM) is a memory deduplication feature within the Linux kernel (stored in mm/ksm.c ) that allows for high-density virtualization. KSM utilizes a background kernel thread ( ksmd ) that periodically scans designated memory regions looking for pages with identical content. When matches are found, KSM merges these pages into a single physical page marked as Copy-on-Write (CoW). This allows multiple virtual machines running similar operating systems (e.g., ten instances of Windows Server) to share the same physical RAM for common libraries and kernel data. While this significantly increases density (memory overcommitment), it comes with a slight CPU overhead due to the scanning process. 1.2.2 Live Migration - between hosts without downtime Live Migration - between hosts without downtime is the capability to move a running virtual machine from one physical host to another with no perceptible downtime to the end-user. The process involves an iterative memory copy mechanism. First, the hypervisor copies the VM - 's memory to the destination host while the VM - continues to execute. As memory changes (becomes \"dirty\") during this copy, those dirty pages are tracked and re-sent in subsequent rounds. Once the remaining dirty pages are small enough to be transferred instantly, the VM - is momentarily paused, the final state (CPU registers and remaining memory) is synced, and execution resumes on the new host. 1.2.3 Nested Virtualization Nested Virtualization refers to the practice of running a hypervisor inside another virtual machine\u2014effectively, a \"VM - inside a VM - .\" This is achieved by forwarding hardware virtualization extensions (Intel VT-x or AMD-V) from the host CPU through Level 0 (Host Hypervisor) to Level 1 (Guest Hypervisor). This is particularly useful for training environments, development labs, or testing cloud orchestration platforms like OpenStack platform without requiring dedicated bare-metal hardware for every node. 1.2.4 PCI Passthrough (VFIO) Standard virtualization relies on QEMU - Type 1 hypervisor for virtualization to emulate hardware devices, which introduces overhead due to the translation of instructions. For workloads requiring extreme performance\u2014such as high-frequency trading, GPU-accelerated Machine Learning, or 100Gbps networking\u2014emulation is insufficient. The Virtual Function I/O (VFIO) framework allows the host kernel to unbind a physical PCI device from the host drivers and pass it directly to the virtual machine. This gives the guest OS direct, exclusive access to the hardware, resulting in near-native performance, albeit at the cost of losing that device on the host system. 1.2.5 Memory Ballooning Memory Ballooning is a dynamic memory management technique that allows the hypervisor to reclaim RAM from running virtual machines. It utilizes the virtio-balloon driver installed within the guest OS . When the host is under memory pressure, it instructs the balloon driver to \"inflate,\" causing the guest kernel to allocate RAM to the driver. The driver then informs the host which physical pages it has claimed, allowing the host to safely repurpose those physical pages for other tasks. Conversely, when the guest needs more memory, the balloon \"deflates,\" returning the pages to the guest's free pool. 1.3 Practical: Verifying KVM - Type 1 hypervisor Support Before creating virtual machines, it is imperative to verify that the host system is correctly configured to support hardware-assisted virtualization. This involves checking the CPU capabilities, kernel module status, and user permissions. 1.3.1 Verifying Hardware Virtualization (CPU Flags) The first step is to confirm that the physical processor supports the necessary virtualization extensions (Intel VT-x or AMD-V) and that these extensions are enabled in the system BIOS/UEFI. In Linux, we verify this by inspecting the CPU flags via the lscpu command. We specifically filter for \"Virtualization\" to see the vendor-specific technology. lscpu | grep Virtualization # Expected Output: VT-x (for Intel processors) or AMD-V (for AMD processors) If this command returns no output, it indicates that hardware virtualization is disabled at the firmware level. You must reboot the machine, enter the BIOS setup, and enable \"Virtualization Technology\" (often labeled as VT-x, Vanderpool, or SVM). 1.3.2 Confirming Kernel Module Loading Once hardware support is confirmed, we must ensure the Linux kernel has loaded the KVM - Type 1 hypervisor modules. The KVM - Type 1 hypervisor system consists of a core module ( kvm.ko ) and a processor-specific module ( kvm_intel.ko or kvm_amd.ko ). We use lsmod to list loaded modules: lsmod | grep kvm # Expected Output: Should list 'kvm - Type 1 hypervisor ' and either 'kvm_intel' or 'kvm_amd' If these modules are not present, they can often be loaded manually using modprobe kvm_intel (or amd ), provided the hardware support is active. 1.3.3 Verifying Access Permissions Security is a critical aspect of virtualization. The kernel interface for creating VMs, located at /dev/kvm , is restricted. Standard users cannot access this device by default. To allow a user to run VMs without root privileges (a security best practice), the user must be added to the specialized kvm group. ls -l /dev/kvm # Shows ownership, typically root:kvm - Type 1 hypervisor with permissions crw-rw---- If you encounter a \"Permission Denied\" error when running QEMU - Type 1 hypervisor for virtualization , verify your group membership using the groups command. If the kvm group is missing, you must add your user to it ( sudo usermod -aG kvm $USER ) and log out/in to apply the changes. 1.3.4 Troubleshooting Common Failures You may encounter specific error messages during this verification process. \"KVM - Type 1 hypervisor : disabled by BIOS\" : This message explicitly states that while the CPU supports virtualization, the feature is turned off in the computer's firmware. This cannot be fixed from within the OS; a physical reboot is required to modify BIOS settings. \"KVM - Type 1 hypervisor support not available\" : This often occurs when trying to run KVM - Type 1 hypervisor inside another virtual machine (e.g., a cloud VPS) that does not support Nested Virtualization. In this scenario, the \"outer\" hypervisor has not passed the hardware extensions through to your \"inner\" guest. 1.4 The Hardware: QEMU - Type 1 hypervisor for virtualization (Quick Emulator) While KVM - Type 1 hypervisor enables the kernel to execute instructions, it does not provide the \"Computer.\" It is the role of QEMU - Type 1 hypervisor for virtualization to provide the motherboard, the chipset, the PCI bus, and the plugged-in devices. Without QEMU - Type 1 hypervisor for virtualization , KVM - Type 1 hypervisor is just a fast calculator; with QEMU - Type 1 hypervisor for virtualization , it becomes a server. 1.4.1 The Threading Architecture To the Host Linux Kernel, a running Virtual Machine is purely a standard user-space process ( qemu-system-x86_64 ) that happens to be multi-threaded. It does not look different from a web browser or database server from the scheduler's perspective. The architecture relies on three distinct thread types. The Main Loop (iothread) is a single thread running a glib-based event loop responsible for non-blocking tasks, such as handling the QEMU - Type 1 hypervisor for virtualization Monitor (management interface), updating VNC/SPICE displays, and dispatching general I/O events. Parallel to this are the vCPU - Threads ; for every Virtual CPU core assigned to the guest, QEMU - Type 1 hypervisor for virtualization spawns a dedicated POSIX thread ( pthread ) that enters the KVM_RUN ioctl loop to execute guest code on the physical CPU. Finally, heavy I/O operations, such as writing large blocks to a disk image, are offloaded to a pool of Worker Threads . This asynchronous design prevents the main loop or vCPU - threads from blocking (freezing) while waiting for slow physical storage. 1.4.2 Defining the Virtual Motherboard When you configure a VM - in Proxmox platform combining KVM - Type 1 hypervisor and LXC , you are actually selecting arguments for the QEMU - Type 1 hypervisor for virtualization binary, starting with the Machine Type. -machine pc-q35-8.1,accel =kvm This single line defines the fundamental architecture of the virtual motherboard. The pc-q35 argument selects the modern Q35 chipset, which provides support for PCIe native handling and Secure Boot, as opposed to the legacy i440fx type which mimics a 1996-era PC. Crucially, the accel=kvm flag explicitly links the Emulator (QEMU - Type 1 hypervisor for virtualization ) to the Engine (KVM - Type 1 hypervisor ). Without this flag, QEMU - Type 1 hypervisor for virtualization would default to \"TCG\" (Tiny Code Generator), a software-only mode that interprets every instruction, resulting in agonizingly slow performance. 1.4.3 Constructing the Processor Topology QEMU - Type 1 hypervisor for virtualization allows granular control over how the CPU is presented to the guest OS . This is critical for licensing (some software is licensed per-socket) and performance (aligning with physical NUMA nodes). -smp 4,sockets = 1,cores = 4,threads = 1 This argument creates a topology of 1 Socket with 4 Cores. The Guest OS sees this exactly as if it were physical silicon. 1.4.4 Device Emulation: Frontend vs Backend In QEMU - Type 1 hypervisor for virtualization , every device is composed of two parts: the Frontend (what the Guest OS sees) and the Backend (how the Host handles the data). -drive file =/dev/pve/vm-100-disk-0,format =raw,if =virtio # Backend & Frontend -netdev tap,id =net0,ifname =tap100i0 # Network Backend -device virtio-net-pci,netdev =net0 # Network Frontend The Backend (defined by -netdev or -drive ) refers to the host-side resource, such as connecting to a Linux Bridge ( tap100i0 ) or a disk image file. (Note: The concept of the Linux Bridge and how it connects VMs to the physical network will be discussed in depth in the Network Chapter in Week 4.) The Frontend (defined by -device ) creates the virtual hardware that appears on the guest's PCI bus, such as a \"VirtIO Network Card\". The guest OS writes data to the Frontend device, and QEMU - Type 1 hypervisor for virtualization is responsible for passing that data to the polling Backend. 1.5 Optimization: VirtIO (Paravirtualization) Emulating physical hardware (like an Intel E1000 network card) is \"expensive\" because every packet sent requires a context switch (VM - Exit) to write to device registers, which QEMU - Type 1 hypervisor for virtualization then has to decode and simulate. The Solution: VirtIO VirtIO replaces full hardware emulation with a standardized \"Paravirtualized\" architecture. In this model, the Guest OS is aware that it is running in a virtual environment. Instead of trapping and emulating legacy hardware registers (which is slow), the Guest uses a specialized virtio driver to communicate directly with the Host via a shared memory interface. Mechanism (The Ring Buffer) : The core of VirtIO's performance is the Virtqueue , implemented as a circular ring buffer in shared memory. The process begins with Shared Memory , where QEMU - Type 1 hypervisor for virtualization allocates a region of RAM that is mapped into the address spaces of both the Host and the Guest. When the Guest needs to send data (e.g., a network packet), it initiates a vRing Operation by placing a descriptor pointer into the vRing buffer. Subsequently, the Guest performs a Kick Notification \u2014a lightweight signal (via ioeventfd )\u2014to alert the Host that new data is available. Finally, the Host performs Zero-Copy Processing by reading the data directly from the shared memory without needing to simulate a physical device operation, processing it, and placing a response back in the ring. This architecture bypasses the context-switching overhead inherent in traditional emulation. Note: This is why you must select \"VirtIO\" for Network and Disk in Proxmox platform combining KVM - Type 1 hypervisor and LXC when performance matters. 1.6 Essential Command Reference While Proxmox platform combining KVM - Type 1 hypervisor and LXC handles these automatically, knowing the underlying commands is useful for debugging and \"deep dive\" understanding. The following commands are provided for reference and theoretical understanding; you will have the opportunity to execute them and observe their output directly in this week's Lab exercises. 1.6.1 Disk Management (qemu - Type 1 hypervisor for virtualization -img) The qemu-img utility handles virtual disk - creation and manipulation. To create a new disk, the create subcommand is used, specifying the format (typically qcow2 for thin provisioning) and the size. For example, qemu-img create -f qcow2 mydisk.qcow2 20G . To inspect an existing disk's virtual size and actual disk usage, the info command provides detailed metadata. Furthermore, the convert subcommand interacts with the format translation engine, allowing administrators to transform a generic .img image into a VMware .vmdk or QEMU - Type 1 hypervisor for virtualization .qcow2 image. 1.6.2 VM - Execution (qemu - Type 1 hypervisor for virtualization -system-x86_64) Launching a VM - manually involves the qemu-system-x86_64 binary. A basic invocation usually requires defining the hard drive ( -hda ), the allocated RAM ( -m ), and crucially, the KVM - Type 1 hypervisor accelerator ( -enable-kvm or -accel kvm ). For booting from an installer ISO, the -cdrom flag is added, often accompanied by -boot d to prioritize the optical drive in the boot order. Section 1 Checkpoint Summary : KVM - Type 1 hypervisor : The kernel module that handles CPU execution (Guest Mode). QEMU - Type 1 hypervisor for virtualization : The process that provides virtual hardware (Disks, NICs). VirtIO : Special drivers that bypass emulation for speed. Reflection : Why does top on the host show a qemu-kvm process using 100% CPU if the VM - is busy? What is the difference between \"Full Emulation\" and \"Paravirtualization\"? Resources : Red Hat: KVM Architecture 2. The Platform: Proxmox platform combining KVM - Type 1 hypervisor and LXC VE To understand Proxmox platform combining KVM - Type 1 hypervisor and LXC VE, one must first understand its relationship with the underlying technologies. If KVM - Type 1 hypervisor is the engine that powers virtualization and QEMU - Type 1 hypervisor for virtualization is the chassis that constructs the virtual hardware, then Proxmox platform combining KVM - Type 1 hypervisor and LXC VE acts as the dashboard and control center . While it is technically possible to manage KVM - Type 1 hypervisor and QEMU - Type 1 hypervisor for virtualization directly via the command line, this approach is granular, tedious, and unscalable for production environments. Proxmox platform combining KVM - Type 1 hypervisor and LXC VE solves this by serving as an orchestration layer; it automates the complex qemu commands and kernel interactions, wrapping them in a cohesive, enterprise-grade management platform that provides visibility, clustering, and backup capabilities which the raw tools lack on their own. 2.1 Architectural Breakdown Figure 2.1: Proxmox platform combining KVM - Type 1 hypervisor and LXC VE Architecture - Decoupling the Web Interface, API, and Core KVM - Type 1 hypervisor Engine. As illustrated in Figure 2.1 , Proxmox platform combining KVM - Type 1 hypervisor and LXC VE is designed as a layered interaction model. It is not a monolithic black box, but a collection of distinct services working in harmony. 2.1.1 The Management Layer (Top) This layer facilitates all human-to-machine interaction and is driven by several key daemons. The pveproxy service acts as the primary \"front door,\" listening on port 8006 and serving the web interface via a secure HTTP server; it forwards valid requests to the internal API (pvedaemon). For console access, spiceproxy and vncterm handle the streaming of graphical display data from VMs to the browser. The pvedaemon (PVE API Daemon) is the worker background process that actually executes privileged tasks; whether triggered by the JSON-based Web GUI, the pvesh CLI tool, or an external script, all roads lead to this daemon, ensuring a consistent execution path for every command. This layer also enforces User Authentication , validating credentials against configurable realms (PAM, LDAP, AD) before granting access. 2.1.2 The Cluster Layer (Middle) Invisible to the user, a suite of daemons maintains the cluster's brain. Corosync engine for group communication is the foundational cluster engine, providing the reliable, low-latency communication required to maintain quorum to function (node consensus). Data consistency is handled by pmxcfs (Proxmox platform combining KVM - Type 1 hypervisor and LXC Cluster File System), a database-driven filesystem that instantly replicates configuration files in /etc/pve to all nodes. Monitoring is the responsibility of pvestatd (PVE Status Daemon), which queries the status of VMs, containers, and storage every 10 seconds to update the management layer. Finally, the High Availability (HA - System design for minimal downtime ) stack consists of two critical components: the pve-ha - System design for minimal downtime -crm (Cluster Resource Manager), which decides where a service should run, and the pve-ha - System design for minimal downtime -lrm (Local Resource Manager), which watches services on the local node and reports their state to the CRM, ensuring rapid recovery if a node fails. 2.1.3 The Resource Layer (Bottom) This is where the actual work happens, overseen by the Proxmox platform combining KVM - Type 1 hypervisor and LXC Kernel. For full virtualization, the KVM - Type 1 hypervisor kernel module turns the Linux kernel into a hypervisor, while QEMU - Type 1 hypervisor for virtualization processes utilize this to run guest operating systems. For lightweight virtualization, LXC (Linux Containers) enables kernel-shared environments, supported by lxcfs , a userspace filesystem that provides containers with virtualized views of /proc files (like CPU and memory stats) so they don't see the host's full resources. Storage Plugins simplify disk management by translating abstract requests (\"create disk\") into specific backend commands (e.g., zfs create or rbd create ). Similarly, the pve-firewall service generates 'iptables' rules dynamically for each guest, creating isolated security zones (Interfaces/Bridges) at the kernel level. 2.2 Key Components Proxmox platform combining KVM - Type 1 hypervisor and LXC VE is not a single application but a suite of integrated components. Understanding how these distinct parts\u2014the interface, storage backends, and clustering services\u2014fit together is essential for designing a resilient infrastructure. Although the architecture is layered, the daily operational experience revolves around extensive interaction with the following key subsystems. 2.2.1 Web Interface (GUI) Figure 2.2: The Proxmox platform combining KVM - Type 1 hypervisor and LXC VE Web Interface (GUI) providing a centralized view of the datacenter. The primary management point is the web-based Graphical User Interface, accessible via port 8006 . It abstracts the complexity of qemu command lines and configuration files, allowing administrators to create VMs, manage storage pools, and configure software-defined networking bridges with visual feedback. The interface organizes these capabilities into four distinct regions. At the top, the Header provides critical status information and action buttons for system-wide operations. To the left, the Resource Tree acts as the main navigation hub, allowing you to select specific objects such as nodes, VMs, or storage pools. The center region contains the Content Panel , which dynamically updates to show the configuration options and status for whichever object is selected in the tree. Finally, the Log Panel resides at the bottom, creating a real-time audit trail of recent tasks; administrators can double-click these entries to view detailed execution logs or abort running operations. 2.2.2 Storage Backends Proxmox platform combining KVM - Type 1 hypervisor and LXC decouples compute from storage to facilitate flexibility across different environments. For standalone setups, administrators often utilize Local Storage such as LVM-Thin or ZFS, which support advanced features like instant snapshots. In clustered environments, the platform leverages Network Storage (NFS, iSCSI, or SMB) to enable VM - mobility and Live Migration - between hosts without downtime . Furthermore, Proxmox platform combining KVM - Type 1 hypervisor and LXC supports Hyper-Converged infrastructure through native integration with Ceph, a distributed object store that runs directly on the nodes themselves, eliminating the need for expensive external SAN hardware. Section 2 Checkpoint Summary : Architecture : Web UI -> API -> Corosync engine for group communication /pmxcfs -> Kernel/KVM - Type 1 hypervisor . pmxcfs : The magic filesystem that keeps cluster configs in sync. Storage : Decoupled from compute to allow flexibility (Local vs Shared). Reflection : Why is an API-first design better for automation? If pmxcfs replicates configs, what happens if you lose network connectivity between nodes? 3. VM - Management Features 3.1 Cloning Cloning is the process of creating a new virtual machine based on the state of an existing one. Proxmox platform combining KVM - Type 1 hypervisor and LXC offers two distinct methods suited for different use cases. 3.1.1 Full Clone A Full Clone is a complete, independent copy of the original VM - . The system performs a block-by-block copy of the source disk image to a new file. Since it duplicates all data, it consumes significant time and storage space. However, its complete isolation makes it ideal for production deployments, as the new VM - has no dependency on the original. 3.1.2 Linked Clone A Linked Clone uses a \"Copy-on-Write\" mechanism. It does not copy the original disk; instead, it creates a new delta file that references the original \"Base\" disk. The new VM - reads from the Base disk but writes changes to its own small delta file. This allows for near-instant creation and minimal storage usage, making it perfect for efficient testing or classroom labs. However, it introduces a critical dependency: the Base disk cannot be deleted without breaking all Linked Clones. Figure 3.1: Full Clones copy data; Linked Clones reference data. 3.2 Snapshots A snapshot - preserves the state of a virtual machine at a specific point in time. Unlike a backup, which is a copy of data, a snapshot - is a freeze-frame of the disk and memory state. 3.2.1 Use Cases and Mechanism Snapshots are primarily used as a safety net before performing risky operations, such as major OS upgrades or testing unstable software. If the operation fails, the administrator can perform a \"Rollback\" to revert the system state exactly to the moment the snapshot - was taken. 3.2.2 The Snapshot - Process When a snapshot - is taken in Proxmox platform combining KVM - Type 1 hypervisor and LXC (specifically on QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format or ZFS storage), the system marks the current data blocks as read-only. Any new writes are diverted to new blocks. If basic disk snapshotting is selected, only the disk state is saved (crash-consistent). If \"Include RAM\" is selected, the entire contents of the running memory are dumped to disk. This allows the VM - to be restored to a running state, preserving open applications and processes, though it takes longer to complete. 3.3 Console Access Accessing the VM - 's display is handled via remote desktop protocols integrated into the browser. 3.3.1 NoVNC NoVNC is the default HTML5-based console. It requires no plugins and renders the VM - 's display directly in any modern web browser using WebSockets. It is lightweight and universally compatible but has limited support for clipboard integration and audio forwarding. 3.3.2 SPICE (Simple Protocol for Independent Computing Environments) For a richer desktop experience, Proxmox platform combining KVM - Type 1 hypervisor and LXC supports SPICE. This protocol offers advanced features such as high-quality audio streaming, multi-monitor support, and USB device redirection (plugging a USB drive into your client and having it appear in the VM - ). However, unlike NoVNC, SPICE requires a dedicated client viewer software ( virt-viewer ) to be installed on the user's machine. Section 3 Checkpoint Summary : Cloning : Full (Performance/Isolation) vs Linked (Speed/Space). Snapshots : \"Save Game\" state before risky changes. Not a backup! Consoles : NoVNC is convenient (browser-based), SPICE is powerful (requires client). Reflection : Why should you delete a Linked Clone if you delete the Parent? Does a snapshot - consume disk space? If so, when? Resources : Proxmox VE: Live Snapshots 4. Summary and Next Steps This week we peeled back the layers of virtualization to reveal the mechanism inside. You learned that a \"Virtual Machine\" is not a magical black box, but simply a standard Linux process ( kvm ) managed by the kernel. You discovered that vCPUs are just threads scheduled by CFS, and vRAM is just memory allocated via mmap() . Finally, you mastered the art of tuning this engine\u2014balancing resources to ensure stability\u2014and explored the management features that make virtualization so powerful. Preparing for Week 3 Next week, we go deeper into the infrastructure. We will explore Virtual Networking Fundamentals . Since you now know that a VM - is just a process, it's time to understand how to wire these processes together using Linux Bridges, veth pairs, and VLANs. Checklist: Can you explain what happens during a \"VM - Exit\"? Do you understand why a vCPU - is treated like a Chrome tab by the scheduler? Have you successfully SSH'd into your new Ubuntu Server? 5. Lab Exercises Lab 1: KVM Virtualization Fundamentals Part A : Verification & Preparation (Hardware/Disks). Part B : Launching & Logging In (CLI VM - Boot). Lab 2: Proxmox VM Management Goal : Deploy Ubuntu VMs, install Guest Agents, and perform Snapshots/Cloning. Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 1: Introduction Course Index Next: Week 3: Networking\n\n--- WEEK 3 NOTES ---\nVirtual Networking and Linux Networking Fundamentals Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Virtual Networking and Linux Networking Fundamentals Estimated Reading Time : 35 Minutes [!TIP] How to succeed in this week : Networking is where VMs become useful. The concepts of bridges, NAT Network Address Translation - Maps private to public IPs , and routing apply universally. Draw diagrams as you read\u2014visualizing packet flow makes troubleshooting much easier later. Welcome to Week 3! Networking is the nervous system of any virtualized infrastructure. In a traditional data center, network engineers run physical cables, configure hardware switches, and plug in routers. In a Virtualized or Cloud environment, all of this hardware is replaced by software . As a Cloud Engineer, you are not just managing servers; you are managing the wires between them. When a Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization VM Virtual Machine - A software-based emulation of a physical computer - cannot reach the internet, or when a Kubernetes Container orchestration platform orchestration Automated coordination of systems platform orchestration Automated coordination of systems platform orchestration Automated coordination of systems platform pod Smallest deployable unit in Kubernetes orchestration Automated coordination of systems platform often fails to talk to a database, the problem rarely lies in the physical switch. It lies in the complex web of virtual interfaces, bridges, and namespaces inside the Linux kernel. This week is designed to bridge Network device connecting network segments the gap between \"clicking a button in Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization \" and understanding the kernel-level mechanics that make that click possible. We will dismantle the underlying abstraction and rebuild it command by command. What You'll Learn This Week Linux Networking Core: Understanding the distinction between ephemeral runtime configurations ( ip ) and persistent system configurations ( nmcli ). Virtual Wiring: How veth pairs replace physical patch cables. Virtual Switching: How Linux Bridges simulate Layer 2 switches and manage MAC address tables. Network Isolation: Using Namespaces to solve the \"Multi- Tenant Grouping of users and resources (also called Project) \" problem. Advanced Operations: Implementing redundancy (Bonding), Routing, and Security (Firewalls). Software Defined Networking: An introduction to Open vSwitch (OVS). Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization Implementation: Applying these concepts to real-world VM Virtual Machine - A software-based emulation of a physical computer - networking modes. Part 1: Linux Networking Fundamentals Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization is, at its core, a Debian Linux operating system. To troubleshoot or configure it effectively, you must master Linux networking. 1. Runtime vs. Persistent Networking ( ip vs nmcli ) In the early days of Unix, network interfaces were configured using ifconfig . This tool has been deprecated for over a decade because it cannot handle modern complexities like multiple IP addresses per interface or policy-based routing. Today, we rely on two distinct layers of configuration. 1.1 The Kernel Layer: ip (Runtime) The ip command is part of the iproute2 suite. It communicates directly with the Linux Kernel via Netlink sockets . When you run an ip command, you are modifying the direct, in-memory state of the network stack. Speed : Changes happen instantly (microseconds). Scope : Changes affect the running system only. Risk : Changes are lost on reboot. If you restart the server, the kernel reloads, and your manual ip changes disappear. Key Commands: Link Status : ip link show (Displays MTU, MAC addresses, and UP/DOWN state). IP Addresses : ip addr show (Displays IPv4/IPv6 addresses attached to links). Routing Table : ip route get 8.8.8.8 (Simulates a packet to 8.8.8.8 and tells you which interface and gateway would be used). 1.2 The Configuration Layer: nmcli (Persistent) To make settings permanent, we need a daemon that reads config files and applies them at boot. In modern Linux (including RHEL/CentOS and many Debian setups), this is NetworkManager . The command-line tool for this is nmcli . Profiles : NetworkManager saves settings as \"Connection Profiles\" (saved in /etc/NetworkManager/system-connections/ ). Logic : It handles complex logic, like \"connect to Wi-Fi X when available, otherwise fall back to Ethernet Y.\" Persistence : Changes made here are written to disk and survive reboots. Hands-On Example: Use nmcli to set a static IP, ensuring the server always boots with the same address. # 1. Modify the connection profile for interface 'eno1' # Set the IP address and Subnet Mask nmcli con mod eno1 ipv4.addresses 192.168.1.50/24 # 2. Set the Default Gateway (The router packets go to) nmcli con mod eno1 ipv4.gateway 192.168.1.1 # 3. Disable DHCP (Method: Manual) nmcli con mod eno1 ipv4.method manual # 4. Apply the changes (Bounce the interface) nmcli con up eno1 Section 1 Checkpoint Summary : ip commands are runtime only (lost on reboot). nmcli commands are persistent (saved to disk). Correct networking requires understanding both layers. Reflection : Why did Linux move away from ifconfig ? If you set an IP with ip addr add and then reboot, what happens? Resources : Red Hat: ip Command Cheat Sheet 2. Network Namespaces (The \"Containers\" of Networking) In a standard Linux server, there is one global network stack. There is one routing table, one firewall, and one list of interfaces. This creates a problem for Cloud Providers: The Multi-Tenant Problem . Understanding Linux Namespaces A namespace is a fundamental Linux kernel feature that allows the kernel to provide a process with its own isolated view of global system resources. While there are several types of namespaces\u2014such as pid (Process IDs), mnt (Mount Points), and user (User IDs)\u2014our primary focus here is on the Network Namespace ( net ) . The Network Namespace A Network Namespace provides a completely encapsulated network stack. When a process is placed inside a network namespace, it sees a unique set of: Network Interfaces (e.g., its own eth0 and lo ). IP Addresses (which can overlap with other namespaces). Routing Tables (its own default gateway). Firewall Rules (its own independent iptables chains). Even though multiple containers share the same underlying Linux Kernel, this isolation ensures they cannot see or interfere with each other's traffic. 2.1 The \"Tenant Problem\" Imagine you are hosting Amazon Web Services. Customer A wants a private network using the IP range 192.168.1.0/24 . Customer B also wants to use 192.168.1.0/24 . On a normal OS, you cannot have the same IP address twice. It would cause an IP conflict. To solve this, we use Namespaces . A Network Namespace is like a parallel universe for networking. It partitions the kernel's network structures. Inside a namespace, you have a completely independent set of interfaces, routes, and firewall rules. Customer A lives in Namespace A, and Customer B lives in Namespace B. They can use identical IP addresses without ever conflicting. This is the fundamental technology behind Docker , Kubernetes orchestration platform , and LXC . Figure 2.1: Visualizing Isolation of Network Resources within a Single Linux Kernel. 2.2 Hands-On Example We will create a \"sandbox\" namespace to demonstrate this total isolation. # 1. Create a new namespace called \"sandbox\" sudo ip netns add sandbox # 2. Verify Isolation # Run 'ip link' on the main host. You see all your physical cards. ip link show # Run 'ip link' INSIDE the sandbox. You see NOTHING (except loopback). # The syntax is: ip netns exec <name> <command> sudo ip netns exec sandbox ip link show # 3. Activate the Loopback # Ideally, every network stack needs a standard localhost (127.0.0.1) sudo ip netns exec sandbox ip link set lo up Section 2 Checkpoint Summary : Namespaces isolate network stacks (interfaces, routes, firewalls). They solve the \"Multi-Tenant\" problem (overlapping IPs). They are the foundation of all Container technology. Reflection : Can a process in Namespace A see the network traffic of Namespace B? How does this isolation improve security in a cloud environment? Resources : Introduction to Linux Namespaces 3. Connecting the Dots: Veth Pairs and Bridges Isolation is excellent for security, but now our namespace is a digital islands. It helps no one if it cannot communicate. We need two things to fix this: a cable and a switch. 3.1 The Virtual Cable ( veth ) You cannot plug a physical Ethernet cable into a software namespace. Instead, Linux provides the Virtual Ethernet (veth) device. A veth pair is always created as two connected interfaces\u2014a \"pipe.\" Packets sent into Interface A instantly arrive at Interface B, and vice versa. This allows us to tunnel traffic from the Global Host namespace into the isolated \"sandbox\" namespace. Hands-On Example: Connecting the Sandbox to the Host. # 1. Create the pair. We name the ends specifically to avoid confusion. # 'veth-host' will stay on the outside. 'veth-sandbox' will go inside. sudo ip link add veth-host type veth peer name veth-sandbox # 2. Move one end through the \"portal\" into the namespace sudo ip link set veth-sandbox netns sandbox # 3. Configure IPs (The Host side is the Gateway; The Sandbox side is the Client) # Host Side: 10.0.0.1 sudo ip addr add 10.0.0.1/24 dev veth-host sudo ip link set veth-host up # Sandbox Side: 10.0.0.2 sudo ip netns exec sandbox ip addr add 10.0.0.2/24 dev veth-sandbox sudo ip netns exec sandbox ip link set veth-sandbox up # 4. Test Connectivity sudo ip netns exec sandbox ping 10.0.0.1 3.2 The Virtual Switch ( bridge ) Connecting two points with a veth is easy. But what if you have 50 Containers? You cannot create a mesh of cables between every single one. You need a device to connect them all to a central point. A Linux Bridge is a software implementation of a standard Ethernet Switch. It operates at Layer 2 (Data Link Layer) . *Figure 3.1: Veth pairs connecting Namespaces to a Linux Bridge.* MAC Learning : The bridge listens to incoming frames. If it sees a packet from MAC AA:BB:CC coming from Port 1, it records this in a table. Forwarding : If it later receives a packet destined for AA:BB:CC , it looks up the table and sends it only to Port 1. This reduces traffic congestion compared to a \"Hub\" which broadcasts everything. How devices plug in: Containers : Use veth pairs (as discussed in 3.1). Virtual Machines : Use TAP Interfaces . A tap device simulates an Ethernet device at Layer 2. QEMU - Type 1 hypervisor for virtualization /KVM - Type 1 hypervisor creates a tap interface (e.g., tap100i0 ) for the VM - , and the other end connects to the Bridge. This allows the VM - to send raw Ethernet frames onto the switch. Example: Manually creating a TAP interface (What Proxmox platform combining KVM - Type 1 hypervisor and LXC does behind the scenes) # 1. Create the TAP interface sudo ip tuntap add dev tap0 mode tap # 2. Plug it into the Bridge sudo ip link set tap0 master br0 sudo ip link set tap0 up Hands-On Example: Connecting Two Namespaces (Red & Blue) We will act as a \"Virtual Switch\" administrator. We want to connect two isolated namespaces so they can talk to each other. Create the Bridge (The Switch) : bash sudo ip link add name br0 type bridge sudo ip link set br0 up Create the \"Computers\" (Namespaces) : bash sudo ip netns add red sudo ip netns add blue Create the Cables (Veth Pairs) : We need two cables. One for Red, one for Blue. Cable 1: veth-red (Switch side) <-> veth-red-ns (Computer side) Cable 2: veth-blue (Switch side) <-> veth-blue-ns (Computer side) bash sudo ip link add veth-red type veth peer name veth-red-ns sudo ip link add veth-blue type veth peer name veth-blue-ns Plug them in (Wiring) : Plug the \"NS\" ends into the namespaces. Plug the other ends into the Bridge ( master br0 ). # Wiring Red sudo ip link set veth-red-ns netns red sudo ip link set veth-red master br0 sudo ip link set veth-red up # Wiring Blue sudo ip link set veth-blue-ns netns blue sudo ip link set veth-blue master br0 sudo ip link set veth-blue up Configure IPs (Inside Namespaces) : # Red Computer: 192.168.50.1 sudo ip netns exec red ip addr add 192.168.50.1/24 dev veth-red-ns sudo ip netns exec red ip link set veth-red-ns up # Blue Computer: 192.168.50.2 sudo ip netns exec blue ip addr add 192.168.50.2/24 dev veth-blue-ns sudo ip netns exec blue ip link set veth-blue-ns up Test Connectivity : Ping Blue from Red. The packet goes: Red -> veth -> br0 -> veth -> Blue . sudo ip netns exec red ping -c 3 192.168.50.2 Section 3 Checkpoint Summary : veth pairs act as virtual cables connecting namespaces. Linux Bridges act as virtual switches, forwarding frames based on MAC addresses. We can build complex topologies entirely in software. Reflection : If you forget to \"plug\" the veth into the bridge ( master br0 ), what happens? Why do we need two ends to a veth pair? Resources : Linux Bridge Command Help 4. Container Networking in Practice (Docker, Podman, LXC ) In Week 3, we discussed containers. Now we see how they use the Linux networking primitives we just learned. 4.1 Docker (The Standard Bridge) When you install Docker, it creates a Linux Bridge named docker0 . The Bridge : docker0 (Usually 172.17.0.1/16). The Veth : Every time you run docker run , Docker creates a veth pair. One end sits on the host (plugged into docker0 ), the other sits inside the container. The Namespace : The container is just a process running inside a network namespace. CLI Comparison: Action Manual Linux Command Docker Command Create Net ip link add br0 type bridge docker network create mynet Create NS ip netns add container1 docker run --name container1 ... Connect ip link set veth master br0 Auto-connected to docker0 or custom net End of CLI Comparison table 4.2 Podman (Rootless Networking) Podman often manages containers without root privileges. Standard bridges require root. How does it work? slirp4netns : Podman uses a User-Space networking stack. It \"slurps\" traffic from the host interface and pipes it into the container namespace without touching the kernel's actual bridge settings. Performance : Slightly slower than Docker's bridge, but much more secure. 4.3 LXC (System Containers) LXC is closer to Proxmox platform combining KVM - Type 1 hypervisor and LXC 's approach. It typically uses lxcbr0 . Unlike Docker, LXC networking is defined in a config file . By default, it connects to lxcbr0 and requests an IP via DHCP, just like a physical computer would. # 1. Create a container sudo lxc-create -n my-container -t download -- -d ubuntu -r focal -a amd64 # 2. View the Network Config (How does it get an IP?) # You will see: lxc .net.0.type = veth # You will see: lxc .net.0.link = lxcbr0 cat /var/lib/lxc/my-container/config # 3. Start the container (It creates the veth and asks for DHCP) sudo lxc-start -n my-container # 4. Check the assigned IP sudo lxc-info -n my-container Section 4 Checkpoint Summary : Docker automates the creation of Bridges ( docker0 ) and Veth pairs. Podman uses slirp4netns for rootless, secure networking. LXC behaves more like traditional VMs on the network. Reflection : Why is \"Rootless\" networking considered more secure? How does docker network create simplify what we did in Section 3? Resources : Docker Networking Overview 5. Advanced Linux Operations Mastering the basics is enough to build a lab, but production environments require robustness, redundancy, and security. 5.1 Link Aggregation (Bonding/LACP) Hardware fails. Cables get cut, SFP modules burn out, and switch ports die. If your Hypervisor is connected via a single cable, you have a Single Point of Failure (SPOF) . Bonding allows you to combine multiple physical interfaces (e.g., eno1 and eno2 ) into a single virtual interface ( bond0 ). Mode 1 (Active-Backup) : Only one cable is used. If it fails, the traffic flips to the second cable. Simple, but wastes 50% bandwidth. Mode 4 (802.3ad LACP) : The Gold Standard. The server and switch communicate to balance traffic across both cables simultaneously. You get double the speed and redundancy. Configuration: # 1. Create the Bond Interface nmcli con add type bond con-name bond0 ifname bond0 mode 802.3ad # 2. Enslave Physical NICs to the Bond nmcli con add type ethernet slave-type bond con-name bond0-eno1 ifname eno1 master bond0 nmcli con add type ethernet slave-type bond con-name bond0-eno2 ifname eno2 master bond0 # 3. Bring up the Bond nmcli con up bond0 5.2 IP Forwarding & Routing A Linux server usually behaves like an End Host \u2014it consumes packets sent to it and ignores the rest. However, in virtualization (specifically NAT Mode), the Linux host must act as a Router . It needs to accept packets from VMs and forward them to the Internet. To do this, we must toggle a specialized kernel parameter: ip_forward . We also need to manipulate the Routing Table to tell the kernel where networks live. Commands: # Check status (0 = Disabled, 1 = Enabled) cat /proc/sys/net/ipv4/ip_forward # Enable Forwarding Temporarily sysctl -w net.ipv4.ip_forward = 1 # Add a Static Route # \"Traffic for 10.99.0.0/24 should be sent to the gateway at 192.168.1.254\" sudo ip route add 10.99.0.0/24 via 192.168.1.254 5.3 The Linux Firewall ( iptables ) Security in the cloud isn't just about an external firewall appliance. Every Linux kernel has a built-in firewall called Netfilter . We manipulate it using iptables . It processes packets in \"Chains\": 1. INPUT : Filters traffic destined for the localhost. 2. OUTPUT : Filters traffic created by the localhost. 3. FORWARD : Filters traffic passing through the localhost (e.g., VM - to Internet traffic). Hands-On Example: securing a host. # 1. Allow \"Established\" traffic # If we sent a request out, allow the reply back in. Essential! sudo iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 2. Allow SSH (Port 22) so we can manage the server sudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT # 3. Drop Everything Else # This is the \"Default Deny\" posture. If it's not explicitly allowed, kill it. sudo iptables -P INPUT DROP Modern Frontends: firewalld vs iptables While iptables (and its successor nftables ) is the low-level tool, managing thousands of rules manually is hard. Modern distros use \"Frontend Controllers\": RHEL/CentOS/Fedora : Use firewalld . It organizes rules into \"Zones\" (Public, Home, Work). # 1. Check which zone is active sudo firewall-cmd --get-active-zones # 2. Allow HTTP traffic permanently sudo firewall-cmd --permanent --add-service =http # 3. Allow a specific port (e.g., 8080) sudo firewall-cmd --permanent --add-port = 8080/tcp # 4. Reload to apply changes sudo firewall-cmd --reload Debian/Ubuntu : Often use ufw (Uncomplicated Firewall). # 1. Check status (Inactive by default) sudo ufw status # 2. Allow SSH first (Important!) sudo ufw allow ssh # 3. Allow specific ports sudo ufw allow 80/tcp sudo ufw allow 8080/tcp # 4. Enable the firewall sudo ufw enable These tools write the underlying iptables / nftables rules for you. 5.4 Network Reconnaissance ( ss & nmap ) Before you can analyze traffic, you often need to know what ports are open or who is listening. Socket Statistics ( ss ) ss is the modern replacement for the deprecated netstat . It allows you to see which processes on your server are listening for connections. # Show all Listening (l) TCP (t) and UDP (u) ports with Numeric (n) values and Process names (p) sudo ss -tulpn Network Mapper ( nmap ) nmap is the industry standard for network discovery. Unlike ss (which looks locally), nmap allows you to scan remote servers to see what they are exposing. # Scan a target IP to see open ports nmap 192.168.1.50 5.5 Traffic Analysis ( tcpdump ) When networking breaks, it often fails silently. A firewall drops a packet without an error message. A route sends traffic into a black hole. To fix this, we need Packet Capture . tcpdump is the CLI version of Wireshark. It puts the network card into \"Promiscuous Mode,\" allowing it to see every packet on the wire, not just those addressed to it. Commands: # Basic Capture: Listen on interface 'vmbr0' for Port 80 (HTTP) sudo tcpdump -i vmbr0 port 80 # Source Filtering: Show traffic only from a specific \"Bad IP\" sudo tcpdump -i vmbr0 src 10.0.0.5 # File Capture: Save packets to a file for detailed analysis in Wireshark later sudo tcpdump -i vmbr0 -w crash_report.pcap Section 5 Checkpoint Summary : Bonding aggregates links for redundancy (Active/Backup) or speed (LACP). Routing allows a Linux host to act as a Gateway. IPTables controls traffic flow (Input, Output, Forward). SS/Nmap allow you to discover open ports and listening services. Tcpdump allows us to see the actual packets on the wire. Reflection : If you have a Bond with 2 cables and one is cut, does the server go offline? Why do we need ip_forward=1 for NAT? Resources : Tcpdump 101 Part 2: Virtual Networking in Proxmox platform combining KVM - Type 1 hypervisor and LXC Now that we have dismantled the Linux kernel concepts, we can look at how Proxmox platform combining KVM - Type 1 hypervisor and LXC Virtual Environment (PVE) uses them to manage VM - networking. Proxmox platform combining KVM - Type 1 hypervisor and LXC does not invent its own networking stack; it \"orchestrates\" standard Linux tools (Bridges, OVS, IPTables) via a web interface. 6. Open vSwitch (OVS): The Cloud Switch While the standard Linux Bridge is reliable and simple, it was designed for a single server. It lacks the intelligence required for massive, multi-tenant clouds. Open vSwitch (OVS) is an industrial-grade, programmable virtual switch used by giants like OpenStack platform , Red Hat, and Citrix. 6.1 Architecture: Control Plane vs. Data Plane Unlike a physical dumb switch, OVS is split into two parts: 1. Userspace (Control Plane) : ovs-vswitchd and ovsdb-server handle the database and logic. This is where you configure \"Flows\" and \"Ports\". 2. Kernel Module (Data Plane) : This handles the actual packet switching at wire speed. It caches decisions made by the Control Plane so that future matching packets don't need to check the database again. 6.2 Installing OVS Unlike the standard bridge, OVS is not always installed by default. # Ubuntu/Debian sudo apt update sudo apt install openvswitch-switch # Start the service sudo systemctl enable --now openvswitch-switch # Verify sudo ovs-vsctl show 6.3 The Power of Flows Standard bridges forward based on MAC addresses. OVS forwards based on Flow Rules . A flow rule matches a packet's header fields and performs an action. Example Scenario: You want to block a specific \"Bad Neighbor\" VM - (IP 10.0.0.66) from sending traffic, but allow everything else. Linux Bridge: Cannot do this natively (needs complex iptables). OVS: One simple command. # 1. Create a programmable bridge # This creates a virtual switch that listens for OpenFlow commands. ovs-vsctl add-br ovsbr0 # 2. Add a Flow Rule to DROP traffic from specific source IP # \"If a packet comes from 10.0.0.66, DROP it explicitly.\" ovs-ofctl add-flow ovsbr0 \"ip,nw_src=10.0.0.66 actions=drop\" # 3. Add a fallback rule (Normal Switching) # \"If no other rules match, behave like a normal L2 Switch.\" ovs-ofctl add-flow ovsbr0 \"priority=0 actions=NORMAL\" 6.4 SDN Controller Integration OVS is designed to be controlled remotely by an SDN Controller (like OpenDaylight or ONOS). This allows a central brain to program the switches across the entire datacenter. # Point the switch to a Controller IP ovs-vsctl set-controller ovsbr0 tcp:192.168.56.10:6653 Section 6 Checkpoint Summary : OVS is a programmable, carrier-grade virtual switch. It separates the Control Plane (Logic) from the Data Plane (Forwarding). It uses \"Flows\" instead of just MAC tables, allowing for complex logic (Drop IP X, Redirect Port Y). Reflection : How does OVS differ from a standard Linux Bridge? What is the role of an SDN Controller? Resources : Open vSwitch Crash Course 7. Proxmox platform combining KVM - Type 1 hypervisor and LXC Networking Proxmox platform combining KVM - Type 1 hypervisor and LXC VE is not a \"black box.\" It is simply a Debian Linux host with a nice web interface. Every networking button you click in the GUI corresponds directly to the Linux commands we learned in Part 1. Proxmox platform combining KVM - Type 1 hypervisor and LXC GUI Term Underlying Linux Technology Linux Bridge ( vmbr0 ) Standard bridge (Section 3.2) Bond ( bond0 ) Linux Bonding (Section 5.1) VLAN Tag vlan interface or Bridge VLAN Filtering (Section 7.3) Firewall iptables / nftables (Section 5.3) When you start a VM - , Proxmox platform combining KVM - Type 1 hypervisor and LXC creates a dynamic tap interface (e.g., tap100i0 ) and plugs it into the bridge ( vmbr0 ), exactly like we plugged in veth pairs in the Lab. 7.1 Networking Modes When configuring a VM - 's hardware, you choose how it connects to the bridge. We primarily see three patterns. ![Proxmox platform combining KVM - Type 1 hypervisor and LXC Networking Modes](images/proxmox_networking_modes.png) *Figure 7.1: Comparison of Bridged vs. NAT vs. VLAN Modes.* 1. Bridged Mode (Default) This is the standard configuration for 90% of deployments. The VM - becomes a full peer on the physical network. Packet Flow: [VM eth0] -> [Host tap100i0] -> [Linux Bridge vmbr0] -> [Physical NIC eno1] -> [Physical Switch] Detailed Configuration: In /etc/network/interfaces , a Bridge looks like this: auto vmbr0 iface vmbr0 inet static address 192.168.1.10/24 gateway 192.168.1.1 bridge-ports eno1 # This links the bridge to the physical cable bridge-stp off bridge-fd 0 Implication: DHCP : The VM - sends a broadcast DISCOVER packet. It traverses the bridge, hits the physical wire, and reaches your office router. The router assigns an IP directly to the VM - . Visibility : Every device on your LAN can ping the VM - . 2. NAT Mode (Network Address Translation) Sometimes you cannot get extra public IPs (e.g., in a data center giving you only 1 IP). You need to create a private network inside the host and share the single public IP. Packet Flow: [VM eth0] -> [Host tap100i0] -> [Private Bridge vmbr1] -> [Host Routing/NAT] -> [Physical NIC eno1] -> Internet Detailed Configuration: This mode requires Masquerading (Source NAT) in iptables . # In /etc/network/interfaces auto vmbr1 iface vmbr1 inet static address 10.10.10.1/24 # The Host acts as the Gateway for VMs bridge-ports none # NOT connected to any physical card bridge-stp off bridge-fd 0 # 1. Enable forwarding post-up echo 1 > /proc/sys/net/ipv4/ip_forward # 2. Masquerade traffic leaving the physical interface (eno1) # \"If traffic comes from 10.10.10.0/24, replace Source IP with Host IP\" post-up iptables -t nat -A POSTROUTING -s 10.10.10.0/24 -o eno1 -j MASQUERADE post-down iptables -t nat -D POSTROUTING -s 10.10.10.0/24 -o eno1 -j MASQUERADE 3. VLAN Tagging (802.1Q) VLANs allow multiple logical networks to share one physical cable. To do this, we insert a 4-byte \"Tag\" into the Ethernet Frame Header. Processing Flow: Ingress (VM - -> Host): The VM - sends a standard \"untagged\" packet. Tagging: As the packet enters the bridge port (configured with tag=20 ), Proxmox platform combining KVM - Type 1 hypervisor and LXC inserts the VLAN ID 20. Transit: The bridge sees the packet belongs to VLAN 20 and forwards it only to ports that are members of VLAN 20 (including the uplink). 3.1 Configuring VLANs manually (Linux CLI): You can create VLAN interfaces on standard Linux without a bridge using the ip command. This creates a virtual interface that automatically tags/untags packets. # Create a VLAN 10 interface on top of eno1 # Packets sent to 'eno1.10' will leave physical 'eno1' with Tag 10. sudo ip link add link eno1 name eno1.10 type vlan id 10 sudo ip addr add 192.168.10.1/24 dev eno1.10 sudo ip link set eno1.10 up Egress (Host -> Switch): The packet leaves eno1 with the tag attached. The physical switch reads \"20\" and puts it in the correct broadcast domain. VLAN Aware Bridge: Modern Proxmox platform combining KVM - Type 1 hypervisor and LXC uses \"VLAN Aware\" bridges. Instead of creating vmbr0.10 , vmbr0.20 , etc., you toggle bridge-vlan-aware yes . The bridge then acts like a \"Trunk Port\", capable of carrying all VLANs simultaneously. Section 7 Checkpoint Summary : Proxmox platform combining KVM - Type 1 hypervisor and LXC uses standard Linux Bridges and IPTables underneath the GUI. Bridged Mode : VM - is on the LAN. Simplest. NAT Mode : VM - is hidden behind the Host. Requires Masquerading. VLANs : Isolate traffic on the same physical wire using Tags. Reflection : If you are in a Data Center with only 1 Public IP, which mode MUST you use? How does a switch know which VLAN a packet belongs to? Resources : Proxmox Network Model 8. Additional Resources Linux Bonding Guide : Red Hat Documentation Tcpdump Cheat Sheet : PacketLife Proxmox platform combining KVM - Type 1 hypervisor and LXC Networking : Official Wiki 9. Lab Exercises Lab 1: Linux Networking Goal : Hands-on with ip commands, bridges, and routing. Lab 2: Proxmox Networking Goal : Configuring Bridges, Bonds, and VLANs in Proxmox platform combining KVM - Type 1 hypervisor and LXC VE. Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 2: Virtual Machines Course Index Next: Week 4: Storage\n\n--- WEEK 4 NOTES ---\nStorage and Backup Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Storage and Backup Estimated Reading Time : 25 Minutes [!TIP] How to succeed in this week : Storage decisions have long-term consequences. Understand the tradeoffs between local and shared storage, snapshots versus backups. The \"3-2-1 backup rule\" isn't just theory\u2014it saves careers. Welcome to Week 4! You have learned how to create virtual machines (Compute) and how to connect them (Networking). Now, it is time to tackle the third and perhaps most critical pillar of the cloud: Storage . In the physical world, if a hard drive fails, you lose data. In the virtual world, we have technologies like ZFS Advanced file system with volume management , Ceph Distributed storage system for object/block/file storage , and Snapshots that make data resilient, portable, and practically immortal. This week, we move beyond simply \"installing to a disk\" and learn how the Linux kernel manages block devices, volume groups, and file systems. What You'll Learn This Week Linux Storage primitives: Partitions, Block Devices, and Filesystems. Volume Management: Using LVM Logical Volume Manager - Flexible disk management to resize disks on the fly. Next-Gen Storage: Using ZFS Advanced file system with volume management for instant snapshots and self-healing data. Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization Storage Architecture: How PVE manages Local vs. Shared storage. Disaster Recovery: The critical difference between a Snapshot Point-in-time copy of VM state for rollback - and a Backup. Part 1: Linux Storage Fundamentals (Revision + New Concepts) In Operating Systems 2, you gained hands-on experience with Linux storage management, including fdisk for partitioning, LVM Logical Volume Manager - Flexible disk management for flexible volume management, and NFS Network File System - Remote file access protocol for network-attached storage. This section begins with a focused revision of those familiar tools\u2014reconnecting you with concepts like /dev/sda , lsblk , and pvcreate \u2014but now viewed through the lens of virtualization The creation of virtual versions of physical computing resources . However, we will also introduce an entirely new storage technology: ZFS Advanced file system with volume management , the enterprise-grade filesystem that Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization relies on for advanced features like instant snapshots, self-healing data, and copy-on-write operations. While LVM Logical Volume Manager - Flexible disk management and fdisk are foundational, ZFS Advanced file system with volume management represents the cutting edge of storage management in modern virtualized environments. Think of this as bridging your foundational Linux knowledge with enterprise virtualization The creation of virtual versions of physical computing resources infrastructure. 1. Block Devices and Partitions In Linux, everything is a file. A hard drive is a special type of file called a Block Device . The Linux kernel assigns specific naming conventions to different storage technologies. Traditional SATA and SCSI drives are represented as /dev/sda , /dev/sdb , and so on, while modern NVMe drives use a different naming scheme such as /dev/nvme0n1 and /dev/nvme1n1 . When a disk is divided into sections, these divisions are called partitions, and they are accessed through additional numerical suffixes\u2014for example, /dev/sda1 refers to the first partition on the first drive. 1.1 Examining Storage One of the most powerful diagnostic tools in Linux is lsblk (List Block Devices), which provides a visual tree representation of all connected storage devices. When you run this command in the terminal, it displays a hierarchical view that shows physical disks, their partitions, and any logical volumes built on top of them. This tree structure makes it immediately clear which partitions belong to which disks and how storage is organized across the system. For administrators managing Proxmox platform combining KVM - Type 1 hypervisor and LXC servers, lsblk is indispensable for quickly understanding storage topology without needing to parse complex configuration files. The diagram below illustrates how Linux represents different types of storage devices and their partition schemes: Figure 1: Linux Block Devices and Partitions - How the kernel represents different storage types (SATA, NVMe, and their partitions) # Tree view of all disks lsblk # Detailed view with UUIDs and Filesystem Types lsblk -f 1.2 Managing Partitions ( fdisk ) To create a formatted space on a disk, we use fdisk or parted . # Open the utility for disk /dev/sdb sudo fdisk /dev/sdb # Commands inside tool: # 'n' -> New Partition # 'w' -> Write Changes Section 1 Checkpoint Summary : Linux treats disks as Block Devices (such as /dev/sda ), and the lsblk command visualizes the storage hierarchy in a tree format while fdisk is used to create partitions. It is important to understand that Proxmox platform combining KVM - Type 1 hypervisor and LXC needs the underlying operating system to recognize and manage the disk before it can use it for virtual machine storage. Reflection : Consider why NVMe drives have names like nvme0n1 instead of sda . What happens if you attempt to partition a disk that is already mounted and actively in use? Resources : Arch Wiki: Fdisk 2. Logical Volume Manager (LVM) Traditional partitions are rigid. If /dev/sda1 is 10GB and fills up, you cannot easily \"grow\" it if /dev/sda2 is right next to it. LVM abstracts physical disks into a flexible pool of storage. 2.1 The LVM Hierarchy The three-tier architecture of LVM provides the flexibility that traditional partitions lack. As shown in the diagram below, the hierarchy flows from physical disks to virtual volumes: Figure 2: LVM Three-Tier Hierarchy - Physical Volumes (PV) combine into Volume Groups (VG), which are divided into Logical Volumes (LV) The hierarchy consists of three layers. At the foundation is the Physical Volume (PV) , which represents the actual disk or partition (for example, /dev/sdb ). These physical volumes are then combined into a Volume Group (VG) , which acts as a unified storage pool\u2014for instance, a data_pool might aggregate multiple drives to provide 500GB of total capacity. Finally, Logical Volumes (LV) are carved out from the volume group and allocated for specific uses, such as vm-100-disk for a virtual machine. 2.2 Hands-On LVM Commands Proxmox platform combining KVM - Type 1 hypervisor and LXC uses LVM extensively. Here is how you manage it manually. # 1. Initialize a disk for LVM sudo pvcreate /dev/sdb # 2. Create a Volume Group named 'data_vg' sudo vgcreate data_vg /dev/sdb # 3. Create a 10GB Logical Volume sudo lvcreate -n my_volume -L 10G data_vg # 4. View your work sudo vgs # View Groups sudo lvs # View Volumes Section 2 Checkpoint Summary : LVM adds significant flexibility over static partitions, enabling dynamic resizing and storage pooling. The architectural flow moves from PV (Physical Volume) to VG (Volume Group) to LV (Logical Volume), and Proxmox platform combining KVM - Type 1 hypervisor and LXC installs to LVM by default to take advantage of these capabilities. Reflection : Can you shrink an LVM volume while it is online and actively in use? What is the difference between standard LVM and LVM-Thin provisioning? Resources : Red Hat LVM Administration 3. ZFS: The Enterprise Standard (New Material) ZFS (Zettabyte File System) is vastly superior to standard hardware RAID. It manages the physical disks directly and provides checksumming, compression, and deduplication. 3.1 Why ZFS? Copy-on-Write (CoW) is one of ZFS's foundational design principles. When you edit a file, ZFS does not overwrite the old data in place. Instead, it writes the new data to a fresh block on the disk and then updates the pointer to reference the new location. The benefit of this approach is profound: if power fails during a write operation, the old data remains valid and intact. There is no corruption because the original block is never destroyed until the write is confirmed to be successful. The illustration below compares traditional write operations (which overwrite data in place) versus ZFS's Copy-on-Write approach: Figure 3: ZFS Copy-on-Write (CoW) - Traditional filesystems overwrite data in place; ZFS writes to new blocks and updates pointers Self-Healing is another critical feature of ZFS. The filesystem stores a cryptographic checksum (a digital fingerprint) for every block of data. If a cosmic ray flips a bit on your drive\u2014an event known as bit rot\u2014ZFS detects the mismatch between the data and its checksum. If redundancy exists (such as in a mirrored or RAID-Z configuration), ZFS automatically repairs the corrupted block by restoring it from a valid copy. The self-healing process is visualized below, showing how ZFS detects, validates, and repairs corrupted data blocks: Figure 4: ZFS Self-Healing - Checksums detect corrupted blocks, which are automatically repaired from redundant copies 3.2 Basic ZFS Commands Proxmox platform combining KVM - Type 1 hypervisor and LXC installs ZFS tools by default. # 1. Check the health of your storage pool sudo zpool status # 2. List all datasets (file systems) sudo zfs list # 3. Create a new dataset for ISOs sudo zfs create rpool/isos 3.3 The Power of Instant Snapshots The Copy-on-Write architecture unlocks one of ZFS's most remarkable capabilities: instantaneous snapshots. Unlike traditional backup systems that must copy gigabytes or terabytes of data (a process that can take hours), a ZFS snapshot - is merely a metadata operation\u2014a lightweight bookmark that marks the current state of the filesystem. When you create a snapshot - , ZFS doesn't duplicate any data blocks; it simply freezes a reference point in time. The snapshot - consumes zero disk space initially because it shares all its data blocks with the current filesystem. Only when data begins to change does the snapshot - start consuming space, as ZFS preserves the old blocks that the snapshot - references while writing new data to fresh locations. This means you can take a snapshot - of a 1TB virtual machine in less than a second, with no performance impact and no initial storage overhead. # 1. Take a snapshot - of a dataset sudo zfs snapshot rpool/data/vm-100-disk-0@before_update # 2. Rollback (Undo changes) sudo zfs rollback rpool/data/vm-100-disk-0@before_update Section 3 Checkpoint Summary : ZFS is a next-generation filesystem with RAID, Copy-on-Write, and checksumming built directly into its architecture. Its self-healing capability detects and fixes silent data corruption (commonly known as bit rot), while snapshots are instantaneous and consume zero space initially due to the Copy-on-Write mechanism. Reflection : Why does ZFS need direct access to the disk (passthrough) rather than working through a traditional RAID controller? What is the \"ARC\" in ZFS terms, and how does it improve performance? Resources : OpenZFS Documentation 4. Virtual Disk - Formats When you create a VM - , its hard drive is just a file on the host. 4.1 Raw ( .raw ) The Raw disk format is effectively a bit-for-bit representation of a hard drive without any additional metadata or container structure. Because it lacks a translation layer, the file is read and written directly to the underlying block device, making it the most performant option available. However, this simplicity comes at a cost; creating a 100GB Raw disk immediately consumes 100GB of physical space (unless sparse provisioning is strictly enforced), and it does not support advanced features like internal snapshots. If you require snapshot - capabilities with Raw disks, you must rely on the underlying storage system, such as LVM-Thin or ZFS, to handle them. 4.2 QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format (QEMU - Type 1 hypervisor for virtualization Copy On Write) QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format is a functional, feature-rich format designed specifically for the QEMU - Type 1 hypervisor for virtualization emulator. Unlike Raw, it acts as an intelligent container that creates a layer of abstraction between the VM - and the physical disk. This allows for powerful features such as internal snapshots, transparent compression, and encryption directly within the file itself. While this abstraction layer introduces a minor performance overhead compared to Raw, the flexibility it offers\u2014particularly the ability to grow the disk file dynamically as data is added\u2014makes it the standard choice for file-based storage backends like NFS or local directories. 4.3 Summary Comparison The visual comparison below highlights the key differences between Raw and QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format disk formats: Figure 5: Virtual Disk - Formats - Raw disks offer maximum performance while QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format provides flexibility with snapshots and thin provisioning Feature Raw ( .raw ) QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format Performance Highest (Near Native) High (Slight Overhead) Space Usage Fixed (Pre-allocated) Dynamic (Grow on demand) Snapshots Requires ZFS/LVM support Built-in (Internal) Portability Universal (Byte stream) QEMU - Type 1 hypervisor for virtualization Specific ### Section 4 Checkpoint Summary : Raw : Fast, pre-allocated, simple. Good for Ceph/LVM. QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format : Flexible, thin-provisioned, internal snapshots. Good for Directory/NFS. Trade-off is usually Performance vs Flexibility. Reflection : Why can't you take an internal snapshot - on a Raw disk? How does \"sparse provisioning\" differ from \"thin provisioning\"? Resources : QEMU Disk Images 8. Additional Resources Proxmox platform combining KVM - Type 1 hypervisor and LXC Storage Wiki : Official Docs ZFS for Dummies : ArsTechnica Guide LVM Cheat Sheet : Red Hat LVM 9. Lab Exercises Lab 1: Storage Goal : Introduction to Local Storage and LVM. Lab 2: Shared Storage Goal : Connecting to NFS and iSCSI targets. Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 3: Networking Course Index Next: Week 5: Containers\n\n--- WEEK 5 NOTES ---\nContainers and Resource Management Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Containers and Resource Management Estimated Reading Time : 35 Minutes [!TIP] How to succeed in this week : Containers are NOT lightweight VMs\u2014they're a different paradigm. Focus on understanding namespaces and cgroups. The orchestration Automated coordination of systems section ( Kubernetes Container orchestration platform orchestration Automated coordination of systems platform orchestration Automated coordination of systems platform orchestration Automated coordination of systems platform orchestration Automated coordination of systems platform orchestration Automated coordination of systems platform ) builds on everything you've learned so far. Welcome to Week 5! This week represents a significant shift in our exploration of virtualization The creation of virtual versions of physical computing resources technology. While the previous weeks focused on virtual machines that virtualize complete hardware stacks, this week introduces containers, a fundamentally different approach to virtualization The creation of virtual versions of physical computing resources that operates at the operating system level. Understanding containers is no longer optional for IT professionals. Container Lightweight package with application code and dependencies technology has become pervasive across cloud computing Computing services delivered over the internet , application development, and modern infrastructure management. Major technology companies deploy millions of containers daily, and cloud platforms like OpenStack Open-source cloud computing platform platform integrate container Lightweight package with application code and dependencies services as core offerings. The conceptual leap from virtual machines to containers requires adjusting your mental model of what virtualization The creation of virtual versions of physical computing resources means. Virtual machines create the illusion of complete computers with virtual hardware, allowing each VM Virtual Machine - A software-based emulation of a physical computer - to run its own operating system kernel. Containers create the illusion of separate systems while sharing the host operating system's kernel. This distinction, while seemingly subtle, has profound implications for performance, resource utilization, deployment speed, and appropriate use cases. By the end of this week, you will not only understand these differences conceptually but also have hands-on experience creating and managing containers in Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization VE using LXC Linux Containers - OS-level virtualization , and familiarity with Docker Platform for developing and running containers and Podman, the dominant container Lightweight package with application code and dependencies technologies in cloud environments. What You'll Learn This Week Articulate the architectural differences between containers and virtual machines Explain the roles of Namespaces and Control Groups (cgroups) in Linux Compare different container Lightweight package with application code and dependencies technologies: LXC Linux Containers - OS-level virtualization , Docker Platform for developing and running containers , Podman, and Apptainer Master the command-line workflows for standard LXC Linux Containers - OS-level virtualization and Docker Platform for developing and running containers Deploy and manage LXC Linux Containers - OS-level virtualization containers using the Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization VE GUI 1. The Container Paradigm To truly grasp how containers differ from virtual machines, we must examine what happens at the kernel level during creation and operation. When you boot a virtual machine on Proxmox platform combining KVM - Type 1 hypervisor and LXC VE, a complex sequence occurs. The hypervisor creates a virtualized hardware environment including virtual CPU, memory, storage controllers, and network adapters. The VM - 's virtual BIOS initializes, the bootloader loads from the virtual disk - , and the kernel initializes. From the kernel's perspective, it is running on a physical computer with exclusive control of hardware resources. Containers operate on entirely different principles. When you create a container, no new kernel boots. The Proxmox platform combining KVM - Type 1 hypervisor and LXC host's kernel\u2014which is itself a Linux kernel\u2014remains the only kernel running. What changes is that the kernel creates isolation boundaries using features built into Linux itself. These features, primarily namespaces and control groups, allow the kernel to present different views of system resources to different groups of processes. Figure 1: VMs vs Containers Architecture - VMs include full guest OS with separate kernel; containers share the host kernel with isolation via namespaces 1.1. The Foundation: Namespaces and Control Groups Linux namespaces provide the fundamental isolation mechanism enabling containers. Each namespace type isolates a different aspect of the system. The PID (Process ID) Namespace isolates the process tree. On a normal Linux system, all processes share a single process ID space. With PID namespaces, each namespace has its own isolated process tree. The container's init process appears as PID 1 from the container's perspective, even if it is PID 2450 on the host. This prevents processes inside the container from seeing or signaling processes on the host. The Network Namespace isolates network configuration completely. Each network namespace has its own network interfaces, IP addresses, routing tables, and firewall rules. This enables functionality that would otherwise conflict, such as multiple containers binding to port 80 on the same physical host. The Mount Namespace controls filesystem visibility, giving each container its own root filesystem (rootfs) that appears as the top of the directory tree. While namespaces provide isolation, they do not inherently limit resource consumption. Without additional mechanisms, a process in a container could consume all available CPU time or memory. Control Groups (cgroups) solve this problem by providing resource limiting, accounting, and prioritization. Understanding cgroups is essential because they form the foundation of resource management in all modern container platforms, from Docker to Kubernetes orchestration platform . 1.2. Understanding Control Groups in Depth Control Groups, commonly abbreviated as cgroups , are a Linux kernel feature that organizes processes into hierarchical groups and applies resource constraints to those groups. Think of cgroups as the traffic police of the Linux system: while namespaces create separate lanes of traffic (isolation), cgroups enforce speed limits and allocate road capacity (resource limits). When you create a container and specify that it can use \"2 CPU cores\" and \"1GB of RAM,\" these limits are enforced through cgroups. The container runtime creates a cgroup for that container and configures the appropriate resource controllers. If the containerized application tries to exceed these limits, the kernel's cgroup subsystem intervenes to enforce the constraints. 1.3. The cgroup Hierarchy Cgroups are organized in a tree-like hierarchy, similar to how processes form a process tree. Each cgroup can have child cgroups, and resource limits applied to a parent cgroup affect all its children. This hierarchical structure enables sophisticated resource allocation strategies. For example, you might create a parent cgroup allocating 50% of CPU to \"production workloads\" and 50% to \"development workloads,\" then subdivide those allocations further among individual containers. 1.4. cgroup Controllers (Subsystems) The power of cgroups comes from specialized controllers (also called subsystems), each responsible for managing a specific type of resource. The most critical controllers for container management include: 1.4.1. CPU Controller This controller regulates access to CPU time. Administrators can use several mechanisms to control CPU allocation: - CPU Shares : A proportional weight system where containers with higher shares get more CPU time when there's contention. For example, if Container A has 1024 shares and Container B has 512 shares, Container A receives twice as much CPU time during periods of high demand. - CPU Quota : An absolute hard limit on CPU usage over a period. Setting a quota of 50000 microseconds per 100000 microsecond period limits the container to 50% of one CPU core. - CPU Sets : Pins containers to specific CPU cores, which is useful for performance-sensitive applications that benefit from CPU cache locality. 1.4.2. Memory Controller This controller manages memory allocation and prevents containers from consuming excessive RAM: - Memory Limit : Sets a hard cap on the amount of RAM a cgroup can use. When this limit is reached, the kernel may trigger the Out-Of-Memory (OOM) killer to terminate processes within the cgroup. - Memory Reservation : A soft limit that acts as a target. The kernel will attempt to reclaim memory from the cgroup when system-wide memory pressure occurs. - Swap Control : Determines whether the cgroup can use swap space and how much. 1.4.3. Block I/O Controller (blkio) This controller limits disk read/write operations: - I/O Weight : Similar to CPU shares, assigns proportional I/O bandwidth. - Throttling : Sets absolute limits on read/write operations per second (IOPS) or bytes per second. 1.4.4. Network Controller While technically separate from the primary cgroup implementation, network bandwidth can also be controlled through Traffic Control (tc) in conjunction with cgroups, allowing administrators to limit network throughput per container. 1.4.5. PID Controller Limits the number of processes (PIDs) that can be created within a cgroup, preventing fork bombs and runaway process creation. 1.5. Practical cgroup Management On modern Linux systems, cgroups are typically managed through the systemd init system, which uses cgroups extensively for service management. However, understanding direct cgroup manipulation is valuable for troubleshooting and advanced container configurations. The cgroup filesystem is typically mounted at /sys/fs/cgroup/ . Each controller has its own subdirectory, and creating a new cgroup is as simple as creating a directory within that hierarchy. 1.5.1. Example: Creating and Configuring a cgroup # View the cgroup hierarchy (cgroups v2) ls /sys/fs/cgroup/ # Create a new cgroup called \"limited_container\" sudo mkdir /sys/fs/cgroup/limited_container # Set memory limit to 512MB echo \"536870912\" | sudo tee /sys/fs/cgroup/limited_container/memory.max # Set CPU quota to 50% of one core (50000 microseconds per 100000 microsecond period) echo \"50000 100000\" | sudo tee /sys/fs/cgroup/limited_container/cpu.max # Add the current shell to this cgroup to test echo $$ | sudo tee /sys/fs/cgroup/limited_container/cgroup.procs # Any process started from this shell now inherits the resource limits 1.5.2. Monitoring cgroup Resource Usage # View current memory usage of a cgroup cat /sys/fs/cgroup/limited_container/memory.current # View CPU usage statistics cat /sys/fs/cgroup/limited_container/cpu.stat 1.6. cgroups v1 vs v2 The Linux kernel has two major versions of the cgroup implementation. cgroups v1 (the original implementation) allowed each controller to operate independently with separate hierarchies. cgroups v2 (the modern unified hierarchy) consolidates all controllers into a single tree structure, simplifying management and improving consistency. Most modern Linux distributions (such as Ubuntu 22.04+ and Debian 11+) have transitioned to cgroups v2 by default. Docker and other container runtimes now support both versions, though v2 offers improved performance and a cleaner API. 1.7. How Container Runtimes Use cgroups When you run a container with resource limits, the container runtime (Docker, containerd, Podman, or LXC ) performs these operations behind the scenes: Create a cgroup : The runtime creates a new cgroup under its management hierarchy (e.g., /sys/fs/cgroup/system.slice/docker-<container-id>.scope/ ). Set limits : Resource constraints specified in the container configuration are written to the appropriate controller files. Add processes : The container's main process (and all its children) are added to the cgroup by writing their PIDs to cgroup.procs . Monitor : The runtime periodically reads resource usage from the cgroup to provide statistics visible through commands like docker stats . 1.8. Example: Docker and cgroups When you run the following Docker command: docker run -d --name web --cpus = \"1.5\" --memory = \"512m\" nginx Docker creates a cgroup and configures it approximately like this: # Memory limit set to 512MB echo \"536870912\" > /sys/fs/cgroup/system.slice/docker-<ID>.scope/memory.max # CPU quota set to 150% (1.5 cores) echo \"150000 100000\" > /sys/fs/cgroup/system.slice/docker-<ID>.scope/cpu.max You can inspect these settings directly: # Find the container ID CONTAINER_ID = $(docker inspect web --format = '{{.Id}}' ) # View the actual cgroup path docker inspect web --format = '{{.HostConfig.CgroupParent}}' # Check memory limit (on systems with cgroups v2) cat /sys/fs/cgroup/system.slice/docker- ${ CONTAINER_ID }.scope/memory.max Figure 2: Linux Namespaces and Cgroups - Namespaces provide isolation (PID, Network, Mount) while cgroups enforce resource limits (CPU, Memory) 1.9. Section 1 Checkpoint Summary : Namespaces : Provide isolation (Network, PID, Mount). The \"Walls\". Control Groups (cgroups) : Provide resource limiting (CPU, RAM). The \"Police\". Containers share the Hot Kernel; VMs use their own Kernel. Reflection : If a container crashes the kernel, does the host crash? Why can't you run a Windows Container on a Linux Host (natively)? Resources : Red Hat: What are Linux Namespaces? 2. The Container Technologies Landscape The term \"container\" is often used broadly, but in practice, distinct technologies utilize these kernel features for different purposes. It is vital to distinguish between System Containers, which act like virtual machines, and Application Containers, which package individual applications. 2.1. LXC (Linux Containers) LXC provides System Containers. The philosophy behind LXC is to offer a lightweight virtual machine experience without the overhead of hardware emulation. An LXC container boots a full init system (like systemd), runs multiple services (SSH, Cron, Syslog), and persists data like a traditional server. It is ideal for infrastructure consolidation where you require long-running servers but want the density and efficiency of containers. 2.2. Docker Docker popularized Application Containers. Unlike LXC , Docker allows you to package an application and its dependencies into a single runnable unit. A Docker container typically runs a single process (such as Nginx or Python) and is ephemeral in nature. Data is stored in external volumes, and the container itself can be destroyed and recreated easily. Docker is the standard for microservices and modern software delivery. 2.3. Podman Podman is a modern alternative to Docker, developed by Red Hat. It maintains compatibility with Docker's commands and image format (OCI) but differs significantly in architecture. Podman is daemonless; it does not require a background service running as root. Instead, it starts containers directly as child processes of the user. This architecture enhances security and natively supports \"rootless\" containers, allowing unprivileged users to run containers safely. 2.4. Apptainer (formerly Singularity) Apptainer is designed specifically for High Performance Computing (HPC) and research environments. In these environments, users run jobs on shared clusters where they do not have root access. Apptainer accommodates this by encapsulating the entire environment into a single file ( .sif ) and running it with the user's existing privileges. It prioritizes mobility of compute and integration with batch schedulers like Slurm. 2.5. Comparison Table Figure 3: Container Technologies Landscape - LXC for system containers, Docker for application containers, Podman for secure daemonless containers, Apptainer for HPC workloads Feature LXC Docker Podman Apptainer Type System Container Application Container Application Container Compute Container Philosophy \"Lightweight VM \"Single Service\" \"Single Service\" \"Portable Application\" Management lxc-* commands docker CLI podman CLI apptainer CLI Daemon None (Process based) Yes (dockerd) No (Fork/Exec) No Network System IP (Bridge) Port Mapping Port Mapping Host Network (typ.) Root Access Required for setup Required (Daemon) No (Rootless) No (Rootless) Image Format System Templates OCI Layers OCI Layers Single File (.sif) Primary Use Infrastructure / VPS Microservices Secure Microservices Scientific / Research 2.6. Section 2 Checkpoint Summary : LXC : System Containers (OS-like, persistent). Used for VPS/Infrastructure. Docker : Application Containers (Single process, ephemeral). Used for Dev/Microservices. Podman : Daemonless, secure alternative to Docker. Reflection : Why is \"Daemonless\" considered a security feature? Which technology would you use to host a permanent MySQL server: LXC or Docker? Resources : Open Container Initiative (OCI) 3. Working with System Containers (LXC CLI) Before utilizing the graphical interface of Proxmox platform combining KVM - Type 1 hypervisor and LXC VE, it is important to understand the underlying mechanics of LXC using standard command-line tools. This knowledge is applicable to any Linux system running LXC . 3.1. Creating a Container In the Docker ecosystem, users typically \"pull\" an image from a registry. In the LXC ecosystem, the process involves \"creating\" a container from a template - image for quick deployment . A template - image for quick deployment is a script or tarball that constructs the root filesystem for a specific Linux distribution. The lxc-create command handles this process, downloading the necessary files to a directory on the host (typically /var/lib/lxc ). Figure 4: LXC Container Lifecycle - From template - image for quick deployment download through creation, start, attach, stop, to destroy # Syntax: lxc -create -n -t sudo lxc-create -n my-web-server -t download -- --dist ubuntu --release jammy --arch amd64 3.2. Listing and Monitoring To view the status of containers, the lxc-ls command is used. The --fancy flag provides a formatted table showing the state (RUNNING or STOPPED), IP addresses (if running), and autostart configuration. sudo lxc-ls --fancy 3.3. Starting a Container Booting an LXC container is significantly faster than booting a virtual machine because there is no kernel initialization. The lxc-start command simply initiates the init system within the isolated namespace environment. sudo lxc-start -n my-web-server 3.4. Accessing the Container While it is possible to configure SSH for an LXC container, administrators often use lxc-attach to enter the container's namespace directly from the host. This works similarly to jumping into a chroot environment but respects the namespace boundaries. Once attached, you are the root user inside the container and can manage packages and services normally. # Attach to the running process space sudo lxc-attach -n my-web-server # Example operations inside the container root@my-web-server:~# apt update root@my-web-server:~# apt install apache2 3.5. Stopping and Destroying Containers should be stopped gracefully to allow services to terminate correctly. The lxc-stop command sends the appropriate signals. When a container is no longer needed, lxc-destroy removes its configuration and deletes the root filesystem directory. sudo lxc-stop -n my-web-server sudo lxc-destroy -n my-web-server 3.6. Section 3 Checkpoint Summary : Create : Builds a rootfs from a Template - image for quick deployment ( lxc-create ). Start : Boots the init system inside the namespace ( lxc-start ). Attach : Enters the namespace directly ( lxc-attach ). Reflection : How does lxc-attach differ from SSH? Where are the container filesystems actually stored on the host? Resources : Linux Containers (LXC) Project 4. LXC in Proxmox platform combining KVM - Type 1 hypervisor and LXC VE (GUI Workflow) Proxmox platform combining KVM - Type 1 hypervisor and LXC VE integrates LXC natively, wrapping the underlying LXC technologies in a sophisticated management interface. This abstracts the complexity of command-line management while providing powerful features like backup, replication, and high availability. Figure 8: Proxmox platform combining KVM - Type 1 hypervisor and LXC VE Container Management - GUI workflow from template - image for quick deployment download to container creation with dynamic resource management 4.1. Step 1: Downloading Templates Before a container can be created, a template - image for quick deployment must be available on the configured storage. In the Proxmox platform combining KVM - Type 1 hypervisor and LXC GUI, navigate to the storage view (such as local or local-lvm ). The CT Templates section provides a built-in browser for downloading official templates for various distributions like Ubuntu, Debian, Alpine, and CentOS, as well as TurnKey Linux appliances which come pre-configured with software stacks. 4.2. Step 2: Creating a Container The creation wizard guides you through the configuration. Important settings include the Hostname , which sets the container's internal identity, and the Unprivileged option. Unprivileged containers are the default and recommended choice; they use user namespaces to map the container's root user to a non-privileged user on the host, significantly reducing the impact of a potential container escape. During Disk and CPU/Memory configuration, you set the resource limits that cgroups will enforce. 4.3. Step 3: Managing Resources Dynamically One of the key advantages of containers is the ability to adjust resources without rebooting. If a container is under memory pressure, you can navigate to the Resources tab in Proxmox platform combining KVM - Type 1 hypervisor and LXC and increase the Memory limit. The change is applied instantly to the running container's cgroup. This elasticity allows for highly efficient resource management compared to the static allocation often required for virtual machines. 4.4. Section 4 Checkpoint Summary : Proxmox platform combining KVM - Type 1 hypervisor and LXC uses typical LXC tech but wraps it in a GUI for ease of use. Templates : Must be downloaded to storage before creation. Unprivileged : Maps root inside container to non-root outside for security. Reflection : Why is \"Unprivileged\" the default? How does dynamic resource resizing work with cgroups? Resources : Proxmox VE: Linux Container (LXC) 5. Working with Application Containers (Docker CLI) Docker transformed how developers and operations teams approach application deployment. Before Docker, deploying applications required extensive documentation detailing all dependencies, library versions, and system configurations. Each deployment environment potentially differed, causing \"works on my machine\" problems. Docker containers package applications with all dependencies, creating standardized units that run consistently anywhere Docker runs. 5.1. Docker Architecture Components The Docker platform comprises several interconnected components forming a complete ecosystem. Understanding this architecture clarifies how Docker operates. At the foundation, the Docker daemon ( dockerd ) runs as a persistent background service, managing Docker objects like images, containers, networks, and volumes. The Docker CLI ( docker ) provides the familiar command-line interface. When you run a command, it translates this into API calls to the daemon. Figure 5: Docker Architecture - Docker CLI communicates with Docker Daemon via API\" REST API to manage images, containers, networks, and volumes Docker Images serve as the templates from which containers instantiate. An image is a read-only layered filesystem containing everything needed to run an application: base OS files, application code, runtime environments, and system libraries. Registries are repositories that store and serve these images. Docker Hub is the public registry hosting millions of images, while organizations may operate private registries for proprietary applications. 5.2. Running a Container Docker's most fundamental operation is docker run , which combines checking if the image exists locally, pulling it if missing, creating a container, and starting it. # Syntax: docker run [options] [command] docker run -d --name web -p 8080:80 nginx Breaking this command down: -d runs the container detached (in the background). --name web assigns a meaningful name, avoiding random identifiers. -p 8080:80 maps host port 8080 to container port 80, making the web server accessible at http://localhost:8080 . The argument nginx specifies the image name; Docker pulls nginx:latest from Docker Hub by default. 5.3. Managing Container Lifecycle Managing running containers involves a set of essential commands for inspection and control. # List only running containers docker ps # List all containers including stopped ones docker ps -a # View container logs (stdout/stderr) docker logs web # Stop container gracefully docker stop web # Forcefully kill container docker kill web # Remove stopped container docker rm web 5.4. Inspecting and Debugging The docker exec command is particularly useful for troubleshooting. It allows you to execute commands inside a running container. The -it flags allocate an interactive pseudo-TTY, giving you a shell prompt inside the container to inspect processes or check configurations. # Interactive shell in running container docker exec -it web bash Resource usage can be monitored using docker stats , which provides real-time information on CPU, memory, and network usage for running containers. 5.5. Building Custom Images with Dockerfiles While pre-built images satisfy many needs, real applications require custom images. A Dockerfile image is a text file containing instructions for building an image, where each instruction creates a new layer. Consider a simple Python web application. A Dockerfile image might look like this: # Base image - using slim variant to reduce size FROM python:3.11-slim # Set working directory for subsequent instructions WORKDIR /app # Copy requirements file first (for caching efficiency) COPY requirements.txt . # Install dependencies RUN pip install --no-cache-dir -r requirements.txt # Copy the rest of the application code COPY . . # Create a non-root user for security RUN useradd -m appuser USER appuser # Document that the app listens on port 5000 EXPOSE 5000 # Default command to run the application CMD [ \"python\" , \"app.py\" ] Each instruction has a specific purpose. FROM establishes the base image. WORKDIR sets the working directory. The sequence of copying requirements.txt before the rest of the code leverages layer caching : if your application code changes but your dependencies do not, Docker reuses the layer where dependencies are installed, significantly speeding up rebuilds. USER switches to a non-privileged user, a critical security best practice. CMD defines the command that runs when the container starts. Figure 6: Dockerfile image Image Layers - Each instruction creates a new layer; cached layers speed up rebuilds when only code changes Building the image from this Dockerfile image is done with the docker build command: docker build -t myapp:1.0 . This reads the Dockerfile image in the current directory ( . ) and builds an image tagged as myapp:1.0 . 5.6. Section 5 Checkpoint Summary : Images : Read-only layers containing the app. Containers : Runnable instances of images. Dockerfile image : Recipe for building images. Reflection : Why do we put the dependencies copy/install step before copying the app code in a Dockerfile image ? What happens to data inside a Docker container when you delete it? Resources : Docker Curriculum 6. Working with Podman (Daemonless CLI) Podman is a drop-in replacement for Docker in most scenarios, but its architecture is fundamentally different. Because it is daemonless, we do not need a background service. Figure 7: Podman vs Docker - Docker requires root daemon; Podman uses daemonless fork/exec approach for enhanced security 6.1. Running a Container (Rootless) By default, Podman runs containers as the user who invoked the command, mapping the user's UID to root inside the container. This is a significant security advantage. # Run a container just like Docker (commands are identical) podman run -d --name rootless-web -p 8081:80 nginx # List containers (These belong to YOUR user, not root) podman ps 6.2. Pods Podman allows you to manage \"Pods\" locally. A Pod orchestration platform is a group of containers that share the same network namespace (localhost), a concept directly compatible with Kubernetes orchestration platform . # Create a Pod orchestration platform that exposes port 8888 podman pod create --name my-pod -p 8888:80 # Run a container INSIDE the Pod orchestration platform # Note: We don't map ports here; the Pod orchestration platform holds the port mapping podman run -d --pod my-pod --name member1 nginx 6.3. Section 6 Checkpoint Summary : Daemonless : Podman runs as the user process, avoiding the \"Root Daemon\" risk. Rootless : Allows unprivileged users to run containers safely. Pods : Groups of containers sharing a network namespace (localhost). Reflection : How can a rootless container bind to port 80 (privileged port)? Why is Podman considered \"Kubernetes orchestration platform -friendly\"? Resources : Podman: Getting Started 7. Future Preview: Kubernetes orchestration platform and kubectl While Docker and Podman manage containers on a single host , modern cloud infrastructure requires managing containers across hundreds of hosts . This is the role of Kubernetes orchestration platform (K8s) , the Container Orchestrator. In this course, we will not install Kubernetes orchestration platform manually (the \"Hard Way\"). Instead, in Week 11 , we will use OpenStack platform Magnum to deploy a production-ready Kubernetes orchestration platform cluster instantly. However, the tool you use to talk to that cluster\u2014 kubectl \u2014is universal. 7.1. The Core Concepts (The K8s Dictionary) Understanding K8s requires learning a new vocabulary, distinct from Docker's: Docker Concept Kubernetes orchestration platform Concept Description Container Pod orchestration platform A Pod orchestration platform is the smallest unit. It usually contains one container (like Nginx), but can contain helpers (\"Sidecars\"). Pods share a network namespace (localhost). Volme Volume / PVC Storage that persists beyond the Pod orchestration platform 's lifecycle. Network Service A stable IP address/DNS name that sits in front of dynamic Pods. If a Pod orchestration platform dies and is replaced, the Service IP stays the same. Compose File Manifest A YAML file describing the \"Desired State\" (e.g., \"I want 3 copies of Nginx\"). 7.2. The Control Loop (Desired State) Kubernetes orchestration platform is Declarative . Unlike Docker, where you say \"Run this container\" (Imperative), in Kubernetes orchestration platform you say \"I want 3 Nginx Pods\" (Declarative). The Control Plane constantly checks: 1. What is the User's Desired State? (3 Pods) 2. What is the Actual State? (2 Pods running) 3. Action : Create 1 more Pod orchestration platform to match the desire. 7.3. The kubectl CLI Start familiarizing yourself with these commands now. You will use them extensively in Week 11. 1. Creating Resources (Imperative) # Docker equivalent: docker run -d --name nginx nginx kubectl run nginx-pod --image =nginx --restart =Never Command Analysis : * run : Tells Kubernetes orchestration platform to create a single Pod orchestration platform . * --image=nginx : Uses the standard Nginx image from Docker Hub. * --restart=Never : Ensures this is treated as a simple Pod orchestration platform , not a managed service that automatically restarts. 2. Inspecting Resources # List all Pods (Docker equivalent: docker ps) kubectl get pods # Get detailed info (logs, events, IP) kubectl describe pod nginx-pod Command Analysis : * get : The universal command to list resources. You can use it for pods , nodes , services , etc. * describe : Shows the \"Event Log\" for a specific resource. If your Pod orchestration platform creation failed (e.g., \"ImagePullBackOff\"), this command tells you why . 3. Scaling Applications # Create a Deployment (Manages Replicas) kubectl create deployment web-app --image =nginx --replicas = 3 # Scale it up manually kubectl scale deployment web-app --replicas = 10 Command Analysis : * create deployment : Instead of a single Pod orchestration platform , we create a Controller that manages Pods. * --replicas=3 : We tell K8s we want 3 identical copies. K8s will start 3 Pods immediately. * scale : Changing this number updates the \"Desired State.\" Kubernetes orchestration platform effectively \"forks\" 7 more copies to reach 10. 4. Exposing to the World # Create a Service to give the Pods a stable IP kubectl expose deployment web-app --port = 80 --type =NodePort Command Analysis : * expose : Creates a Service that fronts the Deployment. * --type=NodePort : Opens a specific port (e.g., 32000) on every node in the cluster. This allows external traffic to reach your internal Pods. * --port=80 : The internal port the container is listening on. 7.4. Self-Correction Checklist Pod orchestration platform vs Container : A Pod orchestration platform wraps a container. K8s manages Pods, not containers directly. Service : Without a Service, you cannot reliably talk to a Pod orchestration platform because its IP changes every time it restarts. Magnum : We will use OpenStack platform Magnum to build the cluster, so we don't have to manage the Master Nodes ourselves. 8. Summary This week you have bridged the gap between traditional virtualization and modern \"Cloud Native\" approaches. We have distinguished between Virtual Machines , which provide strong hardware-level isolation for complete operating systems, and LXC Containers , which provide lightweight efficiency for long-running Linux infrastructure. Additionally, we explored Application Containers (Docker/Podman), which provide ephemeral, portable packages for shipping software. In enterprise environments, these technologies often coexist in a layered architecture: a physical cluster runs Proxmox platform combining KVM - Type 1 hypervisor and LXC VE; Proxmox platform combining KVM - Type 1 hypervisor and LXC hosts Virtual Machines and LXC containers; and inside those VMs, Docker runs the individual microservices. This \"nesting\" leverages the strengths of each technology: the management and security of virtualization with the agility of application containers. 9. Additional Resources 9.1. Video Tutorials LXC in Proxmox platform combining KVM - Type 1 hypervisor and LXC : YouTube (Compares to VMs, setup guide). Docker vs. Podman : YouTube (Explains differences, cloud use cases). Docker Basics : YouTube (Beginner guide to Docker containers). 9.2. Documentation & Further Reading Proxmox LXC Documentation Docker Documentation Podman Documentation OpenStack Zun 10. Lab Exercises Lab 1: System Containers (LXC) Goal : Introduction to System Containers on Proxmox platform combining KVM - Type 1 hypervisor and LXC . Lab 2: Docker Ecosystem & Orchestration Goal : Docker Fundamentals, Networking, Compose, and Portainer. Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 4: Storage Course Index Next: Week 6: Clustering\n\n--- WEEK 6 NOTES ---\nCluster and High Availability Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Cluster Group of servers working together and High Availability System design for minimal downtime (99.9%+ uptime) Estimated Reading Time : 30 Minutes [!TIP] How to succeed in this week : Clustering is a complex topic where \"split-brain\" and \" quorum Minimum nodes needed for cluster to function to function \" are more than just buzzwords\u2014they determine if your data survives a failure. We strongly recommend you strictly follow the lab exercises to build a 3-node cluster Group of servers working together . Seeing a node fail and recover automatically in your lab will make these abstract concepts concrete. Welcome to Week 6! Up until now, our journey has focused on singular, standalone \"pet\" servers. We have treated each Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization node as an individual entity: if it fails, the virtual machines hosted on it fail with it, requiring manual intervention to restore service. This model is acceptable for home labs but insufficient for enterprise environments where uptime is critical. In a true Cloud Environment, servers are treated as \"cattle\"\u2014interchangeable resources where the failure of one unit does not disrupt the overall system. This week, we transition from standalone virtualization The creation of virtual versions of physical computing resources to Clustering . We will link multiple Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization nodes together to form a unified datacenter. We will explore the mechanics of the Quorum Minimum nodes needed for cluster to function to function (the decision-making algorithm that maintains cluster Group of servers working together integrity) and implement High Availability System design for minimal downtime (99.9%+ uptime) ( HA High Availability - System design for minimal downtime - System design for minimal downtime ) , ensuring that our workloads become resilient to physical hardware failure. What You'll Learn This Week Cluster Group of servers working together Management: We will master pvecm , the command-line tool for creating, joining, and managing cluster Group of servers working together nodes. The Voting Algorithm: We will deconstruct the mathematical logic behind Quorum Minimum nodes needed for cluster to function to function and how it prevents the catastrophic \"Split Brain\" scenario. Corosync Cluster engine for group communication engine for group communication engine for group communication engine for group communication Engine: We will examine the low-latency communication protocol that acts as the cluster Group of servers working together 's heartbeat. High Availability System design for minimal downtime (99.9%+ uptime) ( HA High Availability - System design for minimal downtime - System design for minimal downtime ): We will configure Watchdogs and Fencing Safety mechanism to isolate failed cluster nodes nodes to automate the recovery of VMs during node failures. Live Migration Moving a running VM between hosts without downtime between hosts without downtime between hosts without downtime - between hosts without downtime : We will perform the magic of moving running workloads between servers without dropping a single network packet. Part 1: The Cluster Stack At the heart of Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization VE's clustering capability lies the Corosync Cluster engine for group communication engine for group communication engine for group communication engine for group communication Cluster Group of servers working together Engine . This crucial component provides the reliable, low-latency communication layer that allows nodes to share state. Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization Cluster Group of servers working together Manager ( pvecm ) wraps this complex engine into a user-friendly toolset. Figure 1: Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization Cluster Group of servers working together Architecture - Nodes communicating via Corosync Cluster engine for group communication engine for group communication engine for group communication engine for group communication Ring X on a dedicated low-latency network to maintain shared state 1. Creating a Cluster While the web interface provides a convenient way to create clusters, understanding the (CLI) is essential for troubleshooting and automation. The CLI exposes the underlying steps of key generation and configuration distribution. 1.1 Initialization When you initialize a cluster, Proxmox platform combining KVM - Type 1 hypervisor and LXC performs several critical actions. It generates a cryptographic key ( /etc/corosync/authkey ) appearing to secure communication and creates the central configuration database ( /etc/pve/corosync.conf ). This database is essentially the \"source of truth\" for the entire cluster. # Create a cluster named 'Education-Cloud' pvecm create Education-Cloud # Verify status to confirm the local node determines it is now part of a cluster pvecm status 1.2 Joining a Node Adding a second server is a \"join\" operation, not a creation operation. You instruct the new node to connect to the existing ring. The new node authenticates using the root password or an explicit join token, downloads the cluster keys and configuration files, and restarts its local services to synchronize with the quorum to function . # Run this on Node 2 pvecm add 192.168.1.10 # IP of Node 1 Crucial Requirement : For a cluster to function correctly, every node must have a unique Hostname and a persistent Static IP address. If an IP address changes after the cluster is formed, Corosync engine for group communication communication will break, causing the node to lose quorum to function and effectively disconnecting it from the datacenter. Section 1 Checkpoint Summary : pvecm is the primary command-line tool for managing the cluster lifecycle, wrapping the underlying Corosync engine for group communication engine. Cluster Requirements are strict: nodes must have unique hostnames, static network configurations, and a reliable low-latency network connection. Joining involves a node authenticating to an existing cluster leader to download shared keys and configuration state. Reflection : Why does Proxmox platform combining KVM - Type 1 hypervisor and LXC use SSH keys for cluster communication alongside Corosync engine for group communication keys? What is the impact on the cluster configuration file /etc/pve/corosync.conf if you change a node's IP address without updating it? Resources : Proxmox VE Cluster Manager 2. Quorum to function : The Rule of Majority Algorithm The most critical safety mechanism in any distributed system is Quorum to function . In the context of Proxmox platform combining KVM - Type 1 hypervisor and LXC , Quorum to function refers to the minimum number of votes required for the cluster to be considered \"functional.\" This ensures that if the cluster fragments into disconnected pieces, only one piece\u2014the majority\u2014is allowed to modify state. This maintains data consistency and prevents divergent histories. Figure 2: Quorum to function Voting Logic - How the formula (Total/2)+1 determines the operational status of a cluster segment 2.1 The Split Brain Condition \"Split Brain\" is a catastrophic failure state in a clustered environment where network communication is severed between nodes, yet the nodes themselves remain operational. Consider a two-node cluster (Node A and Node B) where the heartbeat connection fails: The Divergence : Node A cannot see Node B and assumes Node B has failed. Simultaneously, Node B assumes Node A has failed. The Conflict : Both nodes promote themselves to \"Master\" status and attempt to take ownership of the same resources (e.g., VM - ID 100). The Consequence : Both nodes mount the same shared storage volume and attempt to write data concurrently. The Result : Since they are unaware of each other's write operations, they overwrite each other's filesystem journals, leading to irreversible data corruption within milliseconds. Figure 3: Split Brain Scenario - A network cut leads to dual active masters ensuring data corruption without quorum to function logic 2.2 Quorum to function Logic To prevent Split Brain, the Proxmox platform combining KVM - Type 1 hypervisor and LXC Cluster Manager (pvecm) enforces a strictly democratic requirement: operations can only proceed if a strict majority of nodes are present. The formula for this is (Total Votes / 2) + 1 . In a 2-Node Cluster , there are 2 total votes. The majority needed is (2/2) + 1 = 2 . This implies that if a single node fails, the survivor has only 1 vote. Since 1 is less than 2, Quorum to function is lost. The surviving node essentially \"locks down,\" forcing the filesystem into Read-Only mode to prevent any possibility of corruption. In a 3-Node Cluster , there are 3 total votes. The majority needed is (3/2) + 1 = 2.5 (which rounds down to integer 2). If one node fails, the remaining two nodes have 2 votes. Since 2 equals 2, Quorum to function is maintained, and the cluster remains fully operational. This highlights the architectural best practice of always designing clusters with an ODD number of nodes (3, 5, 7) to allow for reliable tie-breaking. Section 2 Checkpoint Summary : Quorum to function enforces the \"Rule of Majority\" using the formula (Total/2)+1 to ensure only one part of a partitioned cluster remains active. Split Brain occurs when disconnected nodes both attempt to become Master, leading to guaranteed data corruption. Safety Mechanism : If Quorum to function is lost, the cluster automatically locks down to Read-Only mode to preserve data integrity. Reflection : Why is a 2-node cluster considered \"dangerous\" without an external vote (QDevice)? Does a \"Majority\" mean 51% (more than half) or exactly half? Resources : Corosync Project 3. High Availability (HA - System design for minimal downtime ) Manager Clustering provides a unified management interface, but it does not automatically guarantee uptime. If a node fails in a standard cluster, its VMs simply turn off. High Availability (HA - System design for minimal downtime ) is the automated subsystem designed to solve this problem. Its primary function is to detect physical hardware failures (such as power loss or kernel panic) and automatically restart the affected Virtual Machines on the remaining healthy nodes. This capability minimizes downtime from hours (waiting for an administrator to intervene) to minutes (automatic recovery). Figure 4: HA - System design for minimal downtime Manager Architecture - The Master CRM orchestrating Local LRMs to maintain service availability 3.1 Architecture Components The HA - System design for minimal downtime system is composed of two primary agents that work in tandem to maintain service availability. 3.1.1 HA - System design for minimal downtime Resource Manager (CRM) The Cluster Resource Manager ( pve-ha-crm ) acts as the \"Cluster Manager\" or the \"Boss.\" It runs as a single active instance on the current master node. Its job is to maintain the state of the cluster and make high-level decisions about where services should live. If the node running the active CRM fails, the cluster automatically elects a new master to take over this role. 3.1.2 HA - System design for minimal downtime Local Resource Manager (LRM) The Local Resource Manager ( pve-ha-lrm ) acts as the \"Worker.\" An instance runs on every single node in the cluster. It receives orders from the CRM to start or stop services and reports the status of local resources back to the master. It is responsible for the actual execution of service management commands on the local hypervisor. 3.2 Fencing nodes Mechanism The HA - System design for minimal downtime mechanism relies on absolute certainty. Before the cluster can steal VMs from a non-responsive node, it must be 100% sure that the node is truly dead. 3.2.1 The Split-Brain Problem If Node A stops responding to heartbeats, Node B cannot know if Node A has crashed or if just the network cable was unplugged. If Node B starts Node A's VMs while Node A is still running them, both nodes would attempt to write to the same virtual disks simultaneously, guaranteeing severe data corruption. 3.2.2 The STONITH Solution To solve this, we use Fencing nodes , often referred to by the acronym STONITH (Shoot The Other Node In The Head). Upon detecting a failure, the cluster issues a command to a physical hardware device (like an IPMI controller or a Smart PDU) to physically cut power to the faulty node. This guarantees the node is dead. Only after this confirmation does the cluster restart the VMs on healthy nodes. Figure 5: The Fencing nodes Process - How the cluster physically isolates a failed node before recovering its workloads Section 3 Checkpoint Summary : High Availability (HA - System design for minimal downtime ) automates the recovery of services by restarting VMs on healthy nodes after a hardware failure. CRM and LRM act as the \"Manager\" and \"Worker\" services, respectively, to orchestrate the monitoring and recovery process. Fencing nodes (STONITH) is the essential safety mechanism that physically powers off a non-responsive node to prevent Split Brain before recovery begins. Reflection : Why is Fencing nodes (STONITH) safer than just assuming a silent node is down? Can you have High Availability without Shared Storage? (Consider the implications of ZFS Replication). Resources : Proxmox HA Simulator Part 2: CLI Operations 4. Troubleshooting the Cluster When cluster issues arise\u2014typically indicated by red or gray nodes in the GUI\u2014the web interface often lacks sufficient detail to diagnose the root cause. In these scenarios, the becomes the primary diagnostic tool. Figure 6: Cluster Troubleshooting Flowchart - Decision tree for diagnosing Quorum to function , Corosync engine for group communication , and Network issues 4.1 Check Quorum to function The first step in any cluster diagnosis is to verify the voting state. Run pvecm status to see the cluster's health from the perspective of the local node. Key fields to observe are Votes (number of nodes currently active) and Quorate . If Quorate is No , the cluster has lost its majority and will block any changes to the configuration database ( pmxcfs ) to prevent split-brain, effectively locking the cluster into a read-only mode. pvecm status 4.2 Check Corosync engine for group communication If nodes are not syncing but the network appears up, the issue often lies with Corosync engine for group communication latency. Use systemctl status corosync to check the service health. The logs will reveal if the \"token retransmit time\" is being exceeded. Corosync engine for group communication requires extremely low latency (typically < 2ms) to function correctly. High latency links, such as Wi-Fi or saturated 1Gbps uplinks during backups, often cause Corosync engine for group communication to drop packets and declare nodes dead falsely. systemctl status corosync 4.3 Force Quorum to function (Emergency Only) In a catastrophic scenario where you have a 2-node cluster and one node permanently fails, the survivor will lose quorum to function (1 vote < 2 required). To recover management capability on the survivor, you can artificially lower the expected vote count. pvecm expected 1 Warning : This command tells the survivor, \"Pretend we only expected 1 vote.\" This allows it to become quorate alone. You must only do this if you are absolutely certain the other node is dead. If the other node comes back online while this is active, you will cause a Split Brain scenario. Section 4 Checkpoint Summary : pvecm status is the primary diagnostic tool for assessing voting health and determining if the cluster is Quorate. Corosync engine for group communication Latency is the most common cause of instability; high latency triggers false failure detection. Forcing Quorum to function ( expected 1 ) is a destructive emergency measure to recover a surviving node in a broken cluster. Reflection : Why is latency (Ping time) so critical for Corosync engine for group communication compared to bandwidth? What does \"Quorate: No\" actually mean for your ability to start, stop, or migrate VMs? Resources : Clusterlabs Troubleshooting 5. Live Migration - between hosts without downtime CLI Live Migration - between hosts without downtime is the ability to move a running Virtual Machine from one physical node to another with zero downtime . It works by copying the VM - 's active RAM state over the network to the destination node. Once the memory is synchronized, the hypervisor pauses the VM - on the source node for a fraction of a second, transfers the final CPU state, and resumes execution on the destination node. To the user, this transition is seamless\u2014network connections remain active, and applications continue running without interruption. Figure 7: Live Migration - between hosts without downtime Workflow - Iterative RAM copy followed by atomic switchover for zero-downtime maintenance # General Syntax: qm migrate <VMID> <TargetNode> [OPTIONS] To perform this manually via the CLI: # Migrate VM - 100 to Node named 'pve2' # --online: Keep it running (True Live Migration - between hosts without downtime ) # --with-local-disks: Move the storage (disk image) too using storage migration qm migrate 100 pve2 --online --with-local-disks Section 5 Checkpoint Summary : Live Migration - between hosts without downtime moves active RAM state between nodes, allowing hardware maintenance without service interruption. --online ensures the VM - remains responsive during the transfer; without it, the VM - would hibernate and resume (offline migration). --with-local-disks enables migrations even without shared storage by copying the disk image alongside the RAM, though this takes significantly longer. Reflection : What happens to the VM - if the network cable is unplugged during the RAM copy phase of a migration? Why must the CPU Type often be set to kvm64 or host model carefully in heterogeneous clusters? Resources : QEMU Migration Documentation Part 3: Cluster Storage & Backups 6. Enterprise Shared Storage Architectures 6.1 Distributed Storage: Ceph (Advanced) While ZFS is the gold standard for local storage, modern data centers often span multiple servers. Ceph is a massively scalable, distributed, self-healing file system that runs across a cluster of Proxmox platform combining KVM - Type 1 hypervisor and LXC nodes. 6.1.1 Architecture Components Ceph is not just software; it is a living ecosystem made of daemons: OSD (Object Storage Daemon) : The workhorse. One OSD runs per physical disk. It handles reading, writing, and replicating data. MON (Monitor) : The brain. It maintains the \"Cluster Map\"\u2014the master list of which nodes are alive and where data lives. You usually need at least 3 MONs for quorum to function . MGR (Manager) : Collects metrics and state for the GUI dashboard. 6.1.2 Implementation in Proxmox platform combining KVM - Type 1 hypervisor and LXC (HCI) Proxmox platform combining KVM - Type 1 hypervisor and LXC VE is unique because it integrates Ceph directly into the hypervisor (Hyper-Converged Infrastructure). You do not need external storage servers. The architecture diagram below shows how OSDs, MONs, and MGRs work together across a Ceph cluster: Figure 8: Ceph Distributed Storage - OSDs manage disks, MONs maintain cluster maps, and MGRs collect metrics across multiple nodes Hardware Requirements : To be viable, you need at least 3 Nodes (for a 2/3 replica quorum to function ) and a 10GbE+ Dedicated Network (re-balancing data consumes massive bandwidth). The \"Proxmox platform combining KVM - Type 1 hypervisor and LXC Way\" : You don't edit config files manually. You use the Proxmox platform combining KVM - Type 1 hypervisor and LXC Web GUI -> Datacenter -> Ceph to install packages, initialize the network, and create OSDs. Self-Healing : If a drive fails, Ceph detects the missing \"Placement Groups\" (PGs) and automatically re-replicates that data to other OSDs to restore full health. 6.2 External Shared Storage (SAN & NAS) While Ceph is great for internal storage, many enterprises already have massive external storage arrays (SANs). Proxmox platform combining KVM - Type 1 hypervisor and LXC connects to these using standard protocols. Network Attached Storage (NAS) : Uses NFS or SMB . The storage array manages the filesystem. Proxmox platform combining KVM - Type 1 hypervisor and LXC simply mounts a folder. It's easy, but effectively \"Serial\" (files are locked individually). Storage Area Network (SAN) : Uses iSCSI or Fibre Channel . Proxmox platform combining KVM - Type 1 hypervisor and LXC sees a raw block device over the network. Parallel / Cluster File Systems : To allow multiple Proxmox platform combining KVM - Type 1 hypervisor and LXC nodes to mount the same SAN LUN simultaneously and write to it without corrupting data, we use a Clustered File System like GFS2 (Global File System 2) or OCFS2 . Locking : These systems use a specialized Distributed Lock Manager (DLM) to ensure that if Node A is writing to a file, Node B knows about it instantly. LVM-Shared : Alternatively, Proxmox platform combining KVM - Type 1 hypervisor and LXC often uses LVM on top of iSCSI in \"Shared Mode\" to manage raw disk volumes for VMs without a full filesystem layer. Section 6 Checkpoint Summary : Ceph (HCI): Distributed, self-healing storage on compute nodes (3+ nodes req). SAN/NAS : External storage arrays. Block (iSCSI/FC) vs File (NFS). Cluster FS : GFS2/OCFS2 needed for simultaneous shared writes. Reflection : Why is a 10GbE network mandatory for Ceph? What happens if two servers write to a standard ext4 non-clustered disk at the same time? Resources : Ceph Intro 7. Proxmox platform combining KVM - Type 1 hypervisor and LXC Backups (VZDump) One of the most persistent dangerous misconceptions in cloud infrastructure is equating a Snapshot - with a Backup . 7.1 Backups (VZDump) vs Snapshots Snapshot - : A point-in-time \"difference file\" linked to the original disk. Dependent. Backup (VZDump) : A comprehensive, independent archive (config + compressed disk data, e.g., .vma.zst ). It can be moved offsite for disaster recovery. Figure 9: Snapshot - vs. Backup - Snapshots are dependent save points for testing; Backups are independent archives for disaster recovery 7.2 Proxmox platform combining KVM - Type 1 hypervisor and LXC Backup Modes When performing a backup, the state of the VM - determines the consistency of the data. Figure 10: Proxmox platform combining KVM - Type 1 hypervisor and LXC Backup Modes - Live (Snapshot - ), Suspend (Frozen), and Stop (Consistent) modes balance uptime vs. data consistency Snapshot - Mode (Live) : No downtime. Uses QEMU - Type 1 hypervisor for virtualization to pause writes for a microsecond. Ideal for production. Suspend Mode : Brief pause. Good for consistency but disrupts services. Stop Mode : Full power off. 100% consistent, maximum downtime. Section 7 Checkpoint Summary : Backup : Independent Archive, essential for DR. Modes : Snapshot - (Live), Suspend (Frozen), Stop (Consistent). 8. Managing Storage via CLI ( pvesm ) Advanced troubleshooting often requires the **Proxmox platform combining KVM - Type 1 hypervisor and LXC VE Storage Manager`. # 1. Edit the Storage Configuration file nano /etc/pve/storage.cfg # 2. List all configured storage pvesm status # 3. Allocating a volume manually pvesm alloc local-lvm 100 vm-100-disk-1 20G Section 8 Checkpoint Summary : pvesm helps when the GUI is unavailable. storage.cfg is the cluster-wide storage definition file. 9. Additional Resources Cluster Manager Wiki : Proxmox Docs HA - System design for minimal downtime Simulator : PVE HA Sim Corosync engine for group communication Overview : ClusterLabs 10. Lab Exercises Lab: Cluster & HA Goal : Building a Multi-Node Cluster, configuring HA - System design for minimal downtime , and Live Migration - between hosts without downtime . Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 5: Containers Course Index Next: Week 7: Cloud Concepts\n\n--- WEEK 7 NOTES ---\nTransition to Cloud Computing Concepts Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Transition to Cloud Computing Computing services delivered over the internet Concepts Estimated Reading Time : 20 Minutes [!TIP] How to succeed in this week : This week is the bridge Network device connecting network segments from on-premise to cloud thinking. The shift from \"servers\" to \"services\" is fundamental. Pay attention to the business drivers for cloud adoption\u2014technical skills alone aren't enough. Welcome to Week 7! Congratulations. Up to this point, you have mastered Virtualization The creation of virtual versions of physical computing resources . You can manually create Virtual Machines (VMs), configure bridges, and manage storage volumes. You are a competent System Administrator capable of managing \"Pet\" servers. However, the modern enterprise has evolved. We no longer want to file a ticket and wait three days for a SysAdmin to create a VM Virtual Machine - A software-based emulation of a physical computer - . We want to click a button and receive a resource in 30 seconds. This week, we transition from Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization (A Virtualization The creation of virtual versions of physical computing resources Platform) to OpenStack Open-source cloud computing platform platform (A Cloud Operating System), shifting our perspective from managing individual servers to managing infinite pools of resources. Core Comparison: Proxmox platform combining KVM - Type 1 hypervisor and LXC vs. OpenStack platform Before we dive deep, it is crucial to understand why both exist and where they fit in the Enterprise. It comes down to a fundamental philosophical difference in how infrastructure is treated: the difference between \"Pets\" and \"Cattle.\" Figure 1: Pets vs Cattle - Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization manages individual, unique servers (Pets), while OpenStack Open-source cloud computing platform platform manages disposable, scalable fleets (Cattle) 1. The Fundamental Difference Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization VE is built for the \"Pet\" philosophy. In this model, each server is unique, important, and manually cared for. If a \"Pet\" server gets sick (fails), the administrator rushes to nurse it back to health. This approach implies vertical scaling (making the VM Virtual Machine - A software-based emulation of a physical computer - bigger) and is ideal for workloads that require manual fine-tuning and persistence. OpenStack Open-source cloud computing platform platform , in contrast, is built for the \"Cattle\" philosophy. In this model, servers are disposable resources mass-produced in a factory. They are given numbers, not names. If a \"Cattle\" server gets sick, it is replaced, not fixed. This approach relies on horizontal scaling (adding more VMs) and is designed for developers who consume infrastructure via APIs. 2. Enterprise Deployment Scenarios Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization in the Enterprise is typically found in Small-to-Medium Enterprises (SMEs) or specific branch offices. It excels at hosting \"static\" workloads that do not change often, such as File Servers, Print Servers, Legacy ERP systems, and Domain Controllers. IT Managers choose Proxmox Open-source virtualization platform combining KVM and LXC and LXC Linux Containers - OS-level virtualization and LXC Linux Containers - OS-level virtualization platform combining KVM Kernel-based Virtual Machine - A Type 1 hypervisor Hypervisor Software that creates and manages virtual machines \" Type 1 hypervisor A bare-metal hypervisor that runs directly on hardware and LXC Linux Containers - OS-level virtualization because it is lightweight, easy to maintain, and requires less hardware overhead than a full cloud stack. OpenStack Open-source cloud computing platform platform in the Enterprise is the domain of Large Enterprises, Telecommunications providers, and Research Institutes. It is designed for \"dynamic\" workloads where infrastructure needs to scale up and down automatically to match demand, such as web applications, CI/CD build pipelines, and big data clusters. Organizations choose OpenStack Open-source cloud computing platform platform when they need to provide an \"AWS-like\" self-service experience to their internal teams while strictly maintaining Data Sovereignty within their own private data centers. 1. Defining \"The Cloud\" (NIST Model) According to the National Institute of Standards and Technology (NIST), a system can only be classified as a \"Cloud\" if it strictly meets five specific criteria. If any one of these is missing, it is simply a virtualized data center, not a cloud. On-Demand Self-Service : A consumer must be able to provision computing capabilities automatically without requiring any human interaction from the service provider. Broad Network Access : Capabilities are available over the network and accessed through standard mechanisms (web browsers/APIs). Resource Pooling : The provider's computing resources are pooled to serve multiple consumers, with physical resources dynamically assigned according to demand. The user has no control or knowledge over the exact physical location of the resource. Rapid Elasticity : Capabilities can be theoretically provisioned and released instantly to scale rapidly outward and inward commensurate with demand. Measured Service : Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer (Pay-as-you-Go). 1.1 Service Models (The Pizza Analogy) Cloud computing is delivered via three primary models, best understood through the famous \"Pizza as a Service\" analogy which compares managing infrastructure to eating dinner. Figure 2: NIST Cloud Service Models - The progression from managing everything (On-Prem) to managing nothing (SaaS) On-Premise (Homemade Pizza) represents traditional IT. You own the kitchen, buy the ingredients, cook the pizza, and clean the table. You are responsible for managing every layer of the stack, from networking cables to the application code. Infrastructure as a Service - IaaS (Take & Bake Pizza) gives you the \"Kitchen\" but you bring the \"Pizza.\" The provider manages the physical hardware (Networking, Storage, Servers, Virtualization). You rent the Virtual Machine and are responsible for installing the Operating System, patching it, and running your application. This is the model OpenStack platform provides. Platform as a Service - PaaS (Pizza Delivery) handles almost everything. The provider manages the hardware and the Operating System runtime. You simply provide the code (the \"Order\") and they deliver the running application. You don't worry about Windows updates or Linux kernels; you focus solely on development. Software as a Service - SaaS (Dining Out) is the ultimate abstraction. You simply go to the restaurant and eat. The provider manages everything from the hardware to the application software itself. You just log in and use the tool (e.g., Gmail, Salesforce). 1.2 Service Models Expanded While the Pizza analogy serves as an excellent high-level overview, engineering teams require a more granular understanding of responsibilities in the cloud. Infrastructure as a Service (IaaS) represents the foundational layer of cloud computing. In this model, the cloud provider supplies the raw compute power, storage, and networking hardware, effectively serving as a virtual data center. As a consumer, you retain the highest level of control but also bear the greatest responsibility. You are tasked with installing the operating system, patching the kernel, configuring firewalls, and managing all middleware and runtime environments. It offers the flexibility to run almost any workload but requires significant administrative overhead. Prominent examples include AWS EC2, Google Compute Engine, and our focus for this course, OpenStack platform . Platform as a Service (PaaS) abstracts the underlying infrastructure to provide a streamlined environment specifically for developers. The provider manages the servers, storage, networking, operating system, and middleware, allowing developers to focus solely on writing and deploying code. This model significantly increases convenience and speed of deployment but sacrifices granular control over the environment. If your application requires a custom kernel module or specific OS tweak, PaaS may be too restrictive. Common examples include Heroku, Google App Engine, and AWS Lambda. Software as a Service (SaaS) delivers fully functional applications directly to the end-user over the internet. In this model, the consumer has zero visibility or control over the underlying infrastructure or platform. The provider manages everything from the hardware to the application code itself, ensuring high availability and security. This model creates maximum convenience for users who simply need a tool to perform a task, such as email or document collaboration. Examples include Gmail, Salesforce, and Microsoft 365. 1.3 Common Cloud Providers The global cloud market is segmented into public hyperscalers and private cloud solutions, each catering to specific enterprise needs. Amazon Web Services (AWS) is the pioneer and current market leader in public cloud computing. It offers the most extensive catalog of services, numbering over 200 distinct products. AWS is often the default choice for startups and enterprises seeking a \"one-stop-shop\" for everything from simple compute to complex machine learning and satellite ground stations. Its maturity and breadth making it a safe, albeit complex, choice for general-purpose cloud computing. Microsoft Azure dominates the enterprise sector, particularly among organizations already invested in the Microsoft ecosystem. Its primary strength lies in its seamless integration with Windows Server, Active Directory, and Microsoft 365. For companies running .NET applications or requiring hybrid cloud setups that extend their on-premise Active Directory to the cloud, Azure offers the path of least resistance. Google Cloud Platform (GCP) differentiates itself through engineering excellence in data analytics, artificial intelligence, and container orchestration. Having invented Kubernetes orchestration platform , Google makes GCP the premier environment for container-native applications and microservices. It is frequently chosen by organizations heavily focused on big data processing and machine learning workflows. OpenStack platform stands apart as the de facto standard for Private Cloud infrastructure. Unlike the hyperscalers where you rent space on someone else's computer, OpenStack platform allows you to build the cloud in your own data center. This is the preferred choice for telecommunications providers, governments, and research institutes (like CERN) who require absolute control over their data sovereignty, or who operate at such a massive scale that renting public cloud resources becomes cost-prohibitive. 1.4 Cloud Deployment Models Understanding the \"Where\" and \"Who\" of cloud computing is defined by three primary deployment models. Figure 3: Cloud Deployment Models - Public (Shared), Private (Dedicated), and Hybrid (Bridged) Public Cloud is the most common model, where resources are owned and operated by a third-party provider (like AWS or Azure) and shared across millions of customers via the public internet. It offers the highest level of efficiency and elasticity but requires trusting the provider with your data and accepting a multi-tenant environment where your \"neighbor\" could be anyone. It is ideal for startups, web hosting, and highly variable workloads that need to scale instantly. Private Cloud is infrastructure provisioned for the exclusive use of a single organization. It can be hosted on-premise (in your own building) or by a third-party, but the hardware is strictly dedicated and never shared with other customers. This model offers maximum control, security, and performance customization, making it the non-negotiable choice for banks, governments, and regulated industries. OpenStack platform is the global standard for building these Private Clouds. Hybrid Cloud represents the best of both worlds, combining Public and Private clouds bound together by technology that allows data and applications to be shared between them. A typical enterprise use case involves keeping sensitive customer databases \"on-premise\" in a Private Cloud for strict security compliance, while running web-server frontends in a Public Cloud to take advantage of infinite scaling during traffic spikes (a technique known as Cloud Bursting). 1.5 Cloud Structure: Regions and Zones The cloud is not a nebulous entity floating in the sky; it is composed of massive physical data centers connected by high-speed fiber optics. Understanding its physical geography is essential for designing resilient applications. Figure 4: Regions vs Availability Zones - A Region contains multiple isolated AZs to prevent a single disaster from taking down the entire service A Region is a specific geographical location (e.g., \"US-East\", \"Europe-West\", \"Africa-South\") that contains a cluster of data centers. Each region is completely independent; if the US-East region loses power or suffers a natural disaster, the Europe-West region remains unaffected. Data compliance laws (such as GDPR or POPI) often dictate exactly which Region you must store your user data in to remain within legal jurisdictions. An Availability Zone (AZ) is an isolated location within a Region. Think of an AZ as a separate physical building (or cluster of buildings) with its own independent power grids, cooling systems, and networking infrastructure. A Region is typically made up of multiple AZs (usually 3 or more). To achieve High Availability, cloud architects deploy applications across multiple AZs. If \"Building A\" burns down, \"Building B\" continues to run the application without interruption. 1.6 The Four Pillars of Cloud Infrastructure Regardless of whether you use AWS, Azure, or OpenStack platform , all cloud platforms are built on four fundamental pillars. Compute represents the processing power (CPU and RAM). In the cloud, this manifests as ephemeral Virtual Machines (Instances) or Containers. These resources are designed to be launched, used, and destroyed on demand. We treat them as disposable workers rather than permanent fixtures. Networking is the nervous system of the cloud. It involves creating Virtual Private Clouds (VPCs), subnets, and defining how traffic flows between them. In a software-defined cloud, we program routers, load balancers, and floating IPs using APIs rather than plugging in physical cables. Storage is divided into two primary types: Block Storage (like a virtual hard drive attached to a VM - ) for operating systems and databases, and Object Storage (like a massive, limitless bucket) for storing unstructured files like photos, videos, and backups. Security in the cloud shifts from physical perimeter defense (locks and guards) to identity-based and software-defined security. Security Groups act as virtual distributed firewalls applied directly to each instance, controlling inbound and outbound traffic at the packet level regardless of network topology. Identity and Access Management (IAM) acts as the central directory, strictly controlling who (users/services) can perform what actions (permissions) on which specific resources. 1.7 The Shared Responsibility Model Moving to the cloud does not absolve you of security responsibilities; it simply changes what you are responsible for. The Shared Responsibility Model is a framework that defines the security obligations of both the Cloud Provider and the Customer. In an On-Premise environment, you are responsible for everything from the physical door lock to the user's password. In an IaaS model (like OpenStack platform ), the Provider secures the physical data center, network hardware, and hypervisor. You, the Customer, are responsible for securing the Guest OS , applications, data, and firewall configurations. As you move up the stack to PaaS and SaaS , the Provider takes on more responsibility (like patching the OS), but you are always responsible for your Data and Identity (User Accounts). 1.8 Cloud Economics: CapEx vs. OpEx One of the primary drivers for cloud adoption is financial, specifically the shift in how IT is funded. Capital Expenditure (CapEx) is the traditional model where you spend a large amount of money upfront to buy physical servers, storage arrays, and network switches. This is an \"Investment\" that depreciates over 3-5 years. You pay for the maximum capacity you might need, which often leads to underutilization. Operational Expenditure (OpEx) is the cloud model where you pay for services as you use them (Pay-as-you-Go). There is no upfront cost. This aligns spending with actual usage. If you shut down your VMs at night, you stop paying. This shifts IT spending from a \"Fixed Cost\" to a \"Variable Cost,\" allowing businesses to be more agile. 1.9 Scalability vs. Elasticity While often used interchangeably by marketing teams, these two terms describe fundamentally different capabilities of a cloud architecture. Scalability is the strategic ability of a system to handle projected growth over time. A scalable system typically involves human intervention to upgrade capacity. This can be achieved through Vertical Scaling (scaling up by adding more RAM/CPU to a single server) or Horizontal Scaling (scaling out by adding more servers to a cluster). It effectively answers the question: \"Can we get bigger if the company grows?\" Elasticity is the tactical ability to scale resources automatically in real-time response to demand, in both directions. An elastic system adds servers when traffic spikes (e.g., during a Black Friday sale) and removes them when traffic drops (at 3 AM), maintaining performance while optimizing cost. It answers the question: \"Can the system resize itself right now without human help?\" 1.10 Migration Strategies (The 6 Rs) Transitioning to the cloud is not a simple \"copy and paste\" operation; it requires a strategic decision for each application in your portfolio. The industry-standard framework, popularized by Amazon Web Services (AWS) and Gartner, defines the \"6 Rs\" of migration. 1. Rehost (\"Lift and Shift\") This is the fastest and lowest-risk strategy. It involves moving an application from on-premise to the cloud without making any changes to its underlying architecture. For example, exporting a virtual machine from VMware and importing it directly into OpenStack platform . While quick, this method typically fails to realize the long-term benefits of the cloud, such as auto-scaling and elasticity, because the application still thinks it is running on a static server. 2. Replatform (\"Lift, Tweak, and Shift\") This strategy involves making small optimizations to the application to run more efficiently in the cloud without rewriting the core code. A common example is migrating a local MySQL database to a managed SQL service (PaaS) or containerizing a legacy application to run on Kubernetes orchestration platform . This offers a middle ground, providing some cloud benefits without the heavy cost of a full rewrite. 3. Refactor (Re-architect) Refactoring is the most expensive but most rewarding strategy. It involves rewriting the application from scratch to be \"Cloud Native.\" For instance, breaking a monolithic Java application into dozens of Python microservices that communicate via APIs. This allows the application to scale infinitely and heal itself, fully unlocking the power of the cloud. 4. Repurchase (\"Drop and Shop\") This involves abandoning the legacy application entirely and moving to a commercial SaaS solution. A classic example is decommissioning an on-premise Microsoft Exchange Server and migrating all users to Microsoft 365. Further examples include moving logical CRM systems to Salesforce. 5. Retire Upon analysis, many organizations discover applications that are no longer useful. These are simply turned off, resulting in immediate cost savings. 6. Retain Some applications are simply too complex, too sensitive, or too intertwined with legacy hardware (like mainframes) to serve as viable candidates for migration. These are deliberately kept on-premise. Section 1 Checkpoint Summary : NIST Cloud Definition : Self-Service, Broad Access, Pooling, Elasticity, Measured. Service Models : IaaS (Rent Hardware), PaaS (Rent Platform), SaaS (Rent App). Deployment : Public (Shared), Private (Dedicated), Hybrid (Mixed). Shared Responsibility : You are always responsible for Data and Identity. Economics : Shift from CapEx (Investment) to OpEx (Consumption). Dynamics : Scalability (Growth) vs Elasticity (Auto-sizing). Migration : Rehost (Fastest) vs Refactor (Best Long-term). Reflection : In a SaaS model (like Gmail), who is responsible if you share a confidential file with the wrong person? (Hint: Shared Responsibility). Why does the CFO (Chief Financial Officer) prefer OpEx over CapEx for a risky new startup? Can a system be Scalable but not Elastic? Give an example. Which migration strategy would you use for a 20-year-old Legacy ERP system that nobody knows how to modify? (Hint: Lift and Shift). Resources : NIST Cloud Computing Definition (PDF) Book : CompTIA Cloud+ Certification Study Guide (Exam CV0-003) - Ben Piper. Whitepaper : AWS \"6 Strategies for Migrating Applications\" 2. Introducing OpenStack platform Figure 5: The OpenStack platform Component Architecture - A modular operating system for data centers OpenStack platform is the standard for building Private Clouds. It is used by Walmart, CERN, and major telecommunications providers to build their own internal \"AWS.\" It is not a single monolithic program but rather a family of independent projects designed to work together via a standard set of APIs . 2.1 The \"Core\" Services (Remember these Names!) OpenStack platform can be overwhelming because it consists of dozens of projects. However, a functional cloud only requires a handful of core services to operate. These \"Big 5\" services form the foundation of almost every deployment, handling computation, networking, identity, storage images, and the user interface. Figure 6: OpenStack platform Core Services Map - Highlighting the interaction between Nova platform compute service for VMs , Neutron platform networking service , Glance platform image service , Keystone platform identity/authentication service , and Horizon platform web dashboard 2.1.1 Nova platform compute service for VMs (Compute) Nova platform compute service for VMs is the heart of OpenStack platform . It is responsible for the entire lifecycle of a Virtual Machine (Instance), from spawning to termination. It is not a single binary but a distributed system. The nova platform compute service for VMs -api accepts requests, the nova platform compute service for VMs -scheduler uses complex filters (like RamFilter ) to decide which physical server is best suited for the VM - , and nova platform compute service for VMs -compute talks to the underlying hypervisor (KVM - Type 1 hypervisor ) to actually run the process. 2.1.2 Neutron platform networking service (Networking) Neutron platform networking service is the \"Plumber\" of the cloud. It manages \"Software Defined Networking\" (SDN). It uses the ML2 Plugin (Modular Layer 2) to talk to different backend technologies, such as Open vSwitch or Linux Bridge. It employs an L3 Agent to handle virtual routing and floating IPs, and a DHCP Agent to automatically assign IP addresses to instances via dnsmasq . 2.1.3 Keystone platform identity/authentication service (Identity) Keystone platform identity/authentication service is the \"Bouncer.\" It provides a single point of integration for securing the cloud. It maintains the Service Catalog (a registry of where all other APIs live) and issues Fernet Tokens \u2014lightweight, encrypted keys\u2014that users and services present to authenticate themselves for every single action. 2.1.4 Glance platform image service (Image) Glance platform image service is the \"Librarian.\" It stores and retrieves virtual machine disk images. When you want to launch an \"Ubuntu 22.04\" server, Nova platform compute service for VMs asks Glance platform image service to provide the image file. Glance platform image service can store these images on a local hard drive, or more commonly, in an enterprise Object Storage system like Ceph, ensuring that your \"golden master\" images remain immutable and secure. 2.1.5 Horizon platform web dashboard (Dashboard) Horizon platform web dashboard is the \"Face\" of OpenStack platform . It provides a web-based graphical user interface (GUI) that allows users to launch instances, configure networks, and managed storage without typing a single command. Under the hood, Horizon platform web dashboard is simply a Python Django web application that translates user clicks into API calls sent to Nova platform compute service for VMs , Neutron platform networking service , and Keystone platform identity/authentication service . 2.2 How they work together: A Day in the Life of a Request To understand cloud architecture, let's trace exactly what happens when a user clicks \"Launch Instance\". It is a coordinated dance between the services. Figure 7: The 5-Step VM - Provisioning Workflow - A coordinated sequence of API calls ensuring authentication, scheduling, networking, and storage provisioning Step 1: Authorization (Keystone platform identity/authentication service ) The user's request first goes to Keystone platform identity/authentication service . It validates the user's Token and checks the Policy engine to ensure they have the specific permission ( compute:create ) required to launch an instance. If the token is expired or the user lacks the \"Member\" role, the request is rejected immediately with a 403 Forbidden error, protecting the cloud resources from unauthorized access. Step 2: Scheduling (Nova platform compute service for VMs ) Once authorized, the request is passed to nova-api . It asks the nova-scheduler : \"Where should I put this VM - ?\" The Scheduler analyzes the state of all available compute nodes using filters. Data centers are heterogeneous, so the scheduler checks constraints: \"Does this node have 4GB RAM free?\" ( RamFilter ) and \"Is this node currently alive?\" ( ComputeFilter ). It then selects the mathematically optimal candidate (e.g., Compute-Node-05 ). Step 3: Network Provisioning (Neutron platform networking service ) Before starting the VM - , Nova platform compute service for VMs acts as a proxy and contacts Neutron platform networking service . It requests a port on the specified network (e.g., private-net ). Neutron platform networking service creates a logical port entry in its database, assigns a MAC address (e.g., fa:16:3e... ), and allocates an IP address (e.g., 192.168.1.15 ) via its IPAM driver. Crucially, it creates this logical reservation before any hardware is touched. Step 4: Image Retrieval (Glance platform image service ) The nova-compute service on the selected node ( Compute-Node-05 ) receives the command to build. It checks its local image cache for the requested OS image (e.g., Ubuntu 22.04). If the image is missing, it contacts Glance platform image service to download it securely via an HTTP API. It then converts the image into a bootable volume, typically using a Copy-on-Write (CoW) backing file to save space and time. Step 5: Wiring (The Final Assembly) Finally, nova-compute instructs the Hypervisor (KVM - Type 1 hypervisor /Libvirt) to define the VM - . Simultaneously, the Neutron platform networking service L2 Agent on that node detects the new port data and programs the local virtual switch (Open vSwitch) to connect the VM - 's virtual network - communication card to the physical network bridge. Only then does the VM - start booting, drawing its IP address from the DHCP agent as it initializes. Section 2 Checkpoint Summary : Nova platform compute service for VMs : Compute (The Brain). Neutron platform networking service : Networking (The Plumber). Keystone platform identity/authentication service : Identity/Auth (The Bouncer). Glance platform image service : Image Storage (The Librarian). Horizon platform web dashboard : Dashboard GUI (The Face). Reflection : If Keystone platform identity/authentication service goes down, can you still log in to Horizon platform web dashboard ? Why does Nova platform compute service for VMs need to talk to Glance platform image service before starting a VM - ? Resources : OpenStack Components Map 3. Additional Resources OpenStack platform Service Map : OpenStack.org The Pizza as a Service Model : Medium Article NIST Cloud Definition : NIST.gov CompTIA Cloud+ Hub : Official Site (Syllabus for CV0-003/004) OpenStack platform vs Proxmox platform combining KVM - Type 1 hypervisor and LXC (Forum Discussion) : Proxmox Forum 6. Lab Exercises Lab 1: Nested Virtualization Goal : Setting up Nested Virt (OpenStack platform prep). Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 6: Clustering Course Index Next: Week 8: Cloud Foundation\n\n--- WEEK 8 NOTES ---\nCloud Foundation (OpenStack platform ) Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Cloud Foundation ( OpenStack Open-source cloud computing platform platform ) Estimated Reading Time : 25 Minutes [!TIP] How to succeed in this week : OpenStack Open-source cloud computing platform platform is complex, but it's just virtualization The creation of virtual versions of physical computing resources at scale with an API Application Programming Interface . Map each OpenStack Open-source cloud computing platform platform service to concepts you already know ( Nova OpenStack compute service for VMs platform compute service for VMs =VMs, Neutron OpenStack networking service platform networking service =Networking, etc.). The architecture diagram is your roadmap. Welcome to Week 8! 1. Case Study: The \"Nebula Inc.\" Startup To understand how a cloud is built, we will follow a continuous scenario for the next three weeks. You have been hired as the Lead Cloud Engineer for a new software startup called \"Nebula Inc.\" Currently, they have no infrastructure\u2014just a credit card and a dream. Your job is to build their Virtual Data Center (VDC) from scratch using OpenStack Open-source cloud computing platform platform . This is not a theoretical exercise; you will be typing the actual commands that cloud administrators use daily to construct the digital fabric of the modern internet. The Roadmap : Week 8 (Foundation) : You must create the secure environment. This involves setting up the company's \" Tenant Grouping of users and resources (also called Project) \" (Project), hiring the staff (Users), purchasing the software licenses (Images), and cabling the office (Networking). Week 9 (Compute) : You will deploy their first web servers and secure them with firewalls. Week 10 (Storage) : You will attach persistent storage for their customer database. By the end of this module, you will have a fully functional, multi-tier cloud application environment running on infrastructure you defined yourself. 2. Deep Dive: Identity Management (Keystone platform identity/authentication service ) Keystone platform identity/authentication service is the central nervous system of OpenStack platform . If Keystone platform identity/authentication service is down, nothing works. It is responsible for Authentication (AuthNWho are you?\") and Authorization (AuthZWhat can you do?\"). In a physical building, Keystone platform identity/authentication service is the security guard at the front desk who checks your ID badge and decides which floors you are allowed to visit. 2.1 The Authentication Workflow (The \"Token Dance\") When you run a command like openstack server list , a complex sequence of events, often called the \"Token Dance,\" occurs in the background before you see any output. Figure 1: The Keystone platform identity/authentication service \"Token Dance\" - Documenting the 7-step process of authentication and authorization Credentials : Your client (CLI or Horizon platform web dashboard ) sends your Username, Password, and Domain to the Keystone platform identity/authentication service API. Validation : Keystone platform identity/authentication service validates these credentials against its Backend (SQL or LDAP). Token Issue : If the credentials are valid, Keystone platform identity/authentication service generates a Token (a temporary digital ID card) and sends it back to you. Service Request : The client then sends the actual request to Nova platform compute service for VMs ( GET /servers ), placing the Token in the HTTP Header ( X-Auth-Token ) as proof of identity. Token Validation : Nova platform compute service for VMs does not trust you or your token blindly. It authenticates the token by sending it back to Keystone platform identity/authentication service with the question: \"Is this token valid?\" Confirmation : Keystone platform identity/authentication service validates the token's signature and expiry in its database. It replies to Nova platform compute service for VMs : \"Yes, this is valid. The user is 'admin' and has the 'member' role.\" Execution : Once confirmed, Nova platform compute service for VMs finally executes the command and returns the server list. 2.2 The Backend (Where are users stored?) Keystone platform identity/authentication service is modular and capable of integrating with existing enterprise systems. It can store users locally or talk to external systems: SQL (Local) : Users are stored in the OpenStack platform database (MariaDB). This is the standard configuration for small deployments and our lab environment. LDAP / Active Directory (Enterprise) : In large enterprises, you do not want to create separate accounts for every system. Keystone platform identity/authentication service can plug directly into the corporate Active Directory. When a user logs in, Keystone platform identity/authentication service forwards the password to the Domain Controller for validation, ensuring Single Sign-On (SSO). 2.3 Token Providers (Fernet vs UUID) The format of the token itself determines the performance of the cloud. UUID (Legacy) : A random string stored in the Keystone platform identity/authentication service Database. The problem with this method is that every validation requires a Database Lookup. In massive clouds handling 10,000 requests per second, this would crush the database. Fernet (Modern) : A cryptographic token format. The token contains the User ID and Expiry Time, encrypted using a secret key held by the Keystone platform identity/authentication service server. The major benefit is that Keystone platform identity/authentication service does not need to store the token in a database. To validate it, it simply decrypts the token significantly reducing database load and allowing Keystone platform identity/authentication service to scale infinitely. 2.4 The Hierarchy Domain : A high-level container (e.g., \"Default\" or \"Corporate_A\"). Use for multi-tenant isolation. Project (Tenant) : The workspace. Resources (VMs, Networks) belong to a Project. User : The human or service account. Role : The permission set. A User must have a Role on a Project to do anything. 2.5 CLI Implementation (Case Study: Nebula Inc.) Now we apply this theory to our startup. \"Nebula Inc.\" requires a dedicated, isolated environment where its developers can work without interfering with other departments. In OpenStack platform , we achieve this \"Multi-Tenancy\" by creating a specific Project . This project will act as the container for all their future VMs, networks, and storage volumes. It also allows us to set quotas (e.g., \"Maximum 10 CPUs\") to control their budget. Step 1: Create the Project openstack project create --domain default --description \"Nebula Inc. Production\" nebula_prod Explanation : --domain default : Specifies that this project lives in the default domain. --description : Metadata for admins. Result : Creates a record in the projects table. Returns a UUID (e.g., a1b2c3... ). Step 2: Create the User openstack user create --domain default --password-prompt nebula_admin Explanation : --password-prompt : Hides input for security. Result : Creating a user identity does NOT grant access. The user is currently \"unemployed\". Step 3: Assign the Role openstack role add --project nebula_prod --user nebula_admin member Explanation : member : The standard permission level (can create VMs/Networks but cannot delete other users). Result : Creates a row in the role_assignments table linking User+Project+Role. 2.6 Identity Verification & Management In a production cloud, security is an ongoing process, not a one-time setup. The \"Principle of Least Privilege\" dictates that we must continuously verify that only the correct people have access to our sensitive data. Simply listing the users in the system is insufficient; a user might exist but have no access to anything. To audit this, we must inspect the Role Assignments . Auditing Access openstack role assignment list --user nebula_admin --names Explanation : --names : Resolves UUIDs to human-readable names. Result : Displays exactly which project the user can access and with what level of authority. Managing Users Disabling a User : bash openstack user set --disable nebula_admin Explanation : Sets enabled=False in the DB. The user cannot request new tokens. Section 2 Checkpoint Summary : Keystone platform identity/authentication service : The core authentication and authorization service; without it, nothing works. Fernet Tokens : Modern, stateless tokens that improve performance by removing database lookups. Role Assignments : The critical link that grants a User permission on a Project. Reflection : 1. Why do we use tokens (like Fernet) instead of just passing the username/password to every service? (Hint: Performance and Security). 2. If you delete a user, their history disappears. If you disable them, it remains. Why is this important for audit trails? Resources : OpenStack Keystone Guide AWS Identity and Access Management (IAM) Microsoft Entra ID (Azure AD) 3. Deep Dive: Image Management (Glance platform image service ) Every Virtual Machine needs a hard drive to boot from. In the physical world, you might walk around with a USB stick containing a Windows or Linux ISO installer. In the cloud, this is inefficient. Instead, we use Glance platform image service , the OpenStack platform Image Service. Glance platform image service acts as a central library where validated, pre-installed operating system templates are stored. When you launch a VM - , Nova platform compute service for VMs contacts Glance platform image service to request a copy of the \"Master Image\" to be streamed to the hypervisor. 3.1 Understanding Disk Formats Not all virtual disks are created equal. You must choose the right format for your cloud workload: RAW is a bit-for-bit copy of the disk. It offers the fastest performance because there is no overhead, but it is space-inefficient. A 10GB drive takes up 10GB of physical space, even if it is empty, making it slow to copy over the network. QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format (QEMU - Type 1 hypervisor for virtualization Copy On Write) is the standard format for OpenStack platform . It supports compression and \"Thin Provisioning,\" meaning a 10GB drive with only 100MB of data takes up only 100MB of physical space. Crucially, it logic enables snapshot - capabilities, allowing you to save the state of a VM - instantly. ISO is a read-only archive used for installation media. While essential for building images, it is rarely used in cloud \"boot-from-image\" scenarios because we prefer pre-installed operating systems. 3.2 Glance platform image service Architecture Glance platform image service is split into distinct components to separate the metadata from the actual data payload. Figure 2: Glance platform image service Architecture - The separation of the API, Registry (Metadata), and Backend Store (Data) Glance platform image service API : The front-end service that accepts user requests (e.g., \"Upload this image\", \"List images\"). It verifies the user's token with Keystone platform identity/authentication service before proceeding. Glance platform image service Registry : An internal service that stores the metadata about images (Name, Size, Format, Owner) in the SQL database. Backend Store : The driver responsible for storing the actual binary data (the heavy bits). While this can be a local file system ( /var/lib/glance ), production clouds typically use a distributed storage cluster like Ceph or an object store like AWS S3 to ensure data durability and accessibility across all compute nodes. 3.3 CLI Implementation (Case Study: Nebula Inc.) For any software company, consistency is key. We cannot have one developer running Ubuntu 20.04 and another running Fedora 35, as this leads to the infamous \"it works on my machine\" problem. To solve this, Nebula Inc. enforces a Standard Operating Environment (SOE) . We will upload a \"Golden Image\"\u2014a pre-approved, security-hardened operating system template - image for quick deployment that all staff must use. For our initial testing phase, we will utilize CirrOS , a lightweight (15MB) Linux distribution designed specifically for validating OpenStack platform clouds, before graduating to full-sized Ubuntu Server images in production. Step 1: Download the Source wget http://download.cirros-cloud.net/0.5.1/cirros-0.5.1-x86_64-disk.img Explanation : We assume we are on the \"Jumpbox\" or Controller node. Browsers cannot upload directly to Glance platform image service CLI; the file must exist locally. Step 2: Upload to Glance platform image service openstack image create \"nebula_standard_os\" \\ --file cirros-0.5.1-x86_64-disk.img \\ --disk-format qcow2 \\ --container-format bare \\ --public \\ --min-ram 64 \\ --min-disk 1 Explanation : --disk-format qcow2 : Defines how the bits are organized. --container-format bare : Indicates no extra metadata wrapper (OVF) is around the file. --public : IMPORTANT. By default, images are \"Private\" (only visible to the uploader). This flag makes it visible to all projects in the cloud (Nebula, Admin, Testing, etc.). --min-ram 64 : A metadata tag. Nova platform compute service for VMs checks this before booting. If a user tries to launch this on a Flavor - template - image for quick deployment defining vCPUs, RAM, and disk with 32MB RAM, Nova platform compute service for VMs will block the request to prevent a crash. Result : The file is streamed into the Glance platform image service Backend Store, and a UUID is generated. 3.4 Managing Images (Day 2 Operations) Once images are uploaded, they are not static. You may need to update their metadata or remove obsolete versions. Listing Images openstack image list Explanation : Returns a table of available images. Result : Checks ID, Name, and Status. Status should be active . If status is queued , the upload failed. Updating Metadata (Properties) Sometimes we forget a flag or need to deprecate an OS. openstack image set --property cpu_arch =x86_64 nebula_standard_os Explanation : Adds a custom key-value pair to the image metadata. The Scheduler use this to ensure the VM - lands on Intel hardware, not ARM. Result : The Glance platform image service Registry is updated; the actual file is untouched. Deleting Images openstack image delete nebula_standard_os Explanation : Marks the image for deletion. Result : The metadata is removed from the Registry, and the backend storage driver (Ceph/File) is instructed to free the space. Section 3 Checkpoint Summary : Glance platform image service : The image repository that provides boot disks to Nova platform compute service for VMs . QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format : The preferred format for cloud images due to thin provisioning and snapshot - support. SOE : Standard Operating Environment ensuring consistency across all machines. Reflection : 1. If you have a slow 1Gb link interconnecting your data centers, which image format (RAW or QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format ) would be faster to replicate? Why? 2. Why is it dangerous to make every image --public ? (Think about licensed software like Windows Server). Resources : OpenStack Glance Guide AWS AMIs (Amazon Machine Images) Azure Compute Gallery 4. Deep Dive: Networking (Neutron platform networking service ) Neutron platform networking service is the \"Software Defined Networking\" (SDN) component of OpenStack platform . In a traditional data center, creating a new network segment means filing a ticket with the network team to configure VLANs on physical Cisco/Juniper switches. In OpenStack platform , Neutron platform networking service gives this power to the user. A tenant can create their own private switching infrastructure, subnets, and routers via API, without ever touching a physical cable. 4.1 What is SDN (Software Defined Networking)? Network Engineers often rely on physical switches and routers to move traffic. In the cloud, we virtualize this entirely using Software Defined Networking (SDN) . The core concept of SDN is the separation of the Control Plane (The Brain) from the Data Plane (The Muscle). Figure 3: Neutron platform networking service SDN Architecture - The separation of the Logical Control Plane (API) from the Physical Data Plane (Open vSwitch) The Control Plane (Neutron platform networking service API/Server) acts as the brain of the operation. When you execute a command to create a network or open port 80, you are communicating with the Control Plane. It calculates the necessary logic and updates the state of the cloud database, but it does not touch a single network packet itself. The Data Plane (OVS Agent/L2 Agent) sits on every compute node and acts as the muscle. It receives instructions from the Control Plane via a message bus (RabbitMQ) and implements them by programming the local virtual switch. It is the actual software responsible for moving packets from your VM - to the physical network card. 4.2 The Virtual Switch: Open vSwitch (OVS) In a physical rack, servers plug into a top-of-rack switch. In OpenStack platform , VMs plug into a virtual switch called Open vSwitch (OVS) . Unlike a standard unmanaged switch that simply learns MAC addresses, OVS is a production-quality, multilayer virtual switch that uses Flow Tables . A Flow Table is a list of programmable rules that match specific packets (e.g., \"If source IP is A and dest IP is B...\") and applies specific actions (e.g., \"...drop packet\" or \"...tag with VLAN 100\"). Neutron platform networking service programs these flow tables dynamically to implement sophisticated features like Security Groups (Distributed Firewalls) and Virtual Routing. 4.3 Under the Hood: The Linux Connection Everything you learned in Week 4 applies here. Neutron platform networking service uses standard Linux kernel features to build these structures: Isolation = Namespaces : When you create a Router or a DHCP server, Neutron platform networking service creates a Linux Network Namespace ( ip netns ). This allows Project A and Project B to both use \"192.168.0.1\" without conflict; they live in parallel, isolated universes. Cabling = Veth Pairs : When a VMplugs in\" to the OVS Bridge, Neutron platform networking service creates a Virtual Ethernet (veth) pair . One end connects to the VM - 's interface (inside KVM - Type 1 hypervisor ), and the other connects to the OVS Bridge. 4.4 Flow of Traffic (North-South vs East-West) Designing a cloud network requires understanding the two primary directions of traffic flow, as they traverse different paths through the infrastructure. Figure 4: North-South vs East-West Traffic - Visualizing how traffic stays within the cloud versus how it exits to the internet East-West Traffic refers to communication between VMs inside the same cloud environment (e.g., Web Server A talking to Database Server B). Ideally, this traffic should never leave the virtual infrastructure. It flows from the source VM - , through the local OVS Bridge, and is typically encapsulated in a tunnel protocol like VXLAN to cross the physical network before arriving at the destination compute node. North-South Traffic refers to communication entering or leaving the cloud (e.g., a User accessing your Web Server from the Internet). This traffic must leave the virtual overlay network. It passes through the Neutron platform networking service Router (which lives inside a Network Namespace), undergoes SNAT (Source NAT) to mask its private IP, and exits via the external provider network gateway. 4.5 CLI Implementation (Case Study: Nebula Inc.) Now that we understand the theory of pipelines and flows, it is time to build. Nebula Inc. requires a private, isolated network segment where their web servers can communicate safely. We will construct a topology consisting of a private Virtual Switch ( nebula_net ), an IP addressing scheme ( nebula_subnet ), and a Virtual Router ( nebula_router ) to connect to the outside world. Step 1: Create the Switch (Network) openstack network create nebula_net Explanation : Initializes the logical switch in the database (Control Plane). Result : A network UUID is created. OVS is not touched yet. Step 2: Define Addressing (Subnet) openstack subnet create --network nebula_net \\ --subnet-range 192.168.50.0/24 \\ --gateway 192.168.50.1 \\ --dns-nameserver 8.8.8.8 \\ nebula_subnet Explanation : --network nebula_net : Attaches this IP logic to the switch. Result : The DHCP Agent (Data Plane) spawns a dnsmasq process in a namespace to serve IPs. Step 3: Build the Gateway (Router) openstack router create nebula_router Explanation : Creates a virtual router instance. Step 4: Wiring (Interface Attachment) openstack router add subnet nebula_router nebula_subnet Explanation : This is the equivalent of plugging a patch cable from the Switch ( nebula_subnet ) into the Router's LAN port. Result : The L3 Agent creates a generic router namespace and assigns the gateway IP 192.168.50.1 . Step 5: Uplink (External Gateway) openstack router set --external-gateway public nebula_router Explanation : Connects the Router's WAN port to the Provider Network ( public ). Result : Enables the router to route traffic to the internet (North-South flow). 4.6 Verification Log in to Horizon platform web dashboard -> Network -> Network Topology . You should see the Nebula Router creating a bridge between the Blue (Private) line and the Red (Public) line. Section 4 Checkpoint Summary : SDN : Separates the \"Brain\" (Neutron platform networking service API) from the \"Muscle\" (OVS/Agents). OVS : Uses Flow Tables to direct traffic and enforce security, replacing physical switch logic. Linux Foundations : Capabilities like Namespaces and Veth pairs are the building blocks of the cloud. Reflection : 1. Recall : In Week 4, we used ip netns exec . How does that relate to a Neutron platform networking service Router? 2. Flows : If you add a Security Group rule to allow SSH, what actually changes on the Compute Node? (Hint: Does Neutron platform networking service update the database or OVS flow tables?) Resources : OpenStack Neutron Guide AWS VPC (Virtual Private Cloud) Azure Virtual Network (VNet) 6. Industry Comparison: The \"Polyglot\" Cloud Engineer While this course uses OpenStack platform because it allows us to see \"under the hood,\" the concepts you learned this week\u2014Tenants, Golden Images, and SDN\u2014are the exact same primitives used by the public cloud giants. 6.1 Concept Mapping Concept OpenStack platform Term AWS Term Azure Term Identity Service Keystone platform identity/authentication service IAM (Identity & Access Mgmt) Microsoft Entra ID (Azure AD) The \"Container\" Project (Tenant) Account Subscription / Resource Group Image Service Glance platform image service AMI Registry Azure Compute Gallery Network Service Neutron platform networking service VPC (Virtual Private Cloud) VNet (Virtual Network - communication ) Routing Neutron platform networking service Router Internet Gateway (IGW) VPN Gateway / VNet Peering 6.2 The \"Standard Operating Environment\" across Clouds In Section 3, we discussed the \"Golden Image.\" This strategy is universal. OpenStack platform : You use Packer to build a QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format image and upload it to Glance platform image service . AWS : You use Packer to build an AMI and upload it to EC2 . Azure : You use Packer to build a VHD and upload it to Azure Compute Gallery . The Conclusion : The tool (Packer) and the workflow (Build -> Validate -> Upload) are identical; only the target file format changes. 7. Summary We have built the Foundation for Nebula Inc.: 1. Identity : A secure Project and User with assigned Roles. 2. Image : A validated OS template - image for quick deployment in Glance platform image service . 3. Network : A fully routed Layer 3 topology. Next week, we Launch. 4. Lab Exercises Lab 1: Cloud Foundation Goal : Introduction to Keystone platform identity/authentication service (Identity) and Glance platform image service (Image) services. Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 7: Cloud Concepts Course Index Next: Week 9: Compute Ops\n\n--- WEEK 9 NOTES ---\nCompute Operations (Nova platform compute service for VMs ) Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Compute Operations ( Nova OpenStack compute service for VMs platform compute service for VMs ) Estimated Reading Time : 30 Minutes [!TIP] How to succeed in this week : Nova OpenStack compute service for VMs platform compute service for VMs is the compute engine\u2014focus on understanding the scheduling process and instance lifecycle. The CLI Command Line Interface commands build on Week 8's foundation, so review Keystone OpenStack identity/authentication service platform identity/authentication service , Glance OpenStack image service platform image service , and Neutron OpenStack networking service platform networking service concepts if needed. 1. Introduction to OpenStack platform Compute (Nova platform compute service for VMs ) In Week 7, we introduced the core services that make up an OpenStack platform cloud. This week, we tear apart the engine to understand the most critical component: Nova platform compute service for VMs . As the primary computing controller, Nova platform compute service for VMs is responsible for orchestrating, scheduling, and managing the lifecycle of virtual machines. Industry Context : To be technically precise, Nova platform compute service for VMs is the Compute Resource Provider . In Azure , you submit a template - image for quick deployment to the Azure Resource Manager (ARM), which routes the request to the Microsoft.Compute provider to execute the build. In OpenStack platform , Nova platform compute service for VMs plays the role of Microsoft.Compute . It is the backend service that accepts the request and orchestrates the hypervisors. Therefore, do not confuse Nova platform compute service for VMs (the Manager) with the VM - (the Resource). Nova platform compute service for VMs creates the VM - . It is important to clarify that Nova platform compute service for VMs does not provide the virtualization itself; that task remains the job of the Hypervisor, such as KVM - Type 1 hypervisor or QEMU - Type 1 hypervisor for virtualization . Instead, Nova platform compute service for VMs acts as the management layer that allows administrators to scale this virtualization across thousands of physical servers seamlessly. Modern Nova platform compute service for VMs utilizes a Cellular Architecture to achieve this massive scale. In early versions of OpenStack platform , a single central database and message queue handled every request in the cloud. However, as clusters grew to tens of thousands of nodes, the database lock contention became a severe bottleneck. The solution was to partition the cloud into \"Cells.\" In this model, the API and Scheduler remain at the global level, but the Compute nodes are grouped into independent cells, each with its own database and message queue. This implies that if \"Cell 1\" suffers a critical database failure, \"Cell 2\" continues to operate without interruption, effectively partitioning the cloud to isolate failures and improve resilience. Section 1 Checkpoint Summary : Nova platform compute service for VMs is the compute controller, equivalent to Microsoft.Compute . Cellular Architecture : Partitions the cloud for scalability and resilience. Hypervisor Agnostic : Nova platform compute service for VMs manages the hypervisor (KVM - Type 1 hypervisor ) but is not the hypervisor itself. Reflection : Why does Nova platform compute service for VMs need a \"Cellular Architecture\" for large-scale clouds? What is the difference between Nova platform compute service for VMs and KVM - Type 1 hypervisor ? 2. Nova platform compute service for VMs Component Anatomy Nova platform compute service for VMs is not a single monolithic program but a distributed system comprised of several communicating daemons, each with a specific role. These are generally divided into the Control Plane (Global Management) and the Data Plane (Node Execution). Figure 1: Nova platform compute service for VMs Architecture - The flow from API (Entrance) to Scheduler (Brain) to Compute (Worker) 2.1 The Global Components (Control Plane) The entry point for all requests is nova platform compute service for VMs -api . This service accepts REST requests from users and other services. It first validates the user's authentication token via Keystone platform identity/authentication service before passing the request into the system. Crucially, nova-api is stateless, meaning scaling it is as simple as running multiple copies behind a Load Balancer. The decision-making heart of the cloud is nova platform compute service for VMs -scheduler . Its sole responsibility is to decide where a new virtual machine should be placed. It does not create the VM - or touch the hypervisor; it simply selects the most appropriate host from the pool of available resources and passes the message along. It achieves this through a sophisticated Filter-and-Weight algorithm. Finally, the nova platform compute service for VMs -conductor acts as a security guard for the database. In a cloud environment, compute nodes are considered \"untrusted\" because they run user workloads that could potentially be malicious. If a hacker were to escape a VM - and gain control of the compute node, we must ensure they cannot corrupt the entire cloud database. Therefore, compute nodes are not allowed to write to the database directly. Instead, they send a message to the Conductor requesting an update, and the Conductor performs the write operation only after validating the request. 2.2 The Node Components (Data Plane) On every hypervisor server, the nova platform compute service for VMs -compute service acts as the worker. It continually listens for instructions from the message queue (RabbitMQ). When it receives a command, such as \"Run Instance,\" it does not execute it blindly; it follows a rigorous process to ensure the VM - is built correctly on the physical hardware. The Driver Layer (Libvirt) nova-compute is designed to be hypervisor-agnostic. It does not speak directly to the kernel; instead, it uses a driver. In our Linux environment, it uses the Libvirt driver. When you ask for a VM - , nova-compute translates your request into a Libvirt XML file\u2014a precise recipe describing the VM - 's CPU, RAM, and Devices\u2014and passes it to the Libvirt daemon, which ultimately spawns the QEMU - Type 1 hypervisor for virtualization /KVM - Type 1 hypervisor process. The Resource Tracker Beyond creating VMs, nova-compute is responsible for auditing the physical server. It runs a periodic task (typically every 60 seconds) called the Resource Tracker . This task scans the available RAM, CPU cores, and Disk space on the host and compares it against the reserved resources. It then reports this \"Inventory\" back to the central database. This ensures that the Scheduler always possesses an accurate, up-to-date map of the cloud's capacity, preventing it from sending a VM - to a host that is already full. Section 2 Checkpoint Summary : Control Plane : nova-api (Entry), nova-scheduler (Decision), nova-conductor (DB Guard). Data Plane : nova-compute (Hypervisor Worker). Security : Compute nodes cannot talk directly to the DB; they go through Conductor. Reflection : Why is nova-api considered \"stateless\"? Why do we need a \"Conductor\" to protect the database? 3. The Scheduling Algorithm (The Decision Process) When a user requests a new VM - , the scheduler is faced with the task of choosing one single server out of potentially thousands. It solves this problem using a two-pass process: Filtering and Weighting. Figure 2: The Scheduling Funnel - Narrowing down 1000 hosts to the single best candidate 3.1 Pass 1: Filtering (Qualifying) The first pass is designed to remove any hosts that are incapable of running the instance. It works like a sieve. RamFilter : Checks if the host has enough free RAM to satisfy the requested flavor - template - image for quick deployment defining vCPUs, RAM, and disk . ComputeFilter : Ensures the host service is actually alive and reporting. AvailabilityZoneFilter : Ensures the VM - lands in the requested physical location. ImagePropertiesFilter : Checks for specific hardware requirements like GPUs or Secure Boot support. 3.2 Pass 2: Weighting (Ranking) Once the invalid hosts are removed, the second pass ranks the remaining candidates to find the \"best\" fit. The default RamWeigher checks the free RAM on each host. Stacking Strategy : Fills up one server completely before moving to the next. This saves power but creates hotspots. Spreading Strategy (Default): Places the VM - on the emptiest possible server to maximize performance and minimize the \"noisy neighbor\" effect. Section 3 Checkpoint Summary : Filtering : Removes invalid hosts (e.g., Not enough RAM). Weighting : Ranks valid hosts (e.g., Emptiest first). Goal : Select the single best host (Candidate) for the VM - . Reflection : What is the difference between \"Stacking\" and \"Spreading\" strategies? Which filter ensures a VM - lands on a host with a GPU? 4. The Instance Lifecycle (State Machine) A Virtual Machine goes through several status changes during its life. Understanding these transitions is critical for troubleshooting when things go wrong. The process begins in the BUILD state. Initially, the API has accepted the request, but the VM - does not exist yet; the Scheduler is still finding a home for it. The state remains BUILD while the Conductor waits for Neutron platform networking service to assign an IP address and Port (Networking phase). It then transitions to the Spawning phase, where nova-compute downloads the disk image from Glance platform image service . If all goes well, the status changes to ACTIVE , meaning the Hypervisor considers the VM - process to be running successfully. If a failure occurs at any point, the status will change to ERROR , and the administrator must check the nova-compute.log to determine the cause. Other states include SHELVED , where the VM - is written to disk and removed from RAM to save resources (a \"Deep Freeze\"), and RESCUE , where the VM - is booted from a special recovery image to repair a corrupted disk. Section 4 Checkpoint Summary : BUILD : Scheduling and Networking in progress. ACTIVE : VM - is running on the Hypervisor. ERROR : Something went wrong (Check logs). SHELVED : VM - offloaded to disk. Reflection : What happens during the \"Spawning\" phase? How does SHELVED differ from a simple Shutdown? 5. Operations Cookbook (CLI): Launching Nebula Inc. In Week 8, we established the digital foundation for Nebula Inc. We created the Project ( nebula_prod ), hired the User ( nebula_admin ), and wired the Office Network ( nebula_net ). However, the data center currently sits empty. To bring the company online, we must now define the virtual hardware standards (Flavors), issue security credentials (Keys & Groups), and finally press the \"Power On\" button for their first Web Server. Below are the commands to execute this activation. 5.1 Defining Flavors (Capacity) In a physical data center, you buy specific server models. In OpenStack platform , we abstract this capacity into what the platform calls Flavors (Instance Types in AWS/Azure). A Flavor - template - image for quick deployment defining vCPUs, RAM, and disk is a virtual hardware template - image for quick deployment that defines the resource limits (vCPU - , RAM, Disk). The Provider vs. Consumer Role : Public Cloud (AWS/Azure) : You are a Consumer . You cannot create new sizes; you can only Select from the menu Amazon provides ( t2.micro , m5.large ). Private Cloud (OpenStack platform ) : You are the Provider . It is your job to Create the menu that your users will select from. 1. Listing Existing Flavors (The Menu) Before creating new ones, check what is available. openstack flavor list 2. Creating a Custom Flavor - template - image for quick deployment defining vCPUs, RAM, and disk (The Chef) For \"Nebula Inc.\", we need a custom \"Micro\" size for cheap testing. We will name it m1.nebula_micro . Naming Convention Decoding : m1 : Generation/Class . (e.g., \"m\" for General Purpose, \"1\" for 1st Generation). This mirrors AWS naming (e.g., t2.micro = Burstable, 2nd Gen). nebula : Family . Identifies this as a custom flavor - template - image for quick deployment defining vCPUs, RAM, and disk for our organization. micro : Size . Indicates relative capacity (Micro < Small < Medium). openstack flavor create --id auto --ram 512 --disk 1 --vcpus 1 m1.nebula_micro Result : We have added a new item to the menu. Users can now select m1.nebula_micro when launching instances. 5.2 Securing Access (Keys & Groups) Security in the cloud is a two-layered approach. First, we must secure Identity (proving who you are) using Keypairs. Second, we must secure the Network (controlling traffic flow) using Security Groups. You cannot access a VM - unless both of these layers are correctly configured. 5.2.1 Keypairs (Login Access) Unlike traditional servers where you set a root password, Cloud images (AWS, Azure, OpenStack platform ) verify identity using Asymmetric Cryptography . This mechanism leverages a \"Lock and Key\" relationship to secure access. The Public Key acts as the \"Lock\"; you upload this to the cloud, and Nova platform compute service for VMs injects it into the VM - 's .ssh/authorized_keys file during boot. It is safe to share and visible to anyone. The Private Key acts as the unique \"Key\"; you keep this securely on your laptop and must never share it . When you attempt to SSH into the instance, the server sends a digital challenge encrypted with the Lock. Your laptop automatically uses your Private Key to decrypt this challenge. If the decryption is successful, the server grants access without ever requiring a password to be transmitted over the network. Generating a Keypair openstack keypair create nebula_key > nebula_key.pem chmod 600 nebula_key.pem Explanation : This command generates the pair. It stores the Public Key in the Nova platform compute service for VMs Database and writes the Private Key to nebula_key.pem on your disk. The chmod is critical; SSH will refuse to use a key if the file permissions are too open. 5.2.2 Security Groups (The Virtual Firewall) In traditional networking, firewalls are physical appliances sitting at the edge of the network. In Cloud Computing, we use Security Groups . A Security Group is a virtual firewall that is applied directly to the network interface (vNIC) of an instance, regardless of where it runs in the data center. Figure 3: Security Group Architecture - How the Open vSwitch Agent filters packets on the Hypervisor before they reach the VM - Concept (General Cloud) Security groups operate on specific principles: Stateful : If you allow a request out (e.g., download update), the return traffic is automatically allowed in . Allow-List : The default policy is \"Implicit Deny\". All traffic is blocked until you explicitly allow it. Dynamic : Rules are applied immediately to all running instances without rebooting. OpenStack platform Implementation When you create a rule, Neutron platform networking service communicates with the Open vSwitch (OVS) agent on the Compute Node. It translates your high-level rule (e.g., \"Allow Port 80\") into low-level OVS Flow Tables or iptables chains on the physical hypervisor. This ensures malicious traffic is dropped on the physical wire before it ever reaches your VM - , providing a robust first line of defense. CLI: Configuring the Firewall We must explicitly open ports for SSH and Web access. # Create the Container openstack security group create nebula_web_sg # Allow SSH (Port 22) - Administrative Access openstack security group rule create --proto tcp --dst-port 22 nebula_web_sg # Allow HTTP (Port 80) - Public Web Access openstack security group rule create --proto tcp --dst-port 80 nebula_web_sg Result : The OVS Agent on the compute node intercepts traffic to nebula_web_01 and filters it against these rules. 5.3 Launching Instances The server create command brings together the Flavor - template - image for quick deployment defining vCPUs, RAM, and disk , Image, Network, Key, and Security Group to instantiate a VM - . Boot Command openstack server create --flavor m1.nebula_micro \\ --image nebula_standard_os \\ --network nebula_net \\ --key-name nebula_key \\ --security-group nebula_web_sg \\ nebula_web_01 Explanation : --flavor : Defines the size. --image : Defines the software (OS). --network : Defines the wiring. Result : Triggers the entire scheduling and build process seen in Section 4. 5.4 Day 2 Operations (Debugging & Access) Floating IPs (Public Access) To access the VM - from the internet, map a public IP to it. openstack floating ip create public openstack server add floating ip nebula_web_01 172.24.4.10 Console Logs (Troubleshooting) If a VM - fails to become reachable (e.g., no network), check the boot logs. openstack console log show nebula_web_01 Explanation : Retrieves the kernel ring buffer (dmesg) and cloud-init output directly from the hypervisor. Use this to find kernel panics or DHCP failures. Section 5 Checkpoint Summary : Flavor - template - image for quick deployment defining vCPUs, RAM, and disk : Virtual hardware template - image for quick deployment (CPU/RAM). Provider defines, Consumer selects. Security Group : Stateful virtual firewall. \"Implicit Deny\" by default. Keypairs : SSH Keys for identity. Private Key never leaves your laptop. Floating IP : Assigns a public address to reach the VM - from outside. Reflection : Why must we use chmod 600 on the private key? How does an \"Allow-List\" firewall differ from a traditional \"Block-List\"? 6. Industry Comparison: The \"Polyglot\" Cloud Engineer A key goal of this course is to make you a Cloud Engineer , not just an OpenStack platform Administrator. The concepts you learned this week\u2014Flavors, Security Groups, and Booting Instances\u2014are universal. If you know how to launch a server in OpenStack platform , you already know 90% of how to do it in Amazon Web Services (AWS) or Microsoft Azure. 6.1 Concept Mapping Concept OpenStack platform Term AWS Term Azure Term Compute Provider Nova platform compute service for VMs EC2 (Service) Azure Compute ( Microsoft.Compute ) Size Template - image for quick deployment Flavor - template - image for quick deployment defining vCPUs, RAM, and disk (e.g., m1.small ) Instance Type (e.g., t2.micro ) VM - Size (e.g., Standard_B1s ) Firewall Security Group Security Group Network Security Group (NSG) Login Key Keypair Key Pair SSH Key Default User cirros , ubuntu ec2-user , ubuntu azureuser ### 6.2 CLI Rosetta Stone Below is the exact same \"Launch Instance\" workflow translated into the three major languages of the cloud. 1. Create a \"Flavor - template - image for quick deployment defining vCPUs, RAM, and disk \" (Size) OpenStack platform : openstack flavor list (Selects m1.small ) AWS : aws ec2 describe-instance-types (Selects t2.micro ) Azure : az vm list-sizes (Selects Standard_B1s ) 2. Create a Firewall OpenStack platform : openstack security group create web-sg AWS : aws ec2 create-security-group --group-name web-sg Azure : az network nsg create --name web-nsg 3. Launch the Instance (The \"Hello World\" of Cloud) Notice how similar the flags are across all three platforms. OpenStack platform (Nova platform compute service for VMs ) openstack server create --image ubuntu --flavor m1.small --key-name mykey --security-group web-sg my-server AWS (EC2) aws ec2 run-instances --image-id ami-12345 --instance-type t2.micro --key-name mykey --security-group-ids sg-12345 Azure (Compute) az vm create --image UbuntuLTS --size Standard_B1s --ssh-key-value @mykey.pub --nsg web-nsg --name my-server Section 6 Checkpoint Summary : Concepts are universal; only terms change (Flavor - template - image for quick deployment defining vCPUs, RAM, and disk -> Instance Type). Nova platform compute service for VMs = AWS EC2 = Azure Compute. Security Group is the standard term across OpenStack platform and AWS. Reflection : Why is it valuable to learn the underlying concept rather than just the tool command? How does rely on these standardized CLI commands? 7. Summary and Next Steps This week we peeled back the layers of the OpenStack platform cloud to reveal Nova platform compute service for VMs , the engine responsible for \"Computing.\" We moved from the high-level architecture of Cells and Schedulers down to the practical reality of launching instances via the CLI. You learned that a \"VM is not just a random occurrence but the result of a coordinated effort between the API (Entry), Scheduler (Brain), Conductor (Guard), and Compute (Worker). We also took our first steps in \"Nebula Inc.\" by defining the hardware standards (Flavors) and security policies (Groups) necessary to bring the company online. By mapping these OpenStack platform concepts to their AWS and Azure equivalents, you are effectively learning three clouds simultaneously. Preparing for Week 10 Next week, we tackle Storage and Persistence . A web server is useless if it loses all its data when it reboots. We will explore Cinder platform block storage service (Block Storage) to give our instances persistent hard drives. Checklist: Ensure you can launch an instance from the CLI without looking at the manual. Verify you can SSH into your instance using your keypair. Review the \"Instance Lifecycle\" states (Build -> Active). 8. Additional Resources OpenStack platform (The Platform) Nova Architecture Guide Filter Scheduler Logic AWS (The Comparison) Amazon EC2 Documentation Launch an Instance (Tutorial) Azure (The Comparison) Azure Virtual Machines Quickstart: Create a Linux VM 9. Lab Exercises Lab 1: Launch Goal : Launching your first Cloud Instances. Lab 2: Lifecycle Goal : Managing instance lifecycle (Suspend, Resume, Resize). Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 8: Cloud Foundation Course Index Next: Week 10: Persistence\n\n--- WEEK 10 NOTES ---\nStorage and Persistence (Cinder platform block storage service ) Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Storage and Persistence ( Cinder OpenStack block storage service platform block storage service ) Estimated Reading Time : 25 Minutes [!TIP] How to succeed in this week : Understand the critical difference between ephemeral and persistent storage\u2014VMs are temporary, but data should last forever. Cinder OpenStack block storage service platform block storage service volumes are like USB drives for your cloud instances. 1. The Hierarchy of Cloud Storage In Week 9, we launched compute instances, but if we terminate them, the data on their root disk is deleted. This is known as Ephemeral Storage . For a business like Nebula Inc. , we need a place to store our critical customer database that survives even if the Virtual Machine is destroyed. This is the domain of Persistent Block Storage (Cinder platform block storage service ). Industry Context : OpenStack platform Cinder platform block storage service performs the exact same role as AWS Elastic Block Store (EBS) or Azure Managed Disks . It provides a \"raw hard drive\" over the network that you can plug into your instance. 1.1 Ephemeral vs. Persistent Storage It is crucial to distinguish between the two primary ephemeral and persistent storage models in cloud architectures. Ephemeral Storage (Nova platform compute service for VMs ) is strictly tied to the lifecycle of the Compute Instance. It functions effectively as a local scratch disk, optimized for operating system caches and temporary file processing. However, if the instance is terminated or the underlying hypervisor fails, this data is irrevocably lost. In contrast, Persistent Storage (Cinder platform block storage service ) operates as an independent capability. A Cinder platform block storage service volume operates similarly to a physical USB drive or a SAN LUN; it exists independently of any single server. This durability ensures that critical datasets, such as customer databases or file servers, can survive compute failures and can be detached and re-attached to healthy instances during recovery operations. Section 1 Checkpoint Summary : Ephemeral Storage (Nova platform compute service for VMs ): Temporary, fast, dies with the VM - . Use for OS / Cache. Persistent Storage (Cinder platform block storage service ): Durable, independent, survives VM - deletion. Use for Databases / Critical Data. Analogy : Ephemeral is RAM/Swap; Persistent is the Hard Drive. Reflection : Why shouldn't you store your customer MySQL database on the Nova platform compute service for VMs Ephemeral disk? If you delete a Cinder platform block storage service Volume, is the data recoverable? (Hint: Only if you have a Backup). 2. Block Storage Architecture (Cinder platform block storage service ) Cinder platform block storage service is the software that orchestrates storage devices; it does not usually store the data itself. Instead, it acts as a translation layer between the user and the physical storage hardware. 2.1 The Driver Model Just as Nova platform compute service for VMs utilizes virtualization drivers to interact with various CPU architectures, Cinder platform block storage service employs a Volume Driver architecture to communicate with diverse storage backends. Figure 1: Cinder platform block storage service Architecture - The Cinder platform block storage service Scheduler selects the backend, and the Volume Driver translates API calls into storage commands Laboratory : The LVM Driver manages local logical volumes on a standard Linux server. Enterprise : Customized drivers for Dell EMC , NetApp , or HPE arrays translate API calls into proprietary storage commands. Scale-Out : The Ceph Driver allows Cinder platform block storage service to provision resources from a distributed, software-defined storage cluster. When a user executes a creation command, Cinder platform block storage service identifies the correct driver and signals the backend hardware to provision the requested Logical Unit Number (LUN). 2.2 The Attachment Process (iSCSI/RBD) The mechanism for attaching a volume to an instance involves a coordinated handshake between services. Figure 2: The Attachment Handshake - How Nova platform compute service for VMs and Cinder platform block storage service coordinate to plug a remote disk into a running VM - Provision : Cinder platform block storage service provisions the logical volume on the specific storage array. Request : Nova platform compute service for VMs requests the connection parameters\u2014typically the Target IQN (iSCSI) or Monitor IP (Ceph)\u2014from Cinder platform block storage service . Connect : Nova platform compute service for VMs configures the Hypervisor (KVM - Type 1 hypervisor ) to initiate a storage login session using the appropriate protocol. Device Map : The local operating system kernel on the hypervisor detects the new block device (e.g., /dev/sdb ) and passes it through to the virtual machine, which perceives it as a locally attached hard drive. 2.3 Deep Dive: Storage Backends Cinder platform block storage service operates as an abstraction layer, capable of interfacing with a diverse array of storage backends. In private cloud environments, the two most prevalent technologies serve as excellent examples of this flexibility: the Network File System (NFS) and the Ceph distributed storage cluster. 2.3.1 NFS (Network File System) The Network File System (NFS) represents the simpler deployment model, often utilized in smaller environments or laboratories. In this architecture, Cinder platform block storage service acts as a client that mounts a remote directory from an existing NAS appliance or Linux server (e.g., 192.168.1.5:/toptier ). When a user requests a new volume, the Cinder platform block storage service Volume service generates a large file, typically in the QCOW2 - Type 1 hypervisor for virtualization Copy-On-Write disk image format format, within this mounted directory. While this approach is notably easy to implement\u2014requiring only a standard Linux server or a commercial NAS like Synology\u2014it suffers from scalability limitations. The performance of the entire cloud storage pool is often constrained by the throughput of the single network link connecting the Controller to the NAS, creating a significant bottleneck and a single point of failure. 2.3.2 Ceph (The Gold Standard) In contrast, Ceph represents the industry standard for production-grade OpenStack platform deployments. As a software-defined storage solution, Ceph eliminates the need for a central storage controller. Instead, it aggregates storage capacity from hundreds of individual hard drives distributed across many physical servers, unifying them into a massive, scalable \"Pool.\" The integration between Cinder platform block storage service and Ceph is facilitated by the librbd library, which allows Cinder platform block storage service to manage reliable RADOS Block Devices (RBD). Ceph distinguishes itself through its self-healing capabilities and advanced snapshotting mechanism. When data is written to a Ceph-backed volume, it is split into 4MB objects and scattered deterministically across the cluster. If a physical drive fails, the cluster automatically detects the missing objects and replicates them from surviving redundant copies, effectively healing the system without human intervention. Furthermore, because Ceph manages data as discrete objects, it can create instantaneous snapshots using a Copy-on-Write mechanism. This allows administrators to generate thousands of recovery points without incurring the performance penalties associated with traditional storage arrays. Implementation Note : In production, Ceph is the preferred backend because it decouples storage from compute hardware entirely, allowing indefinite scaling. 2.3.3 Configuring Cinder platform block storage service with Ceph Note : This assumes a Ceph cluster is already running. To learn how to build one from scratch, see the Optional Ceph Setup Guide. To configure OpenStack platform Cinder platform block storage service to use a Ceph cluster as its backend, the administrator must edit the cinder.conf file on the Controller node. The process involves three key steps: installing the client libraries, authenticating, and defining the driver. 1. Install Ceph Client: The Cinder platform block storage service service requires the python libraries to communicate with the Ceph public network. sudo apt install python3-rbd ceph-common 2. Authentication (Keyring): OpenStack platform acts as a client \"user\" to the Ceph cluster. You must copy the authentication keyring from the Ceph Monitor node to the Cinder platform block storage service node. # On Cinder platform block storage service Node scp ceph-node:/etc/ceph/ceph.client.cinder.keyring /etc/ceph/ chmod 0640 /etc/ceph/ceph.client.cinder.keyring 3. Driver Configuration ( /etc/cinder/cinder.conf ): Define a new backend section (e.g., [ceph] ) and reference it in the enabled_backends list. [DEFAULT] enabled_backends = ceph [ceph] volume_driver = cinder platform block storage service .volume.drivers.rbd.RBDDriver volume_backend_name = ceph rbd_pool = volumes rbd_ceph_conf = /etc/ceph/ceph.conf rbd_flatten_volume_from_snapshot = false rbd_max_clone_depth = 5 rbd_store_chunk_size = 4 rados_connect_timeout = -1 glance_api_version = 2 2.3.4 Configuring Cinder platform block storage service with NFS For smaller deployments or lab environments, NFS is a common backend. It requires a dedicated text file to list the shares and a specific driver configuration. 1. Create Shares File: Create a text file (e.g., /etc/cinder/nfs_shares ) and list your NFS exports, one per line. 192.168.1.100:/var/nfs/cinder 192.168.1.100:/var/nfs/backup 2. Set Permissions: Ensure the Cinder platform block storage service user can read this file. chown root:cinder /etc/cinder/nfs_shares chmod 0640 /etc/cinder/nfs_shares 3. Driver Configuration ( /etc/cinder/cinder.conf ): [nfs] volume_driver = cinder platform block storage service .volume.drivers.nfs.NfsDriver volume_backend_name = nfs nfs_shares_config = /etc/cinder platform block storage service /nfs_shares nfs_mount_point_base = /var/lib/cinder platform block storage service /nfs Section 2 Checkpoint Summary : Cinder platform block storage service : Manages block storage (Creating/Attaching volumes). Backends : Connects to LVM (Local), NFS (File), or Ceph (Distributed). Ceph : The Gold Standard for OpenStack platform . Self-healing, scalable, compliant. Reflection : Why is Ceph preferred over NFS for large clouds? (Hint: Single Point of Failure). What happens to a generic \"File\" on an NFS share when Cinder platform block storage service creates a volume? (It becomes a .qcow2 or .raw disk image). 3. Data Safety Strategies In enterprise cloud environments, ensuring data durability and availability against hardware failure is a critical architectural requirement. While Cinder platform block storage service provides robust block storage, the physical media underlying these volumes remains susceptible to corruption or catastrophic failure. To mitigate these risks and ensure business continuity, OpenStack platform implements two distinct data protection mechanisms: Snapshotting for point-in-time operational recovery, and Backups for comprehensive disaster recovery. 3.1 Snapshots (The Time Machine) A Snapshot - represents a point-in-time copy of a specific volume using a \"Copy-on-Write\" (Redirect on Write) mechanism. This technique ensures that the snapshot - is created nearly instantly, as it relies on the existing data blocks rather than duplicating the entire drive volume. Snapshots are invaluable for functional recovery scenarios, such as capturing the state of a database before a major upgrade; if the upgrade fails, the administrator can rollback instantly. However, it is critical to note that snapshots typically reside on the same physical hardware as the source volume. Therefore, if the underlying storage array experiences a catastrophic failure, both the active volume and its snapshots will be lost. 3.2 Backups (The Disaster Plan) To mitigate the risk of physical hardware failure, Backups provide a complete disaster recovery solution. Figure 3: Block vs Object Storage - Cinder platform block storage service Backups move data from expensive, fast Block Storage to cheap, durable Object Storage (Swift/S3) A backup involves reading the full content of a block volume and transferring it to a separate, physically isolated system\u2014typically an Object Storage service like Swift or Amazon S3. Although this process is slower due to network transfer requirements, it ensures data survivability. If the primary SAN or Ceph cluster were to be destroyed by fire or malfunction, the data could still be restored from the backup repository located in a different rack or data center. 3.3 Architecting Redundancy: Public vs. Private In a Public Cloud (AWS/Azure), achieving higher redundancy is often as simple as selecting a premium tier in a dropdown menu. However, in a Private Cloud environment using OpenStack platform , you are the architect responsible for building these layers yourself. Understanding how standard cloud redundancy levels map to OpenStack platform implementation is crucial for designing robust infrastructure. Level 1: Local Redundancy (LRS) The foundational level of data safety is Local Redundancy, known as LRS in Azure or EBS in AWS. This concept ensures that data survives disk failures within a single rack or datacenter. In an OpenStack platform private cloud, this is achieved natively through Ceph . By default, Ceph creates a \"Replica=3\" pool, which automatically stores three copies of every object on different physical Object Storage Daemons (OSDs). If a physical drive fails, the system self-heals by replicating the data from the surviving copies to a new drive, mirroring the durability guarantees of public cloud LRS. Level 2: Zonal Redundancy (ZRS) The next tier is Zonal Redundancy (ZRS), designed to ensure data survives the total destruction of a building due to fire or power loss. Public clouds implement this by replicating data across distinct Availability Zones\u2014separate facilities with independent power and cooling. In OpenStack platform , you replicate this architecture using Cinder platform block storage service Availability Zones . By modifying cinder.conf , an administrator can define logical zones (e.g., zone-A , zone-B ) and map them to specific storage racks or entirely separate Ceph clusters. This requires users to consciously select a zone when provisioning a volume, ensuring their application architecture can withstand a facility-level failure. Level 3: Geo Redundancy (GRS) The highest level of protection is Geo Redundancy (GRS), which ensures survival against regional catastrophes, such as major natural disasters. While public clouds handle this via asynchronous replication between regions (e.g., North Europe to West Europe), a private cloud architect typically implements this using Cinder platform block storage service Backup . By configuring Cinder platform block storage service to send volume backups to a remote Swift Object Storage cluster located in a different city, you guarantee that critical data can be restored even if the primary datacenter is lost. More advanced (and expensive) setups can also utilize driver-level volume replication for real-time Active/Passive disaster recovery. Section 3 Checkpoint Summary : Snapshot - : Quick, local, dependent on source. Use for \"Undo\" before changes. Backup : Slow, remote, independent. Use for Disaster Recovery (Fire/Flood). Redundancy : LRS (Disk Fail), ZRS (Rack/Building Fail), GRS (City Fail). Reflection : Why isn't a Snapshot - considered a true Backup? Which Cinder platform block storage service feature would you use to protect against a data center power outage? (Cinder platform block storage service Backup / Replication). 4. Operations Cookbook (Nebula Inc.) We continue our role as the Cloud Engineer for Nebula Inc. We have a Web Server ( nebula_web_01 ). Now we need a Database Server, but we need to ensure its data is safe. 4.1 Creating a Volume We begin by provisioning a specific persistent volume for our database. This is analogous to purchasing a physical hard drive; initially, it exists as an unattached operational resource within the storage inventory. openstack volume create --size 1 nebula_db_vol 4.2 Attaching the Volume Once created, the volume must be physically connected to the compute instance. This command mimics the act of plugging a USB drive or SAS cable into a running server. openstack server add volume nebula_web_01 nebula_db_vol Verification : bash openstack volume list # Status should be \"in-use\" 4.3 Formatting and Mounting (Guest OS ) It is important to remember that Cinder platform block storage service delivers a raw block device (e.g., /dev/vdb ) without any file system. The administrator must log into the Guest OS to format the disk and mount it for use effectively transferring responsibility from the \"Cloud Provider\" to the \"OS Administrator\". # SSH into the VM - ssh cirros@<FLOATING_IP> # Check for new disk (usually /dev/vdb) lsblk # Create Config Files (Simulating a DB) # CirrOS is too small for mkfs, so we will write directly sudo sh -c 'echo \"Customer Data 123\" > /dev/vdb' 4.4 Snapshotting Before executing any destructive changes or updates to the database, standard procedure dictates creating a snapshot - to preserve the current state. openstack volume snapshot create --volume nebula_db_vol nebula_db_snap_v1 Section 4 Checkpoint Summary : Process : Create Volume -> Attach to VM - -> Format (mkfs) -> Mount. Guest Responsibility : The Cloud Provider connects the wire; YOU must format the disk. Persistence : Data survives detach/reattach and VM - deletion. Reflection : Why doesn't the volume show up automatically in /mnt when you attach it? What Linux command lists all block devices? ( lsblk ). 5. Industry Comparison: Storage Feature OpenStack platform Cinder platform block storage service AWS Azure Service Name Cinder platform block storage service EBS (Elastic Block Store) Azure Managed Disks Resource Volume EBS Volume Managed Disk Backup Target Swift (Object Storage) S3 Azure Blob Storage Connection iSCSI / Ceph (RBD) NVMe / EBS Direct VHD / Blob Snapshot - Type Copy-on-Write Incremental Incremental --- 7. Summary and Next Steps This week we ensured that Nebula Inc. doesn't lose data. By implementing Cinder platform block storage service backed by Ceph , we provided a robust, persistent storage layer for our instances. We learned the difference between the \"Scratchpad\" (Ephemeral) and the \"Vault\" (Persistent). We also explored the critical difference between a convenience Snapshot - and a disaster-proof Backup. Preparing for Week 11 Next week, we stop clicking buttons manually. We will introduce Automation and APIs . We will learn how to deploy this entire infrastructure using code (Bash/Python) and configuration scripts (Cloud-Init), moving from \"Pets\" to \"Cattle\" effectively. Checklist: Can you differentiate between nova-compute storage and cinder-volume storage? Do you understand why we need to format a volume after attaching it? Review your Linux command line skills (loops and variables) for next week. 8. Additional Resources OpenStack platform Cinder platform block storage service Docs : docs.openstack.org/cinder/latest/ AWS EBS Documentation : aws.amazon.com/ebs/ Azure Managed Disks : azure.microsoft.com/en-us/products/managed-disks/ 9. Lab Exercises Now that you understand the theory of Persistent Storage, Cinder platform block storage service Backends, and Data Redundancy, it is time to build it. Week 10 Lab 1: Persistent Cloud Storage Goal : Create a volume, attach it to an instance, format it, write data, and simulate a migration to a second instance to prove persistence. Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 9: Compute Ops Course Index Next: Week 11: Automation\n\n--- WEEK 11 NOTES ---\nAutomation and Cloud API Student Notes Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Topic : Automation and Cloud API Application Programming Interface Estimated Reading Time : 30 Minutes [!TIP] How to succeed in this week : Automation is what separates a cloud engineer from a sysadmin. Focus on understanding APIs and \u2014clicking buttons doesn't scale, but code does. Welcome to DevOps Clicking GUI buttons is fine for 1 server. It fails for 100 servers. This creates a scalability bottleneck that manual administration simply cannot overcome. This challenge gave birth to DevOps , a methodology that unifies software development (Dev) and IT operations (Ops). At its core, DevOps is about automation, collaboration, and speed. It shifts the focus from manually configuring servers to writing code that defines them\u2014a concept known as ( IaC Infrastructure as Code ) . In a DevOps model, infrastructure is versioned, tested, and deployed just like software applications, eliminating the \"it works on my machine\" problem. This week, we learn to control the cloud using code. By mastering tools like the OpenStack Open-source cloud computing platform platform CLI Command Line Interface , Cloud-Init, Heat, and Ansible Automation tool for configuration management , you are learning the foundational skills required for a modern DevOps or Site Reliability Engineering (SRE) role. You will transition from clicking buttons in a dashboard to designing self-healing, automated systems that can scale to thousands of instances with a single command. 1. Advanced CLI Techniques The openstack command is powerful, but by default, it formats output as ASCII tables designed for human readability. While this is helpful for interactive use, it poses a significant challenge for automation scripts, which struggle to parse table borders and varying whitespace. To build robust tools, we must use machine-readable output formats. 1.1 Formatting Output The CLI natively supports JSON output, which provides a structured and predictable data format that scripting languages can easily parse. By appending --format json to any command, we strip away the visual formatting and receive raw data objects. # Standard Table (Human Readable) openstack server list # JSON Output (Machine Readable) openstack server list --format json Command Analysis : * --format json : Forces the CLI to output raw JSON data instead of an ASCII table. This is essential for piping data into tools like jq or Python scripts. 1.2 Parsing with jq jq is a lightweight command-line JSON processor that allows us to filter, slice, and map JSON data directly in the terminal. It acts as a bridge between the verbose API output and the specific strings (like UUIDs) needed for subsequent commands. # Basic: Get the ID of the network named 'private-net' NET_ID = $(openstack network show private-net -f json | jq -r .id ) echo $NET_ID # We pipe the server list into jq, filter for specific status, and extract the IDs. ACTIVE_IDS = $(openstack server list -f json | jq -r '.[] | select(.Status==\"ACTIVE\") | .ID' ) Code Analysis : * $() : Command substitution; runs the inner command and assigns the output to the variable. * | : The pipe operator passes the output of the openstack platform command directly to jq . * jq -r .id : Filters the JSON to find the key \"id\". The -r (raw) flag removes quotation marks, leaving just the UUID. * select() : A powerful jq function that acts like a WHERE clause in SQL, allowing you to filter lists based on conditions. In the example above, the -f json flag forces OpenStack platform to output JSON. We then pipe this valid JSON to jq . The -r flag is crucial as it outputs \"raw\" strings without quotation marks, making the output ready for variable assignment. We also use the select function to filter the array, ignoring any servers that are building, paused, or shut down. 1.3 Architectural Insight: Golden Images vs. Post-Boot Config The OpenStack platform for Architects book details two competing strategies for deploying applications: Golden Images and Post-Boot Configuration . Figure 1: Pet vs Cattle - Manual \"Pet\" servers require constant care, while Automated \"Cattle\" servers are replaceable and identical Golden Images (Mutable/Baked) involve installing all application dependencies\u2014such as Apache, PHP, and custom code\u2014into the Virtual Machine image before it is ever launched. This is typically done using tools like Packer. The primary advantage is speed; since the software is pre-installed, the VM - is ready almost instantly upon boot. However, this method suffers from \"Image Sprawl,\" where every minor code change requires building and uploading a new multi-gigabyte image to Glance platform image service , consuming storage and bandwidth. Post-Boot Configuration (Immutable/Runtime) takes a different approach. You launch a generic, \"Vanilla\" operating system image (like Ubuntu Cloud Image) and use automation tools to install software after the instance boots. While this results in a slower initial startup time as packages are downloaded and installed, it offers superior flexibility. A single small base image can serve thousands of different purposes. Modern cloud architecture typically favors a Hybrid Approach , using a base image for the OS and tools like Ansible for the final application configuration. Section 1 Checkpoint Summary : JSON is the lingua franca of Cloud APIs, providing a structured format that is difficult for humans to read but trivial for machines to parse. To work effectively with this data in a shell environment, jq is an essential tool for extracting specific fields like resource IDs. Before writing any automation script, a cloud engineer must master the ability to retrieve clean, predictable data programmatically rather than relying on brittle text parsing methods like grep . Reflection : Consider why grep is a poor choice for parsing JSON data; a simple change in line breaks or spacing could break a script, whereas jq parses the data structure itself. Also, recall that the -r flag in jq strips quotes from the output, which is essentially when passing values to other CLI commands. Resources : jq Tutorial 2. Cloud-Init: The Standard for Bootstrapping When a virtual machine boots in the cloud, it starts as a generic \"clone \" of an Operating System. It has no idea who it is, what its hostname should be, or what software it needs. Cloud-Init is the industry-standard multi-distribution package that solves this identity crisis. It runs during the initial boot process to identify the environment and apply unique configurations. Figure 2: Cloud-Init Workflow - How the script is injected from the Metadata Service and executed during the first boot 2.1 How it Works: The Datasource The magic of Cloud-Init relies on a Datasource . On boot, Cloud-Init acts like a detective, probing the network to find out where it is running. In OpenStack platform (and AWS), it typically queries the Metadata Service at the \"Magic IP\" 169.254.169.254 . If it receives a response, it pulls down a JSON payload containing the instance's Hostname, SSH Keys, and the User Data provided by the operator. 2.2 Execution Stages Cloud-Init does not run as a single script; it executes in distinct stages throughout the boot process to ensure dependencies are met: 1. Generator : Determines if cloud-init should run at all. 2. Local (Init) : Finds the datasource and applies networking. This is critical because without networking, it cannot fetch further data. 3. Network (Config) : Runs after networking is up. Disk formatting, mount points, and SSH key injection happen here. 4. Final : This is where User Data scripts run (installing packages, running commands). This ensures the system is fully online before attempting to install software. 2.3 The Cloud-Config Format While User Data can be a simple Bash script, the preferred format is Cloud-Config . This is a declarative YAML syntax that abstractly defines what you want, rather than how to do it. To use this format, the input string must begin with the #cloud-config directive. #cloud-config hostname : web-01 packages : - apache2 - htop runcmd : - systemctl start apache2 - echo \"<h1>Deployed via Automation</h1>\" > /var/www/html/index.html Config Analysis : * #cloud-config : The required header telling Cloud-Init this is declarative YAML. * packages : A list of software to install via the OS package manager ( apt , yum ). * runcmd : A list of shell commands to execute after packages are installed. This is often used to start services or configure files. 2.4 Common Patterns (The Cookbook) Writing User Data requires understanding common patterns. Below are standard recipes frequently used in production. Pattern 1: The Web Server This pattern installs a web server, writes a custom index file, and ensures the service is running. #cloud-config packages : - nginx write_files : - content : | <h1>Welcome to Cloud</h1> path : /var/www/html/index.html runcmd : - systemctl restart nginx Pattern Analysis : * write_files : Creates use-case specific configuration files. The content block allows multiline text. * runcmd : Restarts the service to ensure the new configuration is applied. Pattern 2: The User Creator This pattern creates a new user account, grants it sudo privileges without a password requirement, and injects an SSH public key for secure access. #cloud-config users : - name : student groups : sudo shell : /bin/bash sudo : [ 'ALL=(ALL) NOPASSWD:ALL' ] ssh_authorized_keys : - ssh-rsa AAAAB3Nza... Pattern Analysis : * users : A dedicated module for user management. * sudo : Grants password-less root access, critical for automated management tools like Ansible. Pattern 3: The Update This pattern instructs the system to upgrade all installed packages on boot. Use this cautiously, as it significantly increases the boot time. #cloud-config package_upgrade : true Pattern Analysis : * package_upgrade: true : Forces an apt-get upgrade or yum update on first boot. While secure, it adds significant time to the boot process. 2.5 Using it in CLI To inject this configuration, you save the YAML to a local file (e.g., setup.yaml ) and pass it to the compute API during the server creation process. openstack server create \\ --flavor m1.tiny \\ --image ubuntu \\ --user-data setup.yaml \\ my-automated-server Command Analysis : * --user-data setup.yaml : Injects the contents of the file setup.yaml into the instance's metadata service. Cloud-Init reads this file upon first boot. 2.6 Troubleshooting (When things go wrong) A common mistake is assuming that if a server boots, the automation worked. If your script fails (e.g., a syntax error in YAML), the server will still boot, but your app won't be there. To debug this, you must SSH into the server and check the logs: /var/log/cloud-init.log : The high-level log of what cloud-init attempted to do. /var/log/cloud-init-output.log : The raw stdout/stderr of your scripts. If your apt-get install failed, the error message will be here. Section 2 Checkpoint Summary : Cloud-Init is the bridge between a generic OS image and a functional server. It relies on a Datasource (Metadata Service) to fetch configuration. It executes in strict Stages (Init -> Config -> Final) to ensure the network is ready before attempting to install software. Debugging automation failures requires inspecting the logs inside the VM - , as errors here rarely stop the instance from booting. Reflection : Why is the \"Magic IP\" ( 169.254.169.254 ) accessible from inside the VM - without any internet access? (Hint: It is a Link-Local address routed explicitly by the Hypervisor/Neutron platform networking service ). Resources : Cloud-Init Documentation 3. Automating with Scripts Now that we have the tools (CLI, jq, Cloud-Init), let's put them together. The most basic form of automation is Scripting . 3.1 The \"Bash Loop\" (Imperative) Imagine a scenario where you need to provision a cluster of five servers for a Load Balancing laboratory. Doing this manually is tedious and error-prone. A simple loop can automate the process effectively. #!/bin/bash echo \"Deploying Cluster...\" for i in { 1..5 } ; do echo \"Launching web- $i ...\" openstack server create \\ --flavor m1.tiny \\ --image cirros \\ --network private-net \\ web- $i done Script Analysis : * for i in {1..5} : Creates a loop that runs 5 times, with variable $i set to 1, 2, 3, 4, 5. * web-$i : Dynamically names the servers (web-1, web-2...) using the variable. * --network private-net : Ensures all servers attach to the same internal network. 3.2 Python Automation (The SDK) While Bash scripts are useful for quick tasks, they often become unmaintainable \"spaghetti code\" when applied to complex systems. For professional cloud engineering, the OpenStack platform SDK (Python) provides a robust alternative. Why Python? Python offers several advantages over shell scripting. First, Error Handling is handled gracefully through try/except blocks, preventing the script from crashing unexpectedly. Second, Python's native Data Structures , such as Dictionaries and Lists, are far easier to manipulate than parsing string output from a CLI commands. Finally, the logic required for Idempotency \u2014checking if a resource exists before attempting to create it\u2014is significantly cleaner to implement. 3.2.1 Authentication (The clouds.yaml ) Hardcoding passwords into scripts is a major security risk. Instead, OpenStack platform uses a standardized configuration file named clouds.yaml to decouple credentials from code. When you run a script, the SDK searches for this file in a specific order of precedence: Current Directory : Checks ./clouds.yaml (Good for project-specific configs). User Config : Checks ~/.config/openstack/clouds.yaml (The standard location for your personal credentials). System Config : Checks /etc/openstack/clouds.yaml (Global settings for all users). This allowing you to share your Python script with a colleague without accidentally sharing your password\u2014they simply use their own clouds.yaml . Example Content ( clouds.yaml ): clouds : openstack platform : auth : auth_url : \"http://10.0.0.10:5000/v3\" username : \"admin\" password : \"secret_password\" project_name : \"admin\" domain_name : \"Default\" region_name : \"RegionOne\" interface : \"public\" identity_api_version : 3 Explanation : * clouds : The top-level key containing all cloud definitions. * openstack : The specific profile name . In Python, we select this with cloud='openstack' . * auth_url : The Keystone platform identity/authentication service API endpoint. The SDK sends credentials here to get a token. Connecting in Python: from openstack platform import connection # Connect using the 'openstack platform ' profile defined above conn = connection . from_config ( cloud = 'openstack platform ' ) 3.2.2 Reading Resources (Listing Servers) The SDK returns Objects , not text. This means you can access properties like .id or .status directly without complex parsing. print ( \"Listing Servers:\" ) servers = conn . compute . servers () for server in servers : print ( f \"ID: { server . id } | Name: { server . name } | Status: { server . status } \" ) Code Analysis : * conn.compute.servers() : Returns a \"generator\" (an iterable list) of Server objects. * server.name : We access the data using dot-notation, which is type-safe and cleaner than grep . 3.2.3 Creating Resources (The Clean Way) Creating a server in Python allows us to wrap the logic in a Try/Except block to handle failures (like Quota errors) gracefully. try : print ( \"Creating Server...\" ) server = conn . compute . create_server ( name = \"web-python-01\" , image_id = \"cirros-id-here\" , flavor_id = \"m1.tiny\" , networks = [{ \"uuid\" : \"private-net-id\" }] ) # Wait for it to be ready conn . compute . wait_for_server ( server ) print ( f \"Created Server: { server . name } \" ) except Exception as e : print ( f \"Failed to create server: { e } \" ) Code Analysis : * create_server() : Accepts arguments as standard Python types (Strings, Lists). * wait_for_server() : A helper function that pauses the script until the server enters the ACTIVE state, replacing manual sleep loops. * try/except : If the cloud is full or the network ID is wrong, the script captures the error and prints a friendly message instead of crashing with a stack trace. 4. : Heat vs Terraform The distinction between \"Imperative\" scripts (Bash) and \"Declarative\" (IaC ) is fundamental. Imperative tools describe how to achieve a task step-by-step, whereas Declarative tools describe the desired end state , leaving the \"how\" to the engine. 4.1 The Two Giants Two primary tools dominate this landscape. Heat is the OpenStack platform Native orchestration engine. It is built directly into the platform, requires no external installation, and uses YAML templates. It is the ideal choice for pure OpenStack platform environments where external tool dependencies are undesirable. Terraform , created by HashiCorp, is the Industry Standard for multi-cloud provisioning. It uses the HashiCorp Configuration Language (HCL) and supports AWS, Azure, Google Cloud, and OpenStack platform simultaneously, making it the dominant skill in the broader job market. 4.2 Syntax Comparison (Creating a Server) Option A: OpenStack platform Heat (HOT) resources : my_server : type : OS::Nova platform compute service for VMs ::Server properties : image : ubuntu flavor - template - image for quick deployment defining vCPUs, RAM, and disk : m1.small Option B: Terraform (HCL) image_name = \"ubuntu\" flavor_name = \"m1.small\" } Comparison : * Heat : Uses type: OS::Nova::Server and nested properties. * Terraform : Uses resource \"type\" \"name\" and = assignment syntax. Both achieve the exact same result. Note : In this course, we focus on Heat because it requires no external setup and allows you to understand the underlying OpenStack platform resource model directly. However, in a multi-cloud professional environment, Terraform is the tool you will most likely encounter. Section 4 Checkpoint Summary : We have moved from Imperative scripts, where we define strict procedural steps, to Declarative IaC , where we define the target architecture. A critical property of these modern tools is Idempotency \u2014the ability to execute the same script multiple times without causing errors or duplicating resources. If the resource already exists in the desired state, the tool simply does nothing. Reflection : Consider why a company using multiple cloud providers (e.g., AWS and on-prem OpenStack platform ) would prefer Terraform over Heat. Also, think about the consequences of removing a resource definition from a Terraform file or Heat template - image for quick deployment ; unlike a script which simply stops running, IaC tools will actively destroy the resource to ensure the real world matches your definition. 5. Orchestration with Heat (The Template - image for quick deployment Engine) Orchestration goes beyond simple resource creation; it manages the dependencies and lifecycle of complex applications. In OpenStack platform , the native Orchestration engine is Heat . 5.1 Anatomy of a Template - image for quick deployment Heat uses YAML templates known as HOT (Heat Orchestration Templates). Every template - image for quick deployment follows a standard skeleton: Version : heat_template_version: 2018-08-31 tells Heat which features are available. Parameters : Inputs provided by the user at runtime (e.g., SSH Key Name, Server Flavor - template - image for quick deployment defining vCPUs, RAM, and disk ). Resources : The actual infrastructure to build (VMs, Networks, Volumes). Outputs : Information returned to the user after deployment (e.g., The Website URL). heat_template_version : 2018-08-31 description : A skeleton template - image for quick deployment parameters : # Inputs defined here resources : # Infrastructure defined here outputs : # Return values defined here Structure Analysis : * Version : Always required. Defines the syntax version (HOT 2018-08-31 is standard for Queens/Rocky releases). * Parameters : Variables passed in (Input). * Outputs : Variables passed out (Return values). 5.2 Building Blocks (Primitives) Rather than writing a massive script immediately, let's look at how to create individual components. Creating a Network resources : my_private_net : type : OS::Neutron platform networking service ::Net properties : name : deep-dive-net Resource Analysis : * resources : The top-level keyword indicating the start of the infrastructure definition block. * my_private_net : The Logical ID (Variable Name) used to reference this resource elsewhere in the template - image for quick deployment . * type : The specific OpenStack platform resource class (e.g., OS::Neutron::Net ). * properties : Configuration specific to that resource (like the network name). Creating a Security Group resources : my_security_group : type : OS::Neutron platform networking service ::SecurityGroup properties : rules : - protocol : tcp port_range_min : 80 port_range_max : 80 remote_ip_prefix : 0.0.0.0/0 Resource Analysis : * rules : OpenStack platform Security Groups are Default Deny . No traffic is allowed unless explicitly permitted here. * protocol : The definition (tcp, udp, icmp). * port_range_min/max : The port range (80 to 80 means just port 80). * remote_ip_prefix : Defines Who can access this port (The Source). 0.0.0.0/0 is CIDR notation for \"The entire internet.\" For specific networks, you would use something like 192.168.1.0/24 . Creating a Block Storage Volume resources : my_data_volume : type : OS::Cinder platform block storage service ::Volume properties : size : 10 name : db_data Resource Analysis : * my_data_volume : The Logical ID. * type: OS::Cinder::Volume : Explicitly creates a block device in Cinder platform block storage service . * size : The capacity in Gigabytes (GB). * name : The display name visible in the dashboard. Creating a Virtual Machine resources : my_server : type : OS::Nova platform compute service for VMs ::Server properties : image : ubuntu flavor - template - image for quick deployment defining vCPUs, RAM, and disk : m1.small Resource Analysis : * my_server : The Logical ID. * type: OS::Nova::Server : The standard compute instance type. * image / flavor : The Mandatory properties defining the specs. * Note : There are many other optional properties not shown here, such as key_name (SSH Access), networks (Connectivity), security_groups (Firewall), and user_data (Cloud-Init Scripts). We will combine these in the Unified Stack example below. 5.3 The Unified Stack The true power of Heat comes from combining these primitives using Intrinsic Functions . { get_resource: X } : Gets the ID of resource X. { get_param: Y } : Gets the value of user input Y. { get_attr: [Z, val] } : Gets a specific attribute (like an IP address) from resource Z. Full Deployment Example ( deployment.yaml ): heat_template_version : 2018-08-31 description : Full Stack Deployment with Nginx and Floating IP parameters : key_name : type : string description : Name of an existing KeyPair to use public_net_id : type : string description : ID of the external network (e.g., public-net) resources : # 1. The Network app_net : type : OS::Neutron platform networking service ::Net app_subnet : type : OS::Neutron platform networking service ::Subnet properties : network : { get_resource : app_net } cidr : 192.168.10.0/24 # 2. The Security Group web_sg : type : OS::Neutron platform networking service ::SecurityGroup properties : rules : - protocol : tcp port_range_min : 80 port_range_max : 80 remote_ip_prefix : 0.0.0.0/0 - protocol : icmp remote_ip_prefix : 0.0.0.0/0 # 3. The Server web_instance : type : OS::Nova platform compute service for VMs ::Server properties : image : ubuntu_focal flavor - template - image for quick deployment defining vCPUs, RAM, and disk : m1.small key_name : { get_param : key_name } networks : - network : { get_resource : app_net } security_groups : - { get_resource : web_sg } user_data : | #cloud-config packages: - docker.io runcmd: - systemctl enable docker - systemctl start docker - docker run -d -p 80:80 nginx # 4. Floating IP (The Bridge to Internet) my_floating_ip : type : OS::Neutron platform networking service ::FloatingIP properties : floating_network : { get_param : public_net_id } # 5. The Association (Connecting IP to Server) association : type : OS::Neutron platform networking service ::FloatingIPAssociation properties : floatingip_id : { get_resource : my_floating_ip } port_id : { get_attr : [ web_instance , addresses , { get_resource : app_net }, 0 , port ] } outputs : website_url : description : The Public URL of the deployed application value : { get_attr : [ my_floating_ip , floating_ip_address ] } Stack Analysis : * Floating IP : We created a FloatingIP resource on the public network and then an Association resource to link it to our server. This is how the server becomes accessible from your laptop. * User Data : We embedded a Cloud-Config payload to install Docker and launch Nginx as a container. Heat injects this into Cloud-Init, which executes the declarative instructions on boot. * Dependency Chain : The association depends on both the floating_ip and the web_instance . Heat orchestrates this perfectly. 5.4 The Terraform Translation (Rosetta Stone) To prove that these skills are transferable, here is the exact same Nginx server we built in Heat, translated into Terraform. Notice that while the keywords differ ( resources vs resource ), the structural logic\u2014defining a network, security group, and server with dependencies\u2014is identical. Terraform (main.tf) variable \"key_name\" {} variable \"public_net_id\" {} # 1. The Network resource \"openstack_networking_network_v2\" \"app_net\" { name = \"app_net\" } resource \"openstack_networking_subnet_v2\" \"app_subnet\" { network_id = openstack_networking_network_v2.app_net.id cidr = \"192.168.10.0/24\" } # 2. The Security Group resource \"openstack_networking_secgroup_v2\" \"web_sg\" { name = \"web_sg\" description = \"Web Security Group\" } resource \"openstack_networking_secgroup_rule_v2\" \"http_rule\" { direction = \"ingress\" ethertype = \"IPv4\" protocol = \"tcp\" port_range_min = 80 port_range_max = 80 remote_ip_prefix = \"0.0.0.0/0\" security_group_id = openstack_networking_secgroup_v2.web_sg.id } # 3. The Server resource \"openstack_compute_instance_v2\" \"web_instance\" { name = \"web-server\" image_name = \"ubuntu_focal\" flavor_name = \"m1.small\" key_pair = var.key_name user_data = <<- EOF #cloud-config packages: - docker.io runcmd: - systemctl start docker - docker run -d -p 80:80 nginx EOF network { uuid = openstack_networking_network_v2.app_net.id } } # 4. Floating IP resource \"openstack_networking_floatingip_v2\" \"my_floating_ip\" { pool = var.public_net_id } # 5. Association resource \"openstack_compute_floatingip_associate_v2\" \"association\" { floating_ip = openstack_networking_floatingip_v2.my_floating_ip.address instance_id = openstack_compute_instance_v2.web_instance.id } output \"website_url\" { value = openstack_networking_floatingip_v2.my_floating_ip.address } Translation Analysis : * References : Heat uses get_resource . Terraform uses resource_type.resource_name.id . * Structure : Both tools define resources, properties, and dependencies. The syntax changes (YAML vs HCL), but the concepts are universal. By learning Heat, you are effectively learning the logic needed for Terraform. 5.5 Beyond Single VMs: Magnum (Kubernetes orchestration platform ) In Section 5.3, we installed Docker on a single VM - . While fine for development, production requires clusters. OpenStack platform Magnum is the service that bridges Heat and Containers. Orchestration : Magnum uses Heat under the hood to deploy a stack. Resources : It automatically creates the Master Nodes , Worker Nodes , Load Balancers , and Private Networks . Result : Instead of a VM - with Docker, you get a fully manageable Kubernetes orchestration platform Cluster . To deploy a production-grade Kubernetes orchestration platform cluster on OpenStack platform , we use the Magnum CLI . This happens in 3 phases: Phase 1: Create the Cluster Template - image for quick deployment This defines the \"Shape\" of the cluster (OS Image, Keypair, Network Driver). openstack coe cluster template create k8s-template \\ --image fedora-coreos-35 \\ --keypair mykey \\ --external-network public \\ --dns-nameserver 8.8.8.8 \\ --flavor m1.medium \\ --master-flavor m1.medium \\ --coe kubernetes Command Analysis : * template create : Sets the blueprint. * --image : Magnum requires special Fedora Atomic or CoreOS images optimized for containers, not standard Ubuntu. * --coe : Specifies the engine. Magnum also supports Docker Swarm and Apache Mesos, but Kubernetes orchestration platform is the standard. Phase 2: Launch the Cluster This triggers Heat to actually build the stack (VMs, Load Balancers, Security Groups). openstack coe cluster create k8s-cluster \\ --template k8s-template \\ --master-count 1 \\ --node-count 2 Command Analysis : * cluster create : The trigger. This tells Heat to start provisioning resources. * --master-count : High Availability (HA - System design for minimal downtime ) starts at 3 masters, but for labs, 1 is sufficient. * --node-count : The number of workers where your actual Pods (like Nginx) will run. Phase 3: Configure Client Access Once the cluster is CREATE_COMPLETE , we download the credentials to talk to it. mkdir -p ~/.kube openstack coe cluster config k8s-cluster > ~/.kube/config export KUBECONFIG =~/.kube/config kubectl get nodes Command Analysis : * cluster config : This command fetches the TLS certificates and API endpoints from OpenStack platform . * export KUBECONFIG : Tells the kubectl tool where to find these credentials. Without this, kubectl doesn't know which cluster to talk to. 5.5.1 Step 4: Deploying Workloads (Pods vs VMs) Now that the cluster is running, we stop talking to OpenStack platform (Heat) and start talking to Kubernetes orchestration platform ( kubectl ). Here is how we deploy Nginx with 3 Replicas (Load Balanced). Kubernetes orchestration platform Manifest ( nginx-deployment.yaml ) apiVersion : apps/v1 kind : Deployment metadata : name : nginx-cluster spec : replicas : 3 # The Power of K8s: 3 Copies automatically managed selector : matchLabels : app : web template - image for quick deployment : metadata : labels : app : web spec : containers : - name : nginx image : docker.io/nginx:latest ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : nginx-lb spec : type : LoadBalancer # OpenStack platform will create a physical Load Balancer for this selector : app : web ports : - port : 80 targetPort : 80 Stack Analysis : * Replicas: 3 : Instead of creating web_server_01 , web_server_02 , etc., we simply ask for \"3 copies\". Kubernetes orchestration platform ensures they are always running. * Service (LoadBalancer) : This object talks to OpenStack platform Neutron platform networking service /Octavia to provision a real Load Balancer that distributes traffic to those 3 pods. Section 5 Checkpoint Summary : Heat templates allow us to define an entire infrastructure stack in a single file. By understanding the core structure (Parameters, Resources, Outputs) and the Building Blocks (Cinder platform block storage service , Nova platform compute service for VMs , Neutron platform networking service resources), we can assemble complex environments that are consistently reproducible. Reflection : Why is it better to define the Security Group inside the template - image for quick deployment rather than assuming it already exists? (Hint: It makes the template - image for quick deployment \"self-contained\" and easier to deploy in a fresh project). 6. Configuration Management with Ansible We have now learned how to create servers using Heat () and how to bootstrap them using Cloud-Init. But what happens on Day 2 ? How do you patch 50 servers? How do you update a config file on all of them? Rebuilding the entire stack with Heat every time you need to change a comma in a config file is inefficient. Ansible is the industry-standard tool for this phase. It is an agentless automation engine that connects to your servers via SSH and enforces a desired state. 6.1 The Inventory Ansible needs to know what it is managing. This is defined in an Inventory file. While it supports a simple INI format, YAML is preferred for clarity. Example Inventory ( hosts.yaml ): all : children : webservers : hosts : web-01 : ansible_host : 192.168.1.10 web-02 : ansible_host : 192.168.1.11 databases : hosts : db-01 : ansible_host : 192.168.1.20 Inventory Analysis : * all : The root group containing every server. * children : Sub-groups (e.g., webservers , databases ) allow you to target specific roles. * ansible_host : Variable defining the actual IP to connect to. 6.2 Ad-Hoc Commands For quick, one-off tasks, you don't need to write a script. You can simply \"speak\" to your cluster using the CLI. # Ping all servers ansible all -m ping -i hosts.yaml # Check uptime on webservers only ansible webservers -m shell -a \"uptime\" -i hosts.yaml Command Analysis : * all / webservers : The target group from the inventory. * -m ping : The Module to run. 'ping' in Ansible checks SSH connectivity and Python availability, not ICMP. * -a : Arguments passed to the module. 6.3 Playbooks (The Core) While Ad-Hoc commands are useful, the real power lies in Playbooks . These are YAML files that describe a complex set of tasks\u2014a \"play.\" Example Playbook ( site.yaml ): - name : Configure Web Servers hosts : webservers become : yes # Run as sudo tasks : - name : Ensure Apache is installed apt : name : apache2 state : present - name : Deploy custom index page copy : content : \"<h1>Managed by Ansible\" dest : /var/www/html/index.html - name : Ensure Service is Running service : name : apache2 state : started Playbook Analysis : * apt , copy , service : These are Modules . They abstract away the OS details (e.g., you don't type apt-get install , you just say state: present ). * Idempotency : This is the most critical concept. If you run this playbook 100 times, it will only make changes the first time. On subsequent runs, it checks \"Is Apache present?\", sees \"Yes\", and does nothing. This makes it safe to run against production systems repeatedly. 6.4 The Unified Pipeline (Integration) The ultimate goal is to chain these tools together. A simple Bash script can act as the \"glue\" that triggers Heat to build the infrastructure, waits for the output, and then passes that information to Ansible for configuration. Example: deploy.sh #!/bin/bash # 1. Launch Infrastructure with Heat echo \"Step 1: Creating Stack...\" openstack stack create -t deployment.yaml -p key_name =mykey my-stack # 2. Wait for completion (Simple loop) echo \"Step 2: Waiting for IP...\" while true ; do STATUS = $(openstack stack show my-stack -f json | jq -r .stack_status ) if [ \" $STATUS \" == \"CREATE_COMPLETE\" ] ; then break ; fi sleep 5 done # 3. Extract the IP Address (Output) IP = $(openstack stack show my-stack -f json | jq -r '.outputs[] | select(.output_key==\"website_url\") | .output_value' ) echo \"deployed at $IP \" # 4. Generate Inventory for Ansible echo \"[webservers]\" > hosts.ini echo \" $IP ansible_user=ubuntu\" >> hosts.ini # 5. Run Configuration echo \"Step 3: Configuring with Ansible...\" ansible-playbook -i hosts.ini site.yaml Pipeline Analysis : * Glue Code : Bash is used here not to manage resources, but to manage tools . It bridges the gap between Heat (Infrastructure) and Ansible (Config). * Dynamic Inventory : Note how we create hosts.ini on the fly. Note: ansible_user=ubuntu assumes an Ubuntu image; adjust this for Rocky/CentOS ( rocky ) or Fedora ( fedora ). Section 6 Checkpoint Summary : Ansible fills the gap of \"Day 2 Operations.\" It uses an Inventory to group servers and Playbooks to define their configuration. Unlike a Bash script which runs blindly, Ansible is Idempotent \u2014it only acts if the system is not in the desired state. Reflection : Compare this to the Bash script in Section 3. If you ran that Bash script twice, it would try to create the servers again (and fail). If you run an Ansible playbook twice, it simply reports \"OK\" (No Change). 7. Version Control: Managing your Templates Treating means your templates ( deployment.yaml ) and playbooks ( site.yaml ) are valuable assets. They should not live in a folder on your laptop; they should be managed in a Version Control System like Git . 7.1 Why Git? History : \"Who changed the firewall rule last Tuesday?\" Git tells you exactly who and why. Rollback : If a new template - image for quick deployment breaks production, git revert allows you to instantly return to the working version. Collaboration : Multiple engineers can work on the same stack without overwriting each other's files. 7.2 The Basic Workflow Students are expected to manage their Capstone project using these commands: # Initialize a repository git init # Add your templates git add deployment.yaml site.yaml # Save a snapshot - (Checkpoint) git commit -m \"Added Nginx installation to Heat template - image for quick deployment \" # View History git log Git Analysis : * commit : This is your \"Save Game\" button. Make a commit every time you reach a stable state (e.g., \"Heat template - image for quick deployment works\", \"Ansible connects\"). * GitOps : In advanced environments, applying a commit to a Git repository automatically triggers the deploy.sh pipeline we wrote above. This is known as GitOps . 7.3 Strategic Summary To help you lock in the mental model of \"Which Tool, When?\" , review this comparison: Tool Phase Scope Cloud-Init Boot time Single VM - Bash Glue Tool orchestration Python SDK API automation Fine-grained logic Heat Infrastructure Declarative Stacks Magnum Clusters Platform-level Ansible Day-2 Ops Fleet Management Kubernetes orchestration platform Workloads Container Orchestration --- 8. Summary and Next Steps This week we evolved from \"ClickOps\" to DevOps . We learned that while GUIs are great for exploration, they do not scale. By using Cloud-Init for boot-time logic, Heat for infrastructure definition, Ansible for configuration, and Magnum for container orchestration, we built a repeatable, self-healing cloud environment. Preparing for Week 12 Next week is the Capstone Project . You will combine everything you have learned in the last 11 weeks to build a Production-Ready Cloud Environment . You will need to provision the network, compute nodes, and storage, and then configure a scalable web service using the automation tools we mastered this week. Checklist: Can you differentiate between Imperative (Bash) and Declarative (Heat) automation? Do you understand why Idempotency is critical for Day-2 operations? Can you explain the transition from Infrastructure-as-Service (Nova platform compute service for VMs ) to Platform-as-Service (Magnum)? Are you ready to use Git to manage your project templates? 9. Additional Resources jq Playground Cloud-Init Examples 10. Lab Exercises Lab 1: Automation with Scripts Goal : Scripting cloud deployments with Bash, Python, and Heat. Lab 2: Configuration with Ansible Goal : Using Ansible Playbooks to configure a web server fleet. Lab 3: heat Infrastructure Goal : Declarative deployment of Network, Security Groups, and Servers. Lab 4: Kubernetes with Magnum Goal : Provisioning a K8s cluster and deploying containers. Test Your Knowledge Ready to check your understanding of this week's material? Take the interactive quiz now! Start Quiz Previous: Week 10: Persistence Course Index Next: Week 12: Final Review\n\n--- WEEK 12 NOTES ---\nWeek 12: Capstone Project Brief Final Project & Review Course : Computer Systems Engineering Module : Operating Systems 3 (Virtualisation & Cloud Technologies) Instructor : KT Nshimba Total Marks : 100 Duration : 4 Hours (Practical) + 1 Hour (Review) 1. Project Overview You have been hired as a Cloud Architect for \"VaalTech Solutions\". They require a fully functional Private Cloud environment to host their new intranet application. You must use OpenStack platform (MicroStack) to build the infrastructure, secure it, and deploy a web application automatically. Figure 1: Conceptual Architecture - The goal is to build a standard 3-Tier Web Application (Frontend, Logic, Database) 2. Requirements Part A: Infrastructure (30 Marks) Network : Create a private network corporate-net with subnet 10.10.10.0/24 . Create a Router corp-router connected to the External Provider Network. Storage : Create a Cinder platform block storage service Volume db-data (2GB). Security : Create two Security Groups: frontend-sg : Allow HTTP (TCP 80), HTTPS (TCP 443), and SSH (TCP 22). backend-sg : Allow MySQL (TCP 3306) ONLY from the Frontend IP. Allow SSH (TCP 22). Part B: Deployment (40 Marks) Frontend Server : Name: web-01 . Flavor - template - image for quick deployment defining vCPUs, RAM, and disk : m1.tiny . Image: ubuntu (or cirros ). Automation : Must use User-Data (Cloud-Init) to install a web server ( apache2 or nginx ) and create a customized index.html containing your Student Name and Number. Networking: Attached to corporate-net . Assigned a Floating IP . Security: frontend-sg . Backend Server : Name: db-01 . Network: Attached to corporate-net . No Floating IP. Storage: Attach the db-data Cinder platform block storage service Volume. Security: backend-sg . Part C: Verification (30 Marks) Access : The examiner must be able to load your website via the Floating IP. The website must display your Name. Persistence : Create a file on the db-01 attached volume. Detach the volume, attach it to web-01 , and prove the file exists. Security Penetration Test : Attempting to SSH into db-01 from the public internet must FAIL. Attempting to SSH into db-01 from web-01 must SUCCEED. Figure 2: Reference Topology - The target network configuration, showing the Router, Internal Network, and Security Boundaries Part D: Bonus Challenge (10 Marks - Extra Credit) Golden Image Strategy : Create a Snapshot - of your configured web-01 server. Launch a new instance web-02 from this Snapshot - . Requirement : web-02 must serve the website immediately upon boot without needing User-Data . Theory : In your report, explain why this \"Golden Image\" approach boots faster than the \"Post-Boot Configuration\" approach used in Part B. 3. Submission Guidelines You must submit a PDF Report containing: Topology Diagram : A screenshot of the Horizon platform web dashboard Network Topology tab. Automation Script : The Cloud-Init ( user-data ) file you used. Proof of Life : A screenshot of the web browser showing your index page. CLI Verification : Output of openstack server list and openstack volume list . 4. Grading Rubric Component Criteria Marks Networking Internal Network, Router, Gateway configured correctly. 20 Security Security Groups implement \"Least Privilege\" correctly. 20 Automation Web server installs without human intervention. 20 Storage Volume created, attached, formatted, and persistent. 20 Documentation Clear screenshots, correct CLI outputs, professional formatting. 20 TOTAL 100 Previous: Week 11: Automation Course Index";

if (typeof module !== 'undefined') module.exports = COURSE_CONTEXT;
