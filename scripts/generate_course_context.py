#!/usr/bin/env python3
"""
Generate Course Context
Scrapes all Week_X_Student_Notes.html files, strips HTML tags, and compiles them
into a single JavaScript file (js/course_context.js) for the AI Chatbot.
"""

import os
import re
import json
from pathlib import Path
from bs4 import BeautifulSoup

def clean_text(text):
    """Deep clean text to save tokens."""
    # Remove excessive whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def get_week_number(filename):
    """Extract week number from filename for sorting."""
    match = re.search(r'Week_(\d+)_', filename)
    return int(match.group(1)) if match else 999

def generate_context():
    base_dir = Path(__file__).parent.parent
    js_output_path = base_dir / "js" / "course_context.js"
    
    print(f"Scanning for Student Notes in {base_dir}...")
    
    # Find all student notes
    note_files = list(base_dir.glob("Week */Week_*_Student_Notes.html"))
    # Sort by week number
    note_files.sort(key=lambda p: get_week_number(p.name))
    
    all_content = []
    
    for note_file in note_files:
        print(f"Processing {note_file.name}...")
        
        try:
            with open(note_file, 'r', encoding='utf-8') as f:
                soup = BeautifulSoup(f.read(), 'html.parser')
                
            # Extract main content (skip nav, header, footer if possible)
            # Assuming main content is in <article> or just body
            article = soup.find('article')
            if not article:
                article = soup.body
            
            if article:
                # Remove scripts and styles
                for script in article(["script", "style"]):
                    script.decompose()
                
                # Get text
                text_content = article.get_text(separator=' ')
                cleaned_text = clean_text(text_content)
                
                week_num = get_week_number(note_file.name)
                all_content.append(f"--- WEEK {week_num} NOTES ---\n{cleaned_text}")
                
        except Exception as e:
            print(f"Error processing {note_file.name}: {e}")

    # Combine all
    full_context = "\n\n".join(all_content)
    token_est = len(full_context) / 4
    print(f"Total compiled characters: {len(full_context)}")
    print(f"Estimated tokens: {int(token_est)}")
    
    # Escape for JS string
    # We use json.dumps to safely escape the string
    json_str = json.dumps(full_context)
    
    js_content = f"""// AUTO-GENERATED BY scripts/generate_course_context.py
// DO NOT EDIT MANUALLY
// Generated on: {os.popen('date').read().strip() if os.name != 'nt' else 'Now'}

const COURSE_CONTEXT = {json_str};

if (typeof module !== 'undefined') module.exports = COURSE_CONTEXT;
"""

    # Ensure js dir exists
    js_output_path.parent.mkdir(exist_ok=True)
    
    with open(js_output_path, 'w', encoding='utf-8') as f:
        f.write(js_content)
        
    print(f"âœ… Context generated at {js_output_path}")

if __name__ == "__main__":
    generate_context()
