<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Virtual Machines (VMs) - Presentation Slides</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #1a1a2e 0%, #16213e 100%);
            color: #ffffff;
            line-height: 1.6;
        }
        
        .slide {
            min-height: 100vh;
            padding: 60px 80px;
            display: flex;
            flex-direction: column;
            justify-content: center;
            page-break-after: always;
            border-bottom: 3px solid #0f3460;
        }
        
        .slide:nth-child(even) {
            background: linear-gradient(135deg, #16213e 0%, #1a1a2e 100%);
        }
        
        .title-slide {
            justify-content: center;
            align-items: center;
            text-align: center;
            background: linear-gradient(135deg, #0f3460 0%, #16213e 100%);
        }
        
        .title-slide h1 {
            font-size: 3.5em;
            margin-bottom: 20px;
            color: #e94560;
            text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
        }
        
        .title-slide .week-number {
            font-size: 1.5em;
            color: #ffd700;
            margin-bottom: 40px;
            letter-spacing: 2px;
        }
        
        .title-slide .course-code {
            font-size: 1.2em;
            color: #a8dadc;
            margin-top: 30px;
        }
        
        h2 {
            font-size: 2.5em;
            color: #e94560;
            margin-bottom: 30px;
            border-bottom: 3px solid #ffd700;
            padding-bottom: 15px;
        }
        
        h3 {
            font-size: 2em;
            color: #ffd700;
            margin-bottom: 25px;
        }
        
        ul {
            font-size: 1.4em;
            margin-left: 40px;
            margin-bottom: 20px;
        }
        
        ul li {
            margin-bottom: 15px;
            line-height: 1.8;
        }
        
        p {
            font-size: 1.3em;
            margin-bottom: 20px;
            line-height: 1.8;
        }
        
        .code-block {
            background: #0f0f0f;
            border-left: 4px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            overflow-x: auto;
        }
        
        pre {
            background: #0f0f0f;
            border-left: 4px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 1.1em;
            line-height: 1.5;
        }
        
        code {
            font-family: 'Courier New', monospace;
            color: #a8dadc;
        }
        
        .quote {
            background: rgba(233, 69, 96, 0.1);
            border-left: 5px solid #e94560;
            padding: 20px;
            margin: 20px 0;
            font-style: italic;
            font-size: 1.2em;
        }
        
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            font-size: 1.2em;
        }
        
        th {
            background: #0f3460;
            color: #ffd700;
            padding: 15px;
            text-align: left;
            border: 1px solid #16213e;
        }
        
        td {
            padding: 12px;
            border: 1px solid #16213e;
            background: rgba(255, 255, 255, 0.05);
        }
        
        tr:hover {
            background: rgba(233, 69, 96, 0.1);
        }
        
        img {
            max-width: 100%;
            height: auto;
            margin: 20px 0;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.3);
        }
        
        .key-points {
            background: rgba(255, 215, 0, 0.1);
            border: 2px solid #ffd700;
            padding: 30px;
            border-radius: 10px;
            margin: 20px 0;
        }
        
        .vut-logo {
            position: fixed;
            bottom: 20px;
            right: 30px;
            height: 60px;
           width: auto;
            opacity: 0.8;
        }
        
        @media print {
            .slide {
                page-break-after: always;
            }
        }
    </style>
</head>
<body>

    <div class="slide title-slide">
        <div class="week-number">Week 2</div>
        <h1>Virtual Machines (VMs)</h1>
        <div class="course-code">OPS3 - Virtualization and Cloud Infrastructure</div>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>Welcome to Week 2!</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>What You'll Learn This Week</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>1. Deep Dive: Linux Virtualization (KVM & QEMU)</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>1.1 The Engine: KVM (Kernel-based Virtual Machine)</h3>
        <ul>
            <li>The Kernel-based Virtual Machine (KVM) is implemented as a loadable kernel module (kvm.ko) that transforms the Linux kernel into a Type-1 hypervisor.</li>
            <li>Unlike traditional hypervisors that run as a separate software layer, KVM leverages the kernel's existing process scheduling, memory management, and I/O stack.</li>
            <li>It exposes a character device, /dev/kvm, which user-space processes (like QEMU) interact with via ioctl() system calls to create and run virtual machines.</li>
        </ul>
        <ul>
            <li>The VM Execution Loop (ioctl Interface):
The interaction between User Mode (the QEMU process) and Guest Mode (the VM code) is handled via a blocking system call known as KVM_RUN.</li>
            <li>The process begins with Setup, where QEMU opens the /dev/kvm device node, issues the KVM_CREATE_VM call to initialize the virtual environment, and maps the necessary memory for the guest.</li>
            <li>Once initialized, the Execution phase begins: QEMU invokes the KVM_RUN ioctl, signaling the kernel to context-switch the CPU into Guest Mode (Ring -1 or VMX Root Operation).</li>
        </ul>
        <ul>
            <li>In this mode, the vCPU executes instructions directly on the silicon at native speed ("Direct Execution").</li>
            <li>This continues until the VM attempts a privileged operation, such as writing to a hardware register or accessing restricted memory, which triggers a VM Exit.</li>
            <li>The hardware forces the CPU back into Host Kernel Mode, where KVM analyzes the exit reason.</li>
            <li>If the exit is "Lightweight" (e.g., a simple timer interrupt or paging request), KVM handles it internally and immediately re-enters the VM.</li>
            <li>However, if the exit is "Heavyweight" (requiring complex I/O like disk writes), KVM returns control to the QEMU user-space process.</li>
        </ul>
        <p>Figure 1.2: The Cycle of Direct Execution and Trapped Emulation.</p>
        <p>Code Snippet: Creating a VM via KVM API (C)</p>
        <p>Figure 1.1: Simplified C code showing how a user-space program instructs the kernel to create a VM.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>1.2 Advanced KVM Technologies</h3>
        <p>Modern KVM deployments leverage several advanced kernel features to maximize efficiency:</p>
        <ul>
            <li>Kernel Samepage Merging (KSM) is a memory deduplication feature within the Linux kernel (stored in mm/ksm.c) that allows for high-density virtualization.</li>
            <li>KSM utilizes a background kernel thread (ksmd) that periodically scans designated memory regions looking for pages with identical content.</li>
            <li>When matches are found, KSM merges these pages into a single physical page marked as Copy-on-Write (CoW).</li>
            <li>This allows multiple virtual machines running similar operating systems (e.g., ten instances of Windows Server) to share the same physical RAM for common libraries and kernel data.</li>
            <li>While this significantly increases density (memory overcommitment), it comes with a slight CPU overhead due to the scanning process.</li>
        </ul>
        <ul>
            <li>Live Migration is the capability to move a running virtual machine from one physical host to another with no perceptible downtime to the end-user.</li>
            <li>The process involves an iterative memory copy mechanism.</li>
            <li>First, the hypervisor copies the VM's memory to the destination host while the VM continues to execute.</li>
            <li>As memory changes (becomes "dirty") during this copy, those dirty pages are tracked and re-sent in subsequent rounds.</li>
            <li>Once the remaining dirty pages are small enough to be transferred instantly, the VM is momentarily paused, the final state (CPU registers and remaining memory) is synced, and execution resumes on the new host.</li>
        </ul>
        <ul>
            <li>Nested Virtualization refers to the practice of running a hypervisor inside another virtual machine—effectively, a "VM inside a VM." This is achieved by forwarding hardware virtualization extensions (Intel VT-x or AMD-V) from the host CPU through Level 0 (Host Hypervisor) to Level 1 (Guest Hypervisor).</li>
            <li>This is particularly useful for training environments, development labs, or testing cloud orchestration platforms like OpenStack without requiring dedicated bare-metal hardware for every node.</li>
        </ul>
        <ul>
            <li>Standard virtualization relies on QEMU to emulate hardware devices, which introduces overhead due to the translation of instructions.</li>
            <li>For workloads requiring extreme performance—such as high-frequency trading, GPU-accelerated Machine Learning, or 100Gbps networking—emulation is insufficient.</li>
            <li>The Virtual Function I/O (VFIO) framework allows the host kernel to unbind a physical PCI device from the host drivers and pass it directly to the virtual machine.</li>
            <li>This gives the guest OS direct, exclusive access to the hardware, resulting in near-native performance, albeit at the cost of losing that device on the host system.</li>
        </ul>
        <ul>
            <li>Memory Ballooning is a dynamic memory management technique that allows the hypervisor to reclaim RAM from running virtual machines.</li>
            <li>It utilizes the virtio-balloon driver installed within the guest OS.</li>
            <li>When the host is under memory pressure, it instructs the balloon driver to "inflate," causing the guest kernel to allocate RAM to the driver.</li>
            <li>The driver then informs the host which physical pages it has claimed, allowing the host to safely repurpose those physical pages for other tasks.</li>
            <li>Conversely, when the guest needs more memory, the balloon "deflates," returning the pages to the guest's free pool.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>1.3 Practical: Verifying KVM Support</h3>
        <p>Before creating virtual machines, it is imperative to verify that the host system is correctly configured to support hardware-assisted virtualization. This involves checking the CPU capabilities, kernel module status, and user permissions.</p>
        <ul>
            <li>The first step is to confirm that the physical processor supports the necessary virtualization extensions (Intel VT-x or AMD-V) and that these extensions are enabled in the system BIOS/UEFI.</li>
            <li>In Linux, we verify this by inspecting the CPU flags via the lscpu command.</li>
            <li>We specifically filter for "Virtualization" to see the vendor-specific technology.</li>
        </ul>
        <p>If this command returns no output, it indicates that hardware virtualization is disabled at the firmware level. You must reboot the machine, enter the BIOS setup, and enable "Virtualization Technology" (often labeled as VT-x, Vanderpool, or SVM).</p>
        <p>Once hardware support is confirmed, we must ensure the Linux kernel has loaded the KVM modules. The KVM system consists of a core module (kvm.ko) and a processor-specific module (kvm_intel.ko or kvm_amd.ko). We use lsmod to list loaded modules:</p>
        <p>If these modules are not present, they can often be loaded manually using modprobe kvm_intel (or amd), provided the hardware support is active.</p>
        <ul>
            <li>Security is a critical aspect of virtualization.</li>
            <li>The kernel interface for creating VMs, located at /dev/kvm, is restricted.</li>
            <li>Standard users cannot access this device by default.</li>
            <li>To allow a user to run VMs without root privileges (a security best practice), the user must be added to the specialized kvm group.</li>
        </ul>
        <p>If you encounter a "Permission Denied" error when running QEMU, verify your group membership using the groups command. If the kvm group is missing, you must add your user to it (sudo usermod -aG kvm $USER) and log out/in to apply the changes.</p>
        <p>You may encounter specific error messages during this verification process.</p>
        <ul>
            <li>"KVM: disabled by BIOS": This message explicitly states that while the CPU supports virtualization, the feature is turned off in the computer's firmware. This cannot be fixed from within the OS; a physical reboot is required to modify BIOS settings.</li>
            <li>"KVM support not available": This often occurs when trying to run KVM inside another virtual machine (e.g., a cloud VPS) that does not support Nested Virtualization. In this scenario, the "outer" hypervisor has not passed the hardware extensions through to your "inner" guest.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>1.4 The Hardware: QEMU (Quick Emulator)</h3>
        <p>While KVM enables the kernel to execute instructions, it does not provide the "Computer." It is the role of QEMU to provide the motherboard, the chipset, the PCI bus, and the plugged-in devices. Without QEMU, KVM is just a fast calculator; with QEMU, it becomes a server.</p>
        <ul>
            <li>To the Host Linux Kernel, a running Virtual Machine is purely a standard user-space process (qemu-system-x86_64) that happens to be multi-threaded.</li>
            <li>It does not look different from a web browser or database server from the scheduler's perspective.</li>
            <li>The architecture relies on three distinct thread types.</li>
            <li>The Main Loop (iothread) is a single thread running a glib-based event loop responsible for non-blocking tasks, such as handling the QEMU Monitor (management interface), updating VNC/SPICE displays, and dispatching general I/O events.</li>
            <li>Parallel to this are the vCPU Threads; for every Virtual CPU core assigned to the guest, QEMU spawns a dedicated POSIX thread (pthread) that enters the KVM_RUN ioctl loop to execute guest code on the physical CPU.</li>
        </ul>
        <p>When you configure a VM in Proxmox, you are actually selecting arguments for the QEMU binary, starting with the Machine Type.</p>
        <ul>
            <li>This single line defines the fundamental architecture of the virtual motherboard.</li>
            <li>The pc-q35 argument selects the modern Q35 chipset, which provides support for PCIe native handling and Secure Boot, as opposed to the legacy i440fx type which mimics a 1996-era PC.</li>
            <li>Crucially, the accel=kvm flag explicitly links the Emulator (QEMU) to the Engine (KVM).</li>
            <li>Without this flag, QEMU would default to "TCG" (Tiny Code Generator), a software-only mode that interprets every instruction, resulting in agonizingly slow performance.</li>
        </ul>
        <p>QEMU allows granular control over how the CPU is presented to the guest OS. This is critical for licensing (some software is licensed per-socket) and performance (aligning with physical NUMA nodes).</p>
        <p>This argument creates a topology of 1 Socket with 4 Cores. The Guest OS sees this exactly as if it were physical silicon.</p>
        <p>In QEMU, every device is composed of two parts: the Frontend (what the Guest OS sees) and the Backend (how the Host handles the data).</p>
        <ul>
            <li>The Backend (defined by -netdev or -drive) refers to the host-side resource, such as connecting to a Linux Bridge (tap100i0) or a disk image file.</li>
            <li>(Note: The concept of the Linux Bridge and how it connects VMs to the physical network will be discussed in depth in the Network Chapter in Week 4.) The Frontend (defined by -device) creates the virtual hardware that appears on the guest's PCI bus, such as a "VirtIO Network Card".</li>
            <li>The guest OS writes data to the Frontend device, and QEMU is responsible for passing that data to the polling Backend.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>1.5 Optimization: VirtIO (Paravirtualization)</h3>
        <p>Emulating physical hardware (like an Intel E1000 network card) is "expensive" because every packet sent requires a context switch (VM Exit) to write to device registers, which QEMU then has to decode and simulate.</p>
        <ul>
            <li>The Solution: VirtIO
VirtIO replaces full hardware emulation with a standardized "Paravirtualized" architecture.</li>
            <li>In this model, the Guest OS is aware that it is running in a virtual environment.</li>
            <li>Instead of trapping and emulating legacy hardware registers (which is slow), the Guest uses a specialized virtio driver to communicate directly with the Host via a shared memory interface.</li>
        </ul>
        <ul>
            <li>Mechanism (The Ring Buffer):
The core of VirtIO's performance is the Virtqueue, implemented as a circular ring buffer in shared memory.</li>
            <li>The process begins with Shared Memory, where QEMU allocates a region of RAM that is mapped into the address spaces of both the Host and the Guest.</li>
            <li>When the Guest needs to send data (e.g., a network packet), it initiates a vRing Operation by placing a descriptor pointer into the vRing buffer.</li>
            <li>Subsequently, the Guest performs a Kick Notification—a lightweight signal (via ioeventfd)—to alert the Host that new data is available.</li>
            <li>Finally, the Host performs Zero-Copy Processing by reading the data directly from the shared memory without needing to simulate a physical device operation, processing it, and placing a response back in the ring.</li>
        </ul>
        <p>Note: This is why you must select "VirtIO" for Network and Disk in Proxmox when performance matters.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>1.6 Essential Command Reference</h3>
        <ul>
            <li>While Proxmox handles these automatically, knowing the underlying commands is useful for debugging and "deep dive" understanding.</li>
            <li>The following commands are provided for reference and theoretical understanding; you will have the opportunity to execute them and observe their output directly in this week's Lab exercises.</li>
        </ul>
        <ul>
            <li>The qemu-img utility handles virtual disk creation and manipulation.</li>
            <li>To create a new disk, the create subcommand is used, specifying the format (typically qcow2 for thin provisioning) and the size.</li>
            <li>For example, qemu-img create -f qcow2 mydisk.qcow2 20G.</li>
            <li>To inspect an existing disk's virtual size and actual disk usage, the info command provides detailed metadata.</li>
            <li>Furthermore, the convert subcommand interacts with the format translation engine, allowing administrators to transform a generic .img image into a VMware .vmdk or QEMU .qcow2 image.</li>
        </ul>
        <ul>
            <li>Launching a VM manually involves the qemu-system-x86_64 binary.</li>
            <li>A basic invocation usually requires defining the hard drive (-hda), the allocated RAM (-m), and crucially, the KVM accelerator (-enable-kvm or -accel kvm).</li>
            <li>For booting from an installer ISO, the -cdrom flag is added, often accompanied by -boot d to prioritize the optical drive in the boot order.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Section 1 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>KVM: The kernel module that handles CPU execution (Guest Mode).</li>
            <li>QEMU: The process that provides virtual hardware (Disks, NICs).</li>
            <li>VirtIO: Special drivers that bypass emulation for speed.</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why does top on the host show a qemu-kvm process using 100% CPU if the VM is busy?</li>
            <li>What is the difference between "Full Emulation" and "Paravirtualization"?</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>Red Hat: KVM Architecture</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>2. The Platform: Proxmox VE</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.1 Architectural Breakdown</h3>
        <p>Figure 2.1: Proxmox VE Architecture - Decoupling the Web Interface, API, and Core KVM Engine.</p>
        <p>As illustrated in Figure 2.1, Proxmox VE is designed as a layered interaction model. It is not a monolithic black box, but a collection of distinct services working in harmony.</p>
        <ul>
            <li>This layer facilitates all human-to-machine interaction and is driven by several key daemons.</li>
            <li>The pveproxy service acts as the primary "front door," listening on port 8006 and serving the web interface via a secure HTTP server; it forwards valid requests to the internal API (pvedaemon).</li>
            <li>For console access, spiceproxy and vncterm handle the streaming of graphical display data from VMs to the browser.</li>
            <li>The pvedaemon (PVE API Daemon) is the worker background process that actually executes privileged tasks; whether triggered by the JSON-based Web GUI, the pvesh CLI tool, or an external script, all roads lead to this daemon, ensuring a consistent execution path for every command.</li>
            <li>This layer also enforces User Authentication, validating credentials against configurable realms (PAM, LDAP, AD) before granting access.</li>
        </ul>
        <ul>
            <li>Invisible to the user, a suite of daemons maintains the cluster's brain.</li>
            <li>Corosync is the foundational cluster engine, providing the reliable, low-latency communication required to maintain quorum (node consensus).</li>
            <li>Data consistency is handled by pmxcfs (Proxmox Cluster File System), a database-driven filesystem that instantly replicates configuration files in /etc/pve to all nodes.</li>
            <li>Monitoring is the responsibility of pvestatd (PVE Status Daemon), which queries the status of VMs, containers, and storage every 10 seconds to update the management layer.</li>
            <li>Finally, the High Availability (HA) stack consists of two critical components: the pve-ha-crm (Cluster Resource Manager), which decides where a service should run, and the pve-ha-lrm (Local Resource Manager), which watches services on the local node and reports their state to the CRM, ensuring rapid recovery if a node fails.</li>
        </ul>
        <ul>
            <li>This is where the actual work happens, overseen by the Proxmox Kernel.</li>
            <li>For full virtualization, the KVM kernel module turns the Linux kernel into a hypervisor, while QEMU processes utilize this to run guest operating systems.</li>
            <li>For lightweight virtualization, LXC (Linux Containers) enables kernel-shared environments, supported by lxcfs, a userspace filesystem that provides containers with virtualized views of /proc files (like CPU and memory stats) so they don't see the host's full resources.</li>
            <li>Storage Plugins simplify disk management by translating abstract requests ("create disk") into specific backend commands (e.g., zfs create or rbd create).</li>
            <li>Similarly, the pve-firewall service generates 'iptables' rules dynamically for each guest, creating isolated security zones (Interfaces/Bridges) at the kernel level.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>2.2 Key Components</h3>
        <ul>
            <li>Proxmox VE is not a single application but a suite of integrated components.</li>
            <li>Understanding how these distinct parts—the interface, storage backends, and clustering services—fit together is essential for designing a resilient infrastructure.</li>
            <li>Although the architecture is layered, the daily operational experience revolves around extensive interaction with the following key subsystems.</li>
        </ul>
        <p>Figure 2.2: The Proxmox VE Web Interface (GUI) providing a centralized view of the datacenter.</p>
        <ul>
            <li>The primary management point is the web-based Graphical User Interface, accessible via port 8006.</li>
            <li>It abstracts the complexity of qemu command lines and configuration files, allowing administrators to create VMs, manage storage pools, and configure software-defined networking bridges with visual feedback.</li>
        </ul>
        <ul>
            <li>The interface organizes these capabilities into four distinct regions.</li>
            <li>At the top, the Header provides critical status information and action buttons for system-wide operations.</li>
            <li>To the left, the Resource Tree acts as the main navigation hub, allowing you to select specific objects such as nodes, VMs, or storage pools.</li>
            <li>The center region contains the Content Panel, which dynamically updates to show the configuration options and status for whichever object is selected in the tree.</li>
            <li>Finally, the Log Panel resides at the bottom, creating a real-time audit trail of recent tasks; administrators can double-click these entries to view detailed execution logs or abort running operations.</li>
        </ul>
        <ul>
            <li>Proxmox decouples compute from storage to facilitate flexibility across different environments.</li>
            <li>For standalone setups, administrators often utilize Local Storage such as LVM-Thin or ZFS, which support advanced features like instant snapshots.</li>
            <li>In clustered environments, the platform leverages Network Storage (NFS, iSCSI, or SMB) to enable VM mobility and Live Migration.</li>
            <li>Furthermore, Proxmox supports Hyper-Converged infrastructure through native integration with Ceph, a distributed object store that runs directly on the nodes themselves, eliminating the need for expensive external SAN hardware.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Section 2 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Architecture: Web UI -> API -> Corosync/pmxcfs -> Kernel/KVM.</li>
            <li>pmxcfs: The magic filesystem that keeps cluster configs in sync.</li>
            <li>Storage: Decoupled from compute to allow flexibility (Local vs Shared).</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why is an API-first design better for automation?</li>
            <li>If pmxcfs replicates configs, what happens if you lose network connectivity between nodes?</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>3. VM Management Features</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>3.1 Cloning</h3>
        <p>Cloning is the process of creating a new virtual machine based on the state of an existing one. Proxmox offers two distinct methods suited for different use cases.</p>
        <ul>
            <li>A Full Clone is a complete, independent copy of the original VM.</li>
            <li>The system performs a block-by-block copy of the source disk image to a new file.</li>
            <li>Since it duplicates all data, it consumes significant time and storage space.</li>
            <li>However, its complete isolation makes it ideal for production deployments, as the new VM has no dependency on the original.</li>
        </ul>
        <ul>
            <li>A Linked Clone uses a "Copy-on-Write" mechanism.</li>
            <li>It does not copy the original disk; instead, it creates a new delta file that references the original "Base" disk.</li>
            <li>The new VM reads from the Base disk but writes changes to its own small delta file.</li>
            <li>This allows for near-instant creation and minimal storage usage, making it perfect for efficient testing or classroom labs.</li>
            <li>However, it introduces a critical dependency: the Base disk cannot be deleted without breaking all Linked Clones.</li>
        </ul>
        <p>Figure 3.1: Full Clones copy data; Linked Clones reference data.</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>3.2 Snapshots</h3>
        <p>A snapshot preserves the state of a virtual machine at a specific point in time. Unlike a backup, which is a copy of data, a snapshot is a freeze-frame of the disk and memory state.</p>
        <p>Snapshots are primarily used as a safety net before performing risky operations, such as major OS upgrades or testing unstable software. If the operation fails, the administrator can perform a "Rollback" to revert the system state exactly to the moment the snapshot was taken.</p>
        <ul>
            <li>When a snapshot is taken in Proxmox (specifically on QCOW2 or ZFS storage), the system marks the current data blocks as read-only.</li>
            <li>Any new writes are diverted to new blocks.</li>
            <li>If basic disk snapshotting is selected, only the disk state is saved (crash-consistent).</li>
            <li>If "Include RAM" is selected, the entire contents of the running memory are dumped to disk.</li>
            <li>This allows the VM to be restored to a running state, preserving open applications and processes, though it takes longer to complete.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>3.3 Console Access</h3>
        <p>Accessing the VM's display is handled via remote desktop protocols integrated into the browser.</p>
        <p>NoVNC is the default HTML5-based console. It requires no plugins and renders the VM's display directly in any modern web browser using WebSockets. It is lightweight and universally compatible but has limited support for clipboard integration and audio forwarding.</p>
        <ul>
            <li>For a richer desktop experience, Proxmox supports SPICE.</li>
            <li>This protocol offers advanced features such as high-quality audio streaming, multi-monitor support, and USB device redirection (plugging a USB drive into your client and having it appear in the VM).</li>
            <li>However, unlike NoVNC, SPICE requires a dedicated client viewer software (virt-viewer) to be installed on the user's machine.</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Section 3 Checkpoint</h3>
        <p>Summary:</p>
        <ul>
            <li>Cloning: Full (Performance/Isolation) vs Linked (Speed/Space).</li>
            <li>Snapshots: "Save Game" state before risky changes. Not a backup!</li>
            <li>Consoles: NoVNC is convenient (browser-based), SPICE is powerful (requires client).</li>
        </ul>
        <p>Reflection:</p>
        <ul>
            <li>Why should you delete a Linked Clone if you delete the Parent?</li>
            <li>Does a snapshot consume disk space? If so, when?</li>
        </ul>
        <p>Resources:</p>
        <ul>
            <li>Proxmox VE: Live Snapshots</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>4. Summary and Next Steps</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h3>Preparing for Week 3</h3>
        <p>Next week, we go deeper into the infrastructure. We will explore Virtual Networking Fundamentals. Since you now know that a VM is just a process, it's time to understand how to wire these processes together using Linux Bridges, veth pairs, and VLANs.</p>
        <p>Checklist:</p>
        <ul>
            <li>Can you explain what happens during a "VM Exit"?</li>
            <li>Do you understand why a vCPU is treated like a Chrome tab by the scheduler?</li>
            <li>Have you successfully SSH'd into your new Ubuntu Server?</li>
        </ul>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide">
        <h2>5. Lab Exercises</h2>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

    <div class="slide title-slide">
        <h2>Summary</h2>
        <p style="font-size: 1.5em; margin-top: 30px;">Review the key concepts covered in this week's material</p>
        <p style="font-size: 1.2em; margin-top: 20px; color: #ffd700;">Questions?</p>
        <img src="../ops3_logo.png" alt="VUT Logo" class="vut-logo">
    </div>

</body>
</html>